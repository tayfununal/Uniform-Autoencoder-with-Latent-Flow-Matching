{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayfununal/Uniform-Autoencoder-with-Latent-Flow-Matching/blob/main/ty_uae_gift/spiral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tayfununal/Uniform-Autoencoder-with-Latent-Flow-Matching.git"
      ],
      "metadata": {
        "id": "jwcQVEUHgpcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05be67c-888f-4236-b9b1-d2b53ec97331"
      },
      "id": "jwcQVEUHgpcu",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Uniform-Autoencoder-with-Latent-Flow-Matching' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
      "metadata": {
        "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a8da4d-d819-4d4c-89db-97931bd31ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "%run /content/Uniform-Autoencoder-with-Latent-Flow-Matching/models/spiral_model.ipynb\n",
        "%run /content/Uniform-Autoencoder-with-Latent-Flow-Matching/datasets/spiral_dataset.ipynb\n",
        "\n",
        "plt.rcParams['font.size'] = 20\n",
        "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman']\n",
        "\n",
        "\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "f7601ea5-682f-44a9-9b06-0f0efd697f46",
      "metadata": {
        "id": "f7601ea5-682f-44a9-9b06-0f0efd697f46"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, device='cpu', max_patience=20):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "\n",
        "        self.max_patience = max_patience\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience = 0\n",
        "\n",
        "        self.val_cost = []\n",
        "        self.train_cost = []\n",
        "\n",
        "        self.output_dir = 'latent_plots'\n",
        "        if not os.path.exists(self.output_dir):\n",
        "            os.makedirs(self.output_dir)\n",
        "        self.image_files = []\n",
        "\n",
        "    def train(self, train_loader, val_loader=None, epochs=10, print_every=1, name=None):\n",
        "        self.model.train()\n",
        "\n",
        "        # Eğitim başlamadan önce bir örnek görsel al (Başlangıç durumu)\n",
        "        self._save_latent_scatter(0, test_loader) # İstediğin görselleştirme burada\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            total_loss = 0.0\n",
        "\n",
        "\n",
        "            for x, x_, y in train_loader:\n",
        "                x = x.to(self.device)\n",
        "                x_ = x_.to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    z_hat_ = self.model.encoder(x_)\n",
        "                z_hat, x_hat = self.model(x)\n",
        "\n",
        "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,3))\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            if val_loader:\n",
        "                val_loss = self.validate(val_loader)\n",
        "\n",
        "                # Early stopping check\n",
        "                if val_loss < self.best_val_loss:\n",
        "                    print('saved!')\n",
        "                    torch.save(self.model, name + '.model')\n",
        "                    self.best_val_loss = val_loss\n",
        "                    self.patience = 0\n",
        "\n",
        "                    # Her epoch sonunda görseli kaydet\n",
        "                    self._save_latent_scatter(epoch, test_loader) # İstediğin görselleştirme burada\n",
        "\n",
        "                else:\n",
        "                    self.patience = self.patience + 1\n",
        "\n",
        "                if self.patience > self.max_patience:\n",
        "                    break\n",
        "\n",
        "            if epoch % print_every == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Train Loss: {total_loss / len(train_loader):.6f} | Validation Loss: {val_loss:.6f}\")\n",
        "\n",
        "            self.val_cost.append(val_loss)\n",
        "            self.train_cost.append(total_loss / len(train_loader))\n",
        "\n",
        "        self.create_gif(name)\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, x_, _ in val_loader:\n",
        "                x = x.to(self.device)\n",
        "                x_ = x_.to(self.device)\n",
        "\n",
        "                z_hat_ = self.model.encoder(x_)\n",
        "                z_hat, x_hat = self.model(x)\n",
        "\n",
        "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,3))\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        self.model.train()\n",
        "        return avg_loss\n",
        "\n",
        "    def _save_latent_scatter(self, epoch, loader):\n",
        "        \"\"\"Verdiğin scatter plot mantığını kullanarak latent uzayını kaydeder.\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Görselleştirme için bir batch al\n",
        "            X_test, y_test = next(iter(loader))\n",
        "            X_test = X_test.to(self.device)\n",
        "\n",
        "            # Modelden latent (z) değerlerini al\n",
        "            z, _ = self.model(X_test)\n",
        "            z = z.cpu().numpy()\n",
        "            y_test = y_test.cpu().numpy()\n",
        "\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            colors = ['blue', 'green', 'orange', 'red']\n",
        "            labels = ['0', '1', '2', '3']\n",
        "\n",
        "            for i in range(4):\n",
        "                mask = (y_test == i)\n",
        "                plt.scatter(z[mask, 0], z[mask, 1],\n",
        "                            color=colors[i], label=labels[i], alpha=1.0)\n",
        "\n",
        "            plt.xlabel('$z_1$')\n",
        "            plt.ylabel('$z_2$')\n",
        "            plt.title(f\"Spiral in 2D Latent Space\")\n",
        "            #plt.legend()\n",
        "            plt.grid(True, linestyle='--', alpha=1.0)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            file_path = os.path.join(self.output_dir, f\"epoch_{epoch:03d}.png\")\n",
        "            plt.savefig(file_path)\n",
        "            plt.close()\n",
        "            self.image_files.append(file_path)\n",
        "        self.model.train()\n",
        "\n",
        "    def create_gif(self, name):\n",
        "        gif_path = f\"{name}_latent_evolution.gif\"\n",
        "        images = [imageio.imread(f) for f in self.image_files]\n",
        "        imageio.mimsave(gif_path, images, fps=5)\n",
        "        print(f\"GIF oluşturuldu: {gif_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "6982ede7-a39e-433c-90a9-8850b1f73c85",
      "metadata": {
        "id": "6982ede7-a39e-433c-90a9-8850b1f73c85"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom Transform\n",
        "class NoiseTransform:\n",
        "    \"\"\"Add some noise.\"\"\"\n",
        "\n",
        "    def __init__(self, split_ratio=0.001, dim=3):\n",
        "\n",
        "        self.split_ratio = split_ratio\n",
        "        self.dim = dim\n",
        "\n",
        "    def __call__(self, x):\n",
        "      return x + self.split_ratio * torch.randn(self.dim, device='cuda' if torch.cuda.is_available() else 'cpu' , dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f",
      "metadata": {
        "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f"
      },
      "outputs": [],
      "source": [
        "# Hyper-Parameters & Settings\n",
        "\n",
        "dataset_size = 5000\n",
        "batch_size = 250\n",
        "lr = 0.001\n",
        "\n",
        "epochs = 5000\n",
        "max_patience = 5000\n",
        "\n",
        "split_ratio = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "07121ef5-db8d-4bd5-a327-a8646f696809",
      "metadata": {
        "id": "07121ef5-db8d-4bd5-a327-a8646f696809"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "train_dataset = SpiralDataset(mode='train', n_samples=dataset_size, transform=NoiseTransform(split_ratio))\n",
        "val_dataset = SpiralDataset(mode='val', n_samples=dataset_size, transform=NoiseTransform(split_ratio))\n",
        "test_dataset = SpiralDataset(mode='test', n_samples=2*10000)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the \"results\" folder\n",
        "os.makedirs(\"/content/Uniform-Autoencoder-with-Latent-Flow-Matching/results/spiral\", exist_ok=True)"
      ],
      "metadata": {
        "id": "nLdB2Zjuhg4I"
      },
      "id": "nLdB2Zjuhg4I",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c9abf79c-a874-418b-b46f-aaa2fe649d6e",
      "metadata": {
        "id": "c9abf79c-a874-418b-b46f-aaa2fe649d6e"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "name = '/content/Uniform-Autoencoder-with-Latent-Flow-Matching/results/spiral/UAE_Spiral'\n",
        "model = To_Uniform(\n",
        "                 encoder_layers=[3, 128, 128, 128, 2],\n",
        "                 decoder_layers=[2, 128, 128, 128, 3],\n",
        "                 encoder_act=nn.ReLU,\n",
        "                 decoder_act=nn.ReLU,\n",
        "                 final_encoder_act=nn.Sigmoid,\n",
        "                 final_decoder_act=nn.Sigmoid,\n",
        "                 use_batchnorm=True\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48",
      "metadata": {
        "scrolled": true,
        "id": "8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374a902b-715c-4bf1-d7e8-8bac875d0fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mGörüntülenen çıkış son 5000 satıra kısaltıldı.\u001b[0m\n",
            "saved!\n",
            "Epoch  21 | Train Loss: 0.000435 | Validation Loss: 0.000366\n",
            "Epoch  22 | Train Loss: 0.000404 | Validation Loss: 0.000482\n",
            "Epoch  23 | Train Loss: 0.000470 | Validation Loss: 0.000551\n",
            "Epoch  24 | Train Loss: 0.000533 | Validation Loss: 0.000422\n",
            "Epoch  25 | Train Loss: 0.000452 | Validation Loss: 0.000529\n",
            "saved!\n",
            "Epoch  26 | Train Loss: 0.000557 | Validation Loss: 0.000363\n",
            "Epoch  27 | Train Loss: 0.000488 | Validation Loss: 0.000556\n",
            "Epoch  28 | Train Loss: 0.000460 | Validation Loss: 0.000375\n",
            "Epoch  29 | Train Loss: 0.000413 | Validation Loss: 0.000372\n",
            "Epoch  30 | Train Loss: 0.000407 | Validation Loss: 0.000411\n",
            "Epoch  31 | Train Loss: 0.000475 | Validation Loss: 0.000382\n",
            "Epoch  32 | Train Loss: 0.000422 | Validation Loss: 0.000384\n",
            "Epoch  33 | Train Loss: 0.000492 | Validation Loss: 0.000476\n",
            "Epoch  34 | Train Loss: 0.000447 | Validation Loss: 0.000364\n",
            "saved!\n",
            "Epoch  35 | Train Loss: 0.000424 | Validation Loss: 0.000349\n",
            "Epoch  36 | Train Loss: 0.000375 | Validation Loss: 0.000371\n",
            "saved!\n",
            "Epoch  37 | Train Loss: 0.000416 | Validation Loss: 0.000325\n",
            "saved!\n",
            "Epoch  38 | Train Loss: 0.000369 | Validation Loss: 0.000320\n",
            "Epoch  39 | Train Loss: 0.000369 | Validation Loss: 0.000347\n",
            "Epoch  40 | Train Loss: 0.000347 | Validation Loss: 0.000356\n",
            "Epoch  41 | Train Loss: 0.000402 | Validation Loss: 0.000326\n",
            "Epoch  42 | Train Loss: 0.000380 | Validation Loss: 0.000373\n",
            "Epoch  43 | Train Loss: 0.000562 | Validation Loss: 0.000528\n",
            "Epoch  44 | Train Loss: 0.000497 | Validation Loss: 0.000389\n",
            "Epoch  45 | Train Loss: 0.000403 | Validation Loss: 0.000326\n",
            "Epoch  46 | Train Loss: 0.000434 | Validation Loss: 0.000339\n",
            "Epoch  47 | Train Loss: 0.000365 | Validation Loss: 0.000333\n",
            "Epoch  48 | Train Loss: 0.000447 | Validation Loss: 0.000446\n",
            "Epoch  49 | Train Loss: 0.000469 | Validation Loss: 0.000379\n",
            "Epoch  50 | Train Loss: 0.000405 | Validation Loss: 0.000329\n",
            "Epoch  51 | Train Loss: 0.000395 | Validation Loss: 0.000363\n",
            "Epoch  52 | Train Loss: 0.000410 | Validation Loss: 0.000321\n",
            "Epoch  53 | Train Loss: 0.000351 | Validation Loss: 0.000332\n",
            "Epoch  54 | Train Loss: 0.000368 | Validation Loss: 0.000338\n",
            "saved!\n",
            "Epoch  55 | Train Loss: 0.000341 | Validation Loss: 0.000295\n",
            "saved!\n",
            "Epoch  56 | Train Loss: 0.000347 | Validation Loss: 0.000294\n",
            "Epoch  57 | Train Loss: 0.000370 | Validation Loss: 0.000321\n",
            "Epoch  58 | Train Loss: 0.000383 | Validation Loss: 0.000355\n",
            "Epoch  59 | Train Loss: 0.000345 | Validation Loss: 0.000316\n",
            "Epoch  60 | Train Loss: 0.000345 | Validation Loss: 0.000361\n",
            "Epoch  61 | Train Loss: 0.000343 | Validation Loss: 0.000333\n",
            "Epoch  62 | Train Loss: 0.000373 | Validation Loss: 0.000369\n",
            "Epoch  63 | Train Loss: 0.000395 | Validation Loss: 0.000454\n",
            "Epoch  64 | Train Loss: 0.000488 | Validation Loss: 0.000392\n",
            "Epoch  65 | Train Loss: 0.000420 | Validation Loss: 0.000333\n",
            "Epoch  66 | Train Loss: 0.000423 | Validation Loss: 0.000465\n",
            "Epoch  67 | Train Loss: 0.000434 | Validation Loss: 0.000349\n",
            "Epoch  68 | Train Loss: 0.000355 | Validation Loss: 0.000334\n",
            "Epoch  69 | Train Loss: 0.000342 | Validation Loss: 0.000344\n",
            "Epoch  70 | Train Loss: 0.000395 | Validation Loss: 0.000382\n",
            "Epoch  71 | Train Loss: 0.000359 | Validation Loss: 0.000317\n",
            "Epoch  72 | Train Loss: 0.000374 | Validation Loss: 0.000325\n",
            "Epoch  73 | Train Loss: 0.000381 | Validation Loss: 0.000314\n",
            "Epoch  74 | Train Loss: 0.000365 | Validation Loss: 0.000298\n",
            "Epoch  75 | Train Loss: 0.000356 | Validation Loss: 0.000341\n",
            "Epoch  76 | Train Loss: 0.000389 | Validation Loss: 0.000305\n",
            "Epoch  77 | Train Loss: 0.000469 | Validation Loss: 0.000332\n",
            "Epoch  78 | Train Loss: 0.000397 | Validation Loss: 0.000330\n",
            "Epoch  79 | Train Loss: 0.000362 | Validation Loss: 0.000318\n",
            "saved!\n",
            "Epoch  80 | Train Loss: 0.000334 | Validation Loss: 0.000291\n",
            "Epoch  81 | Train Loss: 0.000339 | Validation Loss: 0.000328\n",
            "Epoch  82 | Train Loss: 0.000380 | Validation Loss: 0.000308\n",
            "Epoch  83 | Train Loss: 0.000359 | Validation Loss: 0.000300\n",
            "Epoch  84 | Train Loss: 0.000350 | Validation Loss: 0.000306\n",
            "Epoch  85 | Train Loss: 0.000376 | Validation Loss: 0.000335\n",
            "Epoch  86 | Train Loss: 0.000358 | Validation Loss: 0.000311\n",
            "Epoch  87 | Train Loss: 0.000331 | Validation Loss: 0.000310\n",
            "Epoch  88 | Train Loss: 0.000360 | Validation Loss: 0.000364\n",
            "Epoch  89 | Train Loss: 0.000342 | Validation Loss: 0.000307\n",
            "Epoch  90 | Train Loss: 0.000350 | Validation Loss: 0.000351\n",
            "Epoch  91 | Train Loss: 0.000381 | Validation Loss: 0.000347\n",
            "Epoch  92 | Train Loss: 0.000355 | Validation Loss: 0.000333\n",
            "Epoch  93 | Train Loss: 0.000320 | Validation Loss: 0.000367\n",
            "Epoch  94 | Train Loss: 0.000388 | Validation Loss: 0.000369\n",
            "Epoch  95 | Train Loss: 0.000378 | Validation Loss: 0.000331\n",
            "Epoch  96 | Train Loss: 0.000347 | Validation Loss: 0.000293\n",
            "Epoch  97 | Train Loss: 0.000320 | Validation Loss: 0.000313\n",
            "Epoch  98 | Train Loss: 0.000391 | Validation Loss: 0.000320\n",
            "Epoch  99 | Train Loss: 0.000400 | Validation Loss: 0.000385\n",
            "Epoch 100 | Train Loss: 0.000456 | Validation Loss: 0.000352\n",
            "Epoch 101 | Train Loss: 0.000436 | Validation Loss: 0.000323\n",
            "Epoch 102 | Train Loss: 0.000345 | Validation Loss: 0.000326\n",
            "Epoch 103 | Train Loss: 0.000394 | Validation Loss: 0.000317\n",
            "Epoch 104 | Train Loss: 0.000330 | Validation Loss: 0.000368\n",
            "Epoch 105 | Train Loss: 0.000342 | Validation Loss: 0.000324\n",
            "Epoch 106 | Train Loss: 0.000338 | Validation Loss: 0.000317\n",
            "Epoch 107 | Train Loss: 0.000355 | Validation Loss: 0.000309\n",
            "Epoch 108 | Train Loss: 0.000336 | Validation Loss: 0.000292\n",
            "Epoch 109 | Train Loss: 0.000340 | Validation Loss: 0.000296\n",
            "Epoch 110 | Train Loss: 0.000344 | Validation Loss: 0.000322\n",
            "Epoch 111 | Train Loss: 0.000349 | Validation Loss: 0.000310\n",
            "Epoch 112 | Train Loss: 0.000327 | Validation Loss: 0.000294\n",
            "Epoch 113 | Train Loss: 0.000345 | Validation Loss: 0.000301\n",
            "Epoch 114 | Train Loss: 0.000365 | Validation Loss: 0.000309\n",
            "Epoch 115 | Train Loss: 0.000324 | Validation Loss: 0.000309\n",
            "Epoch 116 | Train Loss: 0.000387 | Validation Loss: 0.000332\n",
            "Epoch 117 | Train Loss: 0.000376 | Validation Loss: 0.000334\n",
            "Epoch 118 | Train Loss: 0.000337 | Validation Loss: 0.000314\n",
            "Epoch 119 | Train Loss: 0.000350 | Validation Loss: 0.000333\n",
            "Epoch 120 | Train Loss: 0.000336 | Validation Loss: 0.000293\n",
            "Epoch 121 | Train Loss: 0.000351 | Validation Loss: 0.000337\n",
            "Epoch 122 | Train Loss: 0.000350 | Validation Loss: 0.000307\n",
            "Epoch 123 | Train Loss: 0.000359 | Validation Loss: 0.000345\n",
            "Epoch 124 | Train Loss: 0.000321 | Validation Loss: 0.000310\n",
            "Epoch 125 | Train Loss: 0.000349 | Validation Loss: 0.000296\n",
            "Epoch 126 | Train Loss: 0.000346 | Validation Loss: 0.000448\n",
            "Epoch 127 | Train Loss: 0.000439 | Validation Loss: 0.000407\n",
            "Epoch 128 | Train Loss: 0.000381 | Validation Loss: 0.000325\n",
            "saved!\n",
            "Epoch 129 | Train Loss: 0.000364 | Validation Loss: 0.000290\n",
            "Epoch 130 | Train Loss: 0.000434 | Validation Loss: 0.000359\n",
            "Epoch 131 | Train Loss: 0.000423 | Validation Loss: 0.000404\n",
            "Epoch 132 | Train Loss: 0.000384 | Validation Loss: 0.000405\n",
            "Epoch 133 | Train Loss: 0.000357 | Validation Loss: 0.000331\n",
            "Epoch 134 | Train Loss: 0.000343 | Validation Loss: 0.000299\n",
            "Epoch 135 | Train Loss: 0.000366 | Validation Loss: 0.000312\n",
            "Epoch 136 | Train Loss: 0.000338 | Validation Loss: 0.000308\n",
            "Epoch 137 | Train Loss: 0.000369 | Validation Loss: 0.000326\n",
            "Epoch 138 | Train Loss: 0.000355 | Validation Loss: 0.000332\n",
            "Epoch 139 | Train Loss: 0.000371 | Validation Loss: 0.000305\n",
            "Epoch 140 | Train Loss: 0.000333 | Validation Loss: 0.000311\n",
            "Epoch 141 | Train Loss: 0.000364 | Validation Loss: 0.000315\n",
            "Epoch 142 | Train Loss: 0.000341 | Validation Loss: 0.000320\n",
            "Epoch 143 | Train Loss: 0.000411 | Validation Loss: 0.000368\n",
            "Epoch 144 | Train Loss: 0.000359 | Validation Loss: 0.000336\n",
            "Epoch 145 | Train Loss: 0.000334 | Validation Loss: 0.000335\n",
            "Epoch 146 | Train Loss: 0.000338 | Validation Loss: 0.000293\n",
            "Epoch 147 | Train Loss: 0.000335 | Validation Loss: 0.000346\n",
            "saved!\n",
            "Epoch 148 | Train Loss: 0.000348 | Validation Loss: 0.000284\n",
            "Epoch 149 | Train Loss: 0.000339 | Validation Loss: 0.000343\n",
            "Epoch 150 | Train Loss: 0.000339 | Validation Loss: 0.000305\n",
            "Epoch 151 | Train Loss: 0.000344 | Validation Loss: 0.000289\n",
            "Epoch 152 | Train Loss: 0.000299 | Validation Loss: 0.000305\n",
            "Epoch 153 | Train Loss: 0.000344 | Validation Loss: 0.000348\n",
            "Epoch 154 | Train Loss: 0.000436 | Validation Loss: 0.000367\n",
            "Epoch 155 | Train Loss: 0.000390 | Validation Loss: 0.000360\n",
            "Epoch 156 | Train Loss: 0.000394 | Validation Loss: 0.000351\n",
            "Epoch 157 | Train Loss: 0.000403 | Validation Loss: 0.000348\n",
            "Epoch 158 | Train Loss: 0.000361 | Validation Loss: 0.000336\n",
            "Epoch 159 | Train Loss: 0.000320 | Validation Loss: 0.000342\n",
            "Epoch 160 | Train Loss: 0.000388 | Validation Loss: 0.000438\n",
            "Epoch 161 | Train Loss: 0.000386 | Validation Loss: 0.000393\n",
            "Epoch 162 | Train Loss: 0.000348 | Validation Loss: 0.000310\n",
            "Epoch 163 | Train Loss: 0.000465 | Validation Loss: 0.000389\n",
            "Epoch 164 | Train Loss: 0.000449 | Validation Loss: 0.000351\n",
            "Epoch 165 | Train Loss: 0.000360 | Validation Loss: 0.000368\n",
            "Epoch 166 | Train Loss: 0.000339 | Validation Loss: 0.000302\n",
            "Epoch 167 | Train Loss: 0.000326 | Validation Loss: 0.000297\n",
            "Epoch 168 | Train Loss: 0.000345 | Validation Loss: 0.000309\n",
            "Epoch 169 | Train Loss: 0.000353 | Validation Loss: 0.000306\n",
            "Epoch 170 | Train Loss: 0.000339 | Validation Loss: 0.000361\n",
            "Epoch 171 | Train Loss: 0.000327 | Validation Loss: 0.000348\n",
            "Epoch 172 | Train Loss: 0.000361 | Validation Loss: 0.000376\n",
            "Epoch 173 | Train Loss: 0.000371 | Validation Loss: 0.000315\n",
            "Epoch 174 | Train Loss: 0.000314 | Validation Loss: 0.000302\n",
            "Epoch 175 | Train Loss: 0.000323 | Validation Loss: 0.000292\n",
            "Epoch 176 | Train Loss: 0.000350 | Validation Loss: 0.000406\n",
            "Epoch 177 | Train Loss: 0.000357 | Validation Loss: 0.000343\n",
            "Epoch 178 | Train Loss: 0.000336 | Validation Loss: 0.000366\n",
            "Epoch 179 | Train Loss: 0.000353 | Validation Loss: 0.000318\n",
            "Epoch 180 | Train Loss: 0.000345 | Validation Loss: 0.000320\n",
            "Epoch 181 | Train Loss: 0.000393 | Validation Loss: 0.000373\n",
            "Epoch 182 | Train Loss: 0.000327 | Validation Loss: 0.000331\n",
            "saved!\n",
            "Epoch 183 | Train Loss: 0.000328 | Validation Loss: 0.000272\n",
            "Epoch 184 | Train Loss: 0.000308 | Validation Loss: 0.000306\n",
            "Epoch 185 | Train Loss: 0.000328 | Validation Loss: 0.000294\n",
            "Epoch 186 | Train Loss: 0.000371 | Validation Loss: 0.000316\n",
            "Epoch 187 | Train Loss: 0.000368 | Validation Loss: 0.000371\n",
            "Epoch 188 | Train Loss: 0.000382 | Validation Loss: 0.000361\n",
            "Epoch 189 | Train Loss: 0.000309 | Validation Loss: 0.000299\n",
            "Epoch 190 | Train Loss: 0.000333 | Validation Loss: 0.000325\n",
            "Epoch 191 | Train Loss: 0.000327 | Validation Loss: 0.000311\n",
            "Epoch 192 | Train Loss: 0.000312 | Validation Loss: 0.000300\n",
            "Epoch 193 | Train Loss: 0.000299 | Validation Loss: 0.000292\n",
            "Epoch 194 | Train Loss: 0.000324 | Validation Loss: 0.000297\n",
            "Epoch 195 | Train Loss: 0.000342 | Validation Loss: 0.000304\n",
            "Epoch 196 | Train Loss: 0.000319 | Validation Loss: 0.000297\n",
            "Epoch 197 | Train Loss: 0.000343 | Validation Loss: 0.000323\n",
            "Epoch 198 | Train Loss: 0.000332 | Validation Loss: 0.000305\n",
            "Epoch 199 | Train Loss: 0.000339 | Validation Loss: 0.000332\n",
            "Epoch 200 | Train Loss: 0.000356 | Validation Loss: 0.000338\n",
            "Epoch 201 | Train Loss: 0.000325 | Validation Loss: 0.000320\n",
            "Epoch 202 | Train Loss: 0.000343 | Validation Loss: 0.000348\n",
            "Epoch 203 | Train Loss: 0.000421 | Validation Loss: 0.000436\n",
            "Epoch 204 | Train Loss: 0.000347 | Validation Loss: 0.000304\n",
            "Epoch 205 | Train Loss: 0.000330 | Validation Loss: 0.000291\n",
            "Epoch 206 | Train Loss: 0.000314 | Validation Loss: 0.000296\n",
            "Epoch 207 | Train Loss: 0.000316 | Validation Loss: 0.000277\n",
            "Epoch 208 | Train Loss: 0.000302 | Validation Loss: 0.000297\n",
            "Epoch 209 | Train Loss: 0.000333 | Validation Loss: 0.000288\n",
            "Epoch 210 | Train Loss: 0.000317 | Validation Loss: 0.000284\n",
            "Epoch 211 | Train Loss: 0.000323 | Validation Loss: 0.000308\n",
            "Epoch 212 | Train Loss: 0.000330 | Validation Loss: 0.000283\n",
            "Epoch 213 | Train Loss: 0.000330 | Validation Loss: 0.000289\n",
            "Epoch 214 | Train Loss: 0.000313 | Validation Loss: 0.000305\n",
            "Epoch 215 | Train Loss: 0.000343 | Validation Loss: 0.000347\n",
            "Epoch 216 | Train Loss: 0.000348 | Validation Loss: 0.000295\n",
            "Epoch 217 | Train Loss: 0.000306 | Validation Loss: 0.000295\n",
            "Epoch 218 | Train Loss: 0.000365 | Validation Loss: 0.000329\n",
            "Epoch 219 | Train Loss: 0.000324 | Validation Loss: 0.000359\n",
            "Epoch 220 | Train Loss: 0.000342 | Validation Loss: 0.000312\n",
            "Epoch 221 | Train Loss: 0.000341 | Validation Loss: 0.000330\n",
            "Epoch 222 | Train Loss: 0.000359 | Validation Loss: 0.000386\n",
            "Epoch 223 | Train Loss: 0.000319 | Validation Loss: 0.000304\n",
            "Epoch 224 | Train Loss: 0.000317 | Validation Loss: 0.000295\n",
            "Epoch 225 | Train Loss: 0.000350 | Validation Loss: 0.000333\n",
            "Epoch 226 | Train Loss: 0.000352 | Validation Loss: 0.000316\n",
            "Epoch 227 | Train Loss: 0.000322 | Validation Loss: 0.000314\n",
            "Epoch 228 | Train Loss: 0.000368 | Validation Loss: 0.000328\n",
            "Epoch 229 | Train Loss: 0.000350 | Validation Loss: 0.000294\n",
            "Epoch 230 | Train Loss: 0.000307 | Validation Loss: 0.000289\n",
            "Epoch 231 | Train Loss: 0.000365 | Validation Loss: 0.000347\n",
            "Epoch 232 | Train Loss: 0.000410 | Validation Loss: 0.000466\n",
            "Epoch 233 | Train Loss: 0.000339 | Validation Loss: 0.000359\n",
            "Epoch 234 | Train Loss: 0.000321 | Validation Loss: 0.000316\n",
            "Epoch 235 | Train Loss: 0.000334 | Validation Loss: 0.000329\n",
            "Epoch 236 | Train Loss: 0.000296 | Validation Loss: 0.000282\n",
            "Epoch 237 | Train Loss: 0.000313 | Validation Loss: 0.000338\n",
            "Epoch 238 | Train Loss: 0.000356 | Validation Loss: 0.000292\n",
            "Epoch 239 | Train Loss: 0.000394 | Validation Loss: 0.000498\n",
            "Epoch 240 | Train Loss: 0.000424 | Validation Loss: 0.000367\n",
            "Epoch 241 | Train Loss: 0.000331 | Validation Loss: 0.000318\n",
            "Epoch 242 | Train Loss: 0.000339 | Validation Loss: 0.000278\n",
            "Epoch 243 | Train Loss: 0.000313 | Validation Loss: 0.000306\n",
            "Epoch 244 | Train Loss: 0.000323 | Validation Loss: 0.000310\n",
            "Epoch 245 | Train Loss: 0.000326 | Validation Loss: 0.000325\n",
            "Epoch 246 | Train Loss: 0.000354 | Validation Loss: 0.000302\n",
            "Epoch 247 | Train Loss: 0.000296 | Validation Loss: 0.000309\n",
            "Epoch 248 | Train Loss: 0.000348 | Validation Loss: 0.000317\n",
            "Epoch 249 | Train Loss: 0.000344 | Validation Loss: 0.000346\n",
            "Epoch 250 | Train Loss: 0.000353 | Validation Loss: 0.000393\n",
            "Epoch 251 | Train Loss: 0.000356 | Validation Loss: 0.000329\n",
            "Epoch 252 | Train Loss: 0.000332 | Validation Loss: 0.000287\n",
            "Epoch 253 | Train Loss: 0.000339 | Validation Loss: 0.000308\n",
            "Epoch 254 | Train Loss: 0.000383 | Validation Loss: 0.000295\n",
            "Epoch 255 | Train Loss: 0.000348 | Validation Loss: 0.000333\n",
            "Epoch 256 | Train Loss: 0.000328 | Validation Loss: 0.000312\n",
            "Epoch 257 | Train Loss: 0.000319 | Validation Loss: 0.000340\n",
            "Epoch 258 | Train Loss: 0.000311 | Validation Loss: 0.000303\n",
            "Epoch 259 | Train Loss: 0.000319 | Validation Loss: 0.000319\n",
            "Epoch 260 | Train Loss: 0.000326 | Validation Loss: 0.000314\n",
            "Epoch 261 | Train Loss: 0.000314 | Validation Loss: 0.000288\n",
            "Epoch 262 | Train Loss: 0.000328 | Validation Loss: 0.000306\n",
            "Epoch 263 | Train Loss: 0.000316 | Validation Loss: 0.000306\n",
            "Epoch 264 | Train Loss: 0.000359 | Validation Loss: 0.000295\n",
            "Epoch 265 | Train Loss: 0.000371 | Validation Loss: 0.000344\n",
            "Epoch 266 | Train Loss: 0.000371 | Validation Loss: 0.000324\n",
            "Epoch 267 | Train Loss: 0.000325 | Validation Loss: 0.000389\n",
            "Epoch 268 | Train Loss: 0.000334 | Validation Loss: 0.000360\n",
            "Epoch 269 | Train Loss: 0.000324 | Validation Loss: 0.000310\n",
            "Epoch 270 | Train Loss: 0.000317 | Validation Loss: 0.000286\n",
            "Epoch 271 | Train Loss: 0.000310 | Validation Loss: 0.000311\n",
            "Epoch 272 | Train Loss: 0.000314 | Validation Loss: 0.000291\n",
            "Epoch 273 | Train Loss: 0.000346 | Validation Loss: 0.000285\n",
            "Epoch 274 | Train Loss: 0.000326 | Validation Loss: 0.000291\n",
            "Epoch 275 | Train Loss: 0.000310 | Validation Loss: 0.000303\n",
            "Epoch 276 | Train Loss: 0.000343 | Validation Loss: 0.000328\n",
            "Epoch 277 | Train Loss: 0.000339 | Validation Loss: 0.000310\n",
            "Epoch 278 | Train Loss: 0.000306 | Validation Loss: 0.000299\n",
            "Epoch 279 | Train Loss: 0.000312 | Validation Loss: 0.000280\n",
            "Epoch 280 | Train Loss: 0.000347 | Validation Loss: 0.000352\n",
            "Epoch 281 | Train Loss: 0.000387 | Validation Loss: 0.000368\n",
            "Epoch 282 | Train Loss: 0.000362 | Validation Loss: 0.000361\n",
            "Epoch 283 | Train Loss: 0.000380 | Validation Loss: 0.000353\n",
            "Epoch 284 | Train Loss: 0.000428 | Validation Loss: 0.000348\n",
            "Epoch 285 | Train Loss: 0.000390 | Validation Loss: 0.000325\n",
            "Epoch 286 | Train Loss: 0.000363 | Validation Loss: 0.000297\n",
            "Epoch 287 | Train Loss: 0.000284 | Validation Loss: 0.000306\n",
            "Epoch 288 | Train Loss: 0.000322 | Validation Loss: 0.000313\n",
            "Epoch 289 | Train Loss: 0.000315 | Validation Loss: 0.000334\n",
            "Epoch 290 | Train Loss: 0.000308 | Validation Loss: 0.000288\n",
            "Epoch 291 | Train Loss: 0.000294 | Validation Loss: 0.000296\n",
            "Epoch 292 | Train Loss: 0.000317 | Validation Loss: 0.000303\n",
            "Epoch 293 | Train Loss: 0.000388 | Validation Loss: 0.000335\n",
            "Epoch 294 | Train Loss: 0.000331 | Validation Loss: 0.000309\n",
            "Epoch 295 | Train Loss: 0.000330 | Validation Loss: 0.000311\n",
            "Epoch 296 | Train Loss: 0.000302 | Validation Loss: 0.000289\n",
            "Epoch 297 | Train Loss: 0.000307 | Validation Loss: 0.000307\n",
            "Epoch 298 | Train Loss: 0.000329 | Validation Loss: 0.000339\n",
            "Epoch 299 | Train Loss: 0.000335 | Validation Loss: 0.000280\n",
            "Epoch 300 | Train Loss: 0.000333 | Validation Loss: 0.000391\n",
            "Epoch 301 | Train Loss: 0.000357 | Validation Loss: 0.000304\n",
            "Epoch 302 | Train Loss: 0.000337 | Validation Loss: 0.000366\n",
            "Epoch 303 | Train Loss: 0.000340 | Validation Loss: 0.000354\n",
            "Epoch 304 | Train Loss: 0.000331 | Validation Loss: 0.000319\n",
            "Epoch 305 | Train Loss: 0.000334 | Validation Loss: 0.000335\n",
            "Epoch 306 | Train Loss: 0.000332 | Validation Loss: 0.000323\n",
            "Epoch 307 | Train Loss: 0.000326 | Validation Loss: 0.000285\n",
            "Epoch 308 | Train Loss: 0.000377 | Validation Loss: 0.000300\n",
            "Epoch 309 | Train Loss: 0.000316 | Validation Loss: 0.000319\n",
            "Epoch 310 | Train Loss: 0.000330 | Validation Loss: 0.000310\n",
            "Epoch 311 | Train Loss: 0.000289 | Validation Loss: 0.000344\n",
            "Epoch 312 | Train Loss: 0.000332 | Validation Loss: 0.000291\n",
            "Epoch 313 | Train Loss: 0.000331 | Validation Loss: 0.000310\n",
            "Epoch 314 | Train Loss: 0.000347 | Validation Loss: 0.000374\n",
            "Epoch 315 | Train Loss: 0.000356 | Validation Loss: 0.000334\n",
            "Epoch 316 | Train Loss: 0.000317 | Validation Loss: 0.000309\n",
            "Epoch 317 | Train Loss: 0.000342 | Validation Loss: 0.000308\n",
            "Epoch 318 | Train Loss: 0.000297 | Validation Loss: 0.000312\n",
            "Epoch 319 | Train Loss: 0.000311 | Validation Loss: 0.000293\n",
            "Epoch 320 | Train Loss: 0.000323 | Validation Loss: 0.000321\n",
            "Epoch 321 | Train Loss: 0.000340 | Validation Loss: 0.000281\n",
            "Epoch 322 | Train Loss: 0.000319 | Validation Loss: 0.000289\n",
            "Epoch 323 | Train Loss: 0.000302 | Validation Loss: 0.000294\n",
            "Epoch 324 | Train Loss: 0.000302 | Validation Loss: 0.000304\n",
            "Epoch 325 | Train Loss: 0.000307 | Validation Loss: 0.000285\n",
            "Epoch 326 | Train Loss: 0.000322 | Validation Loss: 0.000327\n",
            "Epoch 327 | Train Loss: 0.000314 | Validation Loss: 0.000288\n",
            "Epoch 328 | Train Loss: 0.000303 | Validation Loss: 0.000321\n",
            "Epoch 329 | Train Loss: 0.000335 | Validation Loss: 0.000297\n",
            "Epoch 330 | Train Loss: 0.000317 | Validation Loss: 0.000310\n",
            "Epoch 331 | Train Loss: 0.000345 | Validation Loss: 0.000309\n",
            "Epoch 332 | Train Loss: 0.000340 | Validation Loss: 0.000312\n",
            "Epoch 333 | Train Loss: 0.000299 | Validation Loss: 0.000313\n",
            "Epoch 334 | Train Loss: 0.000318 | Validation Loss: 0.000297\n",
            "Epoch 335 | Train Loss: 0.000300 | Validation Loss: 0.000285\n",
            "Epoch 336 | Train Loss: 0.000320 | Validation Loss: 0.000329\n",
            "Epoch 337 | Train Loss: 0.000339 | Validation Loss: 0.000312\n",
            "Epoch 338 | Train Loss: 0.000333 | Validation Loss: 0.000322\n",
            "Epoch 339 | Train Loss: 0.000358 | Validation Loss: 0.000318\n",
            "Epoch 340 | Train Loss: 0.000355 | Validation Loss: 0.000312\n",
            "Epoch 341 | Train Loss: 0.000299 | Validation Loss: 0.000339\n",
            "Epoch 342 | Train Loss: 0.000338 | Validation Loss: 0.000305\n",
            "Epoch 343 | Train Loss: 0.000326 | Validation Loss: 0.000298\n",
            "Epoch 344 | Train Loss: 0.000340 | Validation Loss: 0.000292\n",
            "Epoch 345 | Train Loss: 0.000303 | Validation Loss: 0.000292\n",
            "Epoch 346 | Train Loss: 0.000310 | Validation Loss: 0.000288\n",
            "Epoch 347 | Train Loss: 0.000334 | Validation Loss: 0.000312\n",
            "Epoch 348 | Train Loss: 0.000319 | Validation Loss: 0.000306\n",
            "Epoch 349 | Train Loss: 0.000299 | Validation Loss: 0.000325\n",
            "Epoch 350 | Train Loss: 0.000321 | Validation Loss: 0.000331\n",
            "Epoch 351 | Train Loss: 0.000292 | Validation Loss: 0.000306\n",
            "Epoch 352 | Train Loss: 0.000313 | Validation Loss: 0.000284\n",
            "Epoch 353 | Train Loss: 0.000313 | Validation Loss: 0.000296\n",
            "Epoch 354 | Train Loss: 0.000312 | Validation Loss: 0.000297\n",
            "Epoch 355 | Train Loss: 0.000303 | Validation Loss: 0.000291\n",
            "Epoch 356 | Train Loss: 0.000294 | Validation Loss: 0.000281\n",
            "Epoch 357 | Train Loss: 0.000284 | Validation Loss: 0.000292\n",
            "Epoch 358 | Train Loss: 0.000300 | Validation Loss: 0.000273\n",
            "Epoch 359 | Train Loss: 0.000317 | Validation Loss: 0.000310\n",
            "Epoch 360 | Train Loss: 0.000310 | Validation Loss: 0.000278\n",
            "Epoch 361 | Train Loss: 0.000296 | Validation Loss: 0.000281\n",
            "Epoch 362 | Train Loss: 0.000324 | Validation Loss: 0.000312\n",
            "Epoch 363 | Train Loss: 0.000316 | Validation Loss: 0.000299\n",
            "Epoch 364 | Train Loss: 0.000331 | Validation Loss: 0.000296\n",
            "Epoch 365 | Train Loss: 0.000337 | Validation Loss: 0.000361\n",
            "Epoch 366 | Train Loss: 0.000364 | Validation Loss: 0.000308\n",
            "Epoch 367 | Train Loss: 0.000344 | Validation Loss: 0.000313\n",
            "saved!\n",
            "Epoch 368 | Train Loss: 0.000296 | Validation Loss: 0.000262\n",
            "Epoch 369 | Train Loss: 0.000286 | Validation Loss: 0.000312\n",
            "Epoch 370 | Train Loss: 0.000296 | Validation Loss: 0.000282\n",
            "Epoch 371 | Train Loss: 0.000302 | Validation Loss: 0.000303\n",
            "Epoch 372 | Train Loss: 0.000320 | Validation Loss: 0.000299\n",
            "Epoch 373 | Train Loss: 0.000273 | Validation Loss: 0.000281\n",
            "Epoch 374 | Train Loss: 0.000329 | Validation Loss: 0.000280\n",
            "Epoch 375 | Train Loss: 0.000320 | Validation Loss: 0.000353\n",
            "Epoch 376 | Train Loss: 0.000307 | Validation Loss: 0.000305\n",
            "Epoch 377 | Train Loss: 0.000280 | Validation Loss: 0.000287\n",
            "Epoch 378 | Train Loss: 0.000300 | Validation Loss: 0.000287\n",
            "Epoch 379 | Train Loss: 0.000317 | Validation Loss: 0.000359\n",
            "Epoch 380 | Train Loss: 0.000292 | Validation Loss: 0.000292\n",
            "Epoch 381 | Train Loss: 0.000322 | Validation Loss: 0.000282\n",
            "Epoch 382 | Train Loss: 0.000302 | Validation Loss: 0.000280\n",
            "Epoch 383 | Train Loss: 0.000309 | Validation Loss: 0.000308\n",
            "Epoch 384 | Train Loss: 0.000289 | Validation Loss: 0.000309\n",
            "Epoch 385 | Train Loss: 0.000336 | Validation Loss: 0.000284\n",
            "Epoch 386 | Train Loss: 0.000377 | Validation Loss: 0.000325\n",
            "Epoch 387 | Train Loss: 0.000318 | Validation Loss: 0.000314\n",
            "Epoch 388 | Train Loss: 0.000311 | Validation Loss: 0.000294\n",
            "Epoch 389 | Train Loss: 0.000304 | Validation Loss: 0.000283\n",
            "Epoch 390 | Train Loss: 0.000297 | Validation Loss: 0.000295\n",
            "Epoch 391 | Train Loss: 0.000311 | Validation Loss: 0.000297\n",
            "Epoch 392 | Train Loss: 0.000326 | Validation Loss: 0.000301\n",
            "Epoch 393 | Train Loss: 0.000313 | Validation Loss: 0.000309\n",
            "Epoch 394 | Train Loss: 0.000325 | Validation Loss: 0.000302\n",
            "Epoch 395 | Train Loss: 0.000299 | Validation Loss: 0.000285\n",
            "Epoch 396 | Train Loss: 0.000352 | Validation Loss: 0.000312\n",
            "Epoch 397 | Train Loss: 0.000338 | Validation Loss: 0.000309\n",
            "Epoch 398 | Train Loss: 0.000346 | Validation Loss: 0.000297\n",
            "Epoch 399 | Train Loss: 0.000339 | Validation Loss: 0.000325\n",
            "Epoch 400 | Train Loss: 0.000310 | Validation Loss: 0.000298\n",
            "Epoch 401 | Train Loss: 0.000301 | Validation Loss: 0.000324\n",
            "Epoch 402 | Train Loss: 0.000329 | Validation Loss: 0.000310\n",
            "Epoch 403 | Train Loss: 0.000315 | Validation Loss: 0.000314\n",
            "Epoch 404 | Train Loss: 0.000305 | Validation Loss: 0.000293\n",
            "Epoch 405 | Train Loss: 0.000292 | Validation Loss: 0.000301\n",
            "Epoch 406 | Train Loss: 0.000280 | Validation Loss: 0.000303\n",
            "Epoch 407 | Train Loss: 0.000317 | Validation Loss: 0.000307\n",
            "Epoch 408 | Train Loss: 0.000322 | Validation Loss: 0.000347\n",
            "Epoch 409 | Train Loss: 0.000326 | Validation Loss: 0.000331\n",
            "Epoch 410 | Train Loss: 0.000331 | Validation Loss: 0.000310\n",
            "Epoch 411 | Train Loss: 0.000366 | Validation Loss: 0.000304\n",
            "Epoch 412 | Train Loss: 0.000338 | Validation Loss: 0.000288\n",
            "Epoch 413 | Train Loss: 0.000316 | Validation Loss: 0.000311\n",
            "Epoch 414 | Train Loss: 0.000324 | Validation Loss: 0.000314\n",
            "Epoch 415 | Train Loss: 0.000330 | Validation Loss: 0.000308\n",
            "Epoch 416 | Train Loss: 0.000332 | Validation Loss: 0.000311\n",
            "Epoch 417 | Train Loss: 0.000295 | Validation Loss: 0.000302\n",
            "Epoch 418 | Train Loss: 0.000299 | Validation Loss: 0.000314\n",
            "Epoch 419 | Train Loss: 0.000308 | Validation Loss: 0.000311\n",
            "Epoch 420 | Train Loss: 0.000320 | Validation Loss: 0.000323\n",
            "Epoch 421 | Train Loss: 0.000318 | Validation Loss: 0.000318\n",
            "Epoch 422 | Train Loss: 0.000367 | Validation Loss: 0.000298\n",
            "Epoch 423 | Train Loss: 0.000321 | Validation Loss: 0.000309\n",
            "Epoch 424 | Train Loss: 0.000363 | Validation Loss: 0.000321\n",
            "Epoch 425 | Train Loss: 0.000332 | Validation Loss: 0.000306\n",
            "Epoch 426 | Train Loss: 0.000323 | Validation Loss: 0.000356\n",
            "Epoch 427 | Train Loss: 0.000317 | Validation Loss: 0.000323\n",
            "Epoch 428 | Train Loss: 0.000326 | Validation Loss: 0.000322\n",
            "Epoch 429 | Train Loss: 0.000362 | Validation Loss: 0.000309\n",
            "Epoch 430 | Train Loss: 0.000321 | Validation Loss: 0.000296\n",
            "Epoch 431 | Train Loss: 0.000307 | Validation Loss: 0.000362\n",
            "Epoch 432 | Train Loss: 0.000344 | Validation Loss: 0.000315\n",
            "Epoch 433 | Train Loss: 0.000333 | Validation Loss: 0.000306\n",
            "Epoch 434 | Train Loss: 0.000309 | Validation Loss: 0.000292\n",
            "Epoch 435 | Train Loss: 0.000295 | Validation Loss: 0.000283\n",
            "Epoch 436 | Train Loss: 0.000339 | Validation Loss: 0.000309\n",
            "Epoch 437 | Train Loss: 0.000414 | Validation Loss: 0.000351\n",
            "Epoch 438 | Train Loss: 0.000318 | Validation Loss: 0.000312\n",
            "Epoch 439 | Train Loss: 0.000306 | Validation Loss: 0.000302\n",
            "Epoch 440 | Train Loss: 0.000319 | Validation Loss: 0.000309\n",
            "Epoch 441 | Train Loss: 0.000316 | Validation Loss: 0.000312\n",
            "Epoch 442 | Train Loss: 0.000294 | Validation Loss: 0.000284\n",
            "Epoch 443 | Train Loss: 0.000289 | Validation Loss: 0.000297\n",
            "Epoch 444 | Train Loss: 0.000272 | Validation Loss: 0.000289\n",
            "Epoch 445 | Train Loss: 0.000320 | Validation Loss: 0.000369\n",
            "Epoch 446 | Train Loss: 0.000311 | Validation Loss: 0.000300\n",
            "Epoch 447 | Train Loss: 0.000280 | Validation Loss: 0.000309\n",
            "Epoch 448 | Train Loss: 0.000277 | Validation Loss: 0.000292\n",
            "Epoch 449 | Train Loss: 0.000290 | Validation Loss: 0.000303\n",
            "Epoch 450 | Train Loss: 0.000302 | Validation Loss: 0.000298\n",
            "Epoch 451 | Train Loss: 0.000289 | Validation Loss: 0.000291\n",
            "Epoch 452 | Train Loss: 0.000294 | Validation Loss: 0.000300\n",
            "Epoch 453 | Train Loss: 0.000296 | Validation Loss: 0.000327\n",
            "Epoch 454 | Train Loss: 0.000307 | Validation Loss: 0.000314\n",
            "Epoch 455 | Train Loss: 0.000316 | Validation Loss: 0.000321\n",
            "Epoch 456 | Train Loss: 0.000315 | Validation Loss: 0.000307\n",
            "Epoch 457 | Train Loss: 0.000290 | Validation Loss: 0.000290\n",
            "Epoch 458 | Train Loss: 0.000353 | Validation Loss: 0.000359\n",
            "Epoch 459 | Train Loss: 0.000329 | Validation Loss: 0.000288\n",
            "Epoch 460 | Train Loss: 0.000311 | Validation Loss: 0.000302\n",
            "Epoch 461 | Train Loss: 0.000313 | Validation Loss: 0.000281\n",
            "Epoch 462 | Train Loss: 0.000286 | Validation Loss: 0.000285\n",
            "Epoch 463 | Train Loss: 0.000299 | Validation Loss: 0.000290\n",
            "Epoch 464 | Train Loss: 0.000329 | Validation Loss: 0.000318\n",
            "Epoch 465 | Train Loss: 0.000302 | Validation Loss: 0.000306\n",
            "Epoch 466 | Train Loss: 0.000308 | Validation Loss: 0.000302\n",
            "Epoch 467 | Train Loss: 0.000292 | Validation Loss: 0.000296\n",
            "Epoch 468 | Train Loss: 0.000292 | Validation Loss: 0.000279\n",
            "Epoch 469 | Train Loss: 0.000295 | Validation Loss: 0.000294\n",
            "Epoch 470 | Train Loss: 0.000309 | Validation Loss: 0.000299\n",
            "Epoch 471 | Train Loss: 0.000306 | Validation Loss: 0.000334\n",
            "Epoch 472 | Train Loss: 0.000300 | Validation Loss: 0.000293\n",
            "Epoch 473 | Train Loss: 0.000283 | Validation Loss: 0.000297\n",
            "Epoch 474 | Train Loss: 0.000316 | Validation Loss: 0.000293\n",
            "Epoch 475 | Train Loss: 0.000293 | Validation Loss: 0.000287\n",
            "Epoch 476 | Train Loss: 0.000297 | Validation Loss: 0.000298\n",
            "Epoch 477 | Train Loss: 0.000313 | Validation Loss: 0.000336\n",
            "Epoch 478 | Train Loss: 0.000300 | Validation Loss: 0.000302\n",
            "Epoch 479 | Train Loss: 0.000317 | Validation Loss: 0.000300\n",
            "Epoch 480 | Train Loss: 0.000289 | Validation Loss: 0.000294\n",
            "Epoch 481 | Train Loss: 0.000308 | Validation Loss: 0.000296\n",
            "Epoch 482 | Train Loss: 0.000314 | Validation Loss: 0.000294\n",
            "Epoch 483 | Train Loss: 0.000312 | Validation Loss: 0.000288\n",
            "Epoch 484 | Train Loss: 0.000304 | Validation Loss: 0.000317\n",
            "Epoch 485 | Train Loss: 0.000307 | Validation Loss: 0.000295\n",
            "Epoch 486 | Train Loss: 0.000287 | Validation Loss: 0.000295\n",
            "Epoch 487 | Train Loss: 0.000286 | Validation Loss: 0.000289\n",
            "Epoch 488 | Train Loss: 0.000326 | Validation Loss: 0.000307\n",
            "Epoch 489 | Train Loss: 0.000305 | Validation Loss: 0.000314\n",
            "Epoch 490 | Train Loss: 0.000326 | Validation Loss: 0.000344\n",
            "Epoch 491 | Train Loss: 0.000297 | Validation Loss: 0.000299\n",
            "Epoch 492 | Train Loss: 0.000309 | Validation Loss: 0.000297\n",
            "Epoch 493 | Train Loss: 0.000320 | Validation Loss: 0.000303\n",
            "Epoch 494 | Train Loss: 0.000317 | Validation Loss: 0.000310\n",
            "Epoch 495 | Train Loss: 0.000346 | Validation Loss: 0.000306\n",
            "Epoch 496 | Train Loss: 0.000303 | Validation Loss: 0.000297\n",
            "Epoch 497 | Train Loss: 0.000295 | Validation Loss: 0.000287\n",
            "Epoch 498 | Train Loss: 0.000261 | Validation Loss: 0.000289\n",
            "Epoch 499 | Train Loss: 0.000294 | Validation Loss: 0.000288\n",
            "Epoch 500 | Train Loss: 0.000289 | Validation Loss: 0.000312\n",
            "Epoch 501 | Train Loss: 0.000310 | Validation Loss: 0.000308\n",
            "Epoch 502 | Train Loss: 0.000301 | Validation Loss: 0.000287\n",
            "Epoch 503 | Train Loss: 0.000344 | Validation Loss: 0.000339\n",
            "Epoch 504 | Train Loss: 0.000292 | Validation Loss: 0.000292\n",
            "Epoch 505 | Train Loss: 0.000279 | Validation Loss: 0.000271\n",
            "Epoch 506 | Train Loss: 0.000274 | Validation Loss: 0.000291\n",
            "Epoch 507 | Train Loss: 0.000291 | Validation Loss: 0.000293\n",
            "Epoch 508 | Train Loss: 0.000343 | Validation Loss: 0.000329\n",
            "Epoch 509 | Train Loss: 0.000288 | Validation Loss: 0.000296\n",
            "Epoch 510 | Train Loss: 0.000295 | Validation Loss: 0.000290\n",
            "Epoch 511 | Train Loss: 0.000287 | Validation Loss: 0.000284\n",
            "Epoch 512 | Train Loss: 0.000288 | Validation Loss: 0.000272\n",
            "Epoch 513 | Train Loss: 0.000301 | Validation Loss: 0.000298\n",
            "Epoch 514 | Train Loss: 0.000282 | Validation Loss: 0.000286\n",
            "Epoch 515 | Train Loss: 0.000271 | Validation Loss: 0.000287\n",
            "Epoch 516 | Train Loss: 0.000281 | Validation Loss: 0.000282\n",
            "Epoch 517 | Train Loss: 0.000281 | Validation Loss: 0.000299\n",
            "Epoch 518 | Train Loss: 0.000298 | Validation Loss: 0.000312\n",
            "Epoch 519 | Train Loss: 0.000298 | Validation Loss: 0.000316\n",
            "Epoch 520 | Train Loss: 0.000311 | Validation Loss: 0.000291\n",
            "Epoch 521 | Train Loss: 0.000303 | Validation Loss: 0.000307\n",
            "Epoch 522 | Train Loss: 0.000298 | Validation Loss: 0.000294\n",
            "Epoch 523 | Train Loss: 0.000297 | Validation Loss: 0.000298\n",
            "Epoch 524 | Train Loss: 0.000317 | Validation Loss: 0.000291\n",
            "Epoch 525 | Train Loss: 0.000297 | Validation Loss: 0.000280\n",
            "Epoch 526 | Train Loss: 0.000283 | Validation Loss: 0.000298\n",
            "Epoch 527 | Train Loss: 0.000287 | Validation Loss: 0.000288\n",
            "Epoch 528 | Train Loss: 0.000293 | Validation Loss: 0.000289\n",
            "Epoch 529 | Train Loss: 0.000321 | Validation Loss: 0.000294\n",
            "Epoch 530 | Train Loss: 0.000280 | Validation Loss: 0.000298\n",
            "Epoch 531 | Train Loss: 0.000287 | Validation Loss: 0.000294\n",
            "Epoch 532 | Train Loss: 0.000276 | Validation Loss: 0.000298\n",
            "Epoch 533 | Train Loss: 0.000288 | Validation Loss: 0.000309\n",
            "Epoch 534 | Train Loss: 0.000297 | Validation Loss: 0.000272\n",
            "Epoch 535 | Train Loss: 0.000281 | Validation Loss: 0.000299\n",
            "Epoch 536 | Train Loss: 0.000294 | Validation Loss: 0.000302\n",
            "Epoch 537 | Train Loss: 0.000269 | Validation Loss: 0.000275\n",
            "Epoch 538 | Train Loss: 0.000304 | Validation Loss: 0.000331\n",
            "Epoch 539 | Train Loss: 0.000316 | Validation Loss: 0.000302\n",
            "Epoch 540 | Train Loss: 0.000292 | Validation Loss: 0.000307\n",
            "Epoch 541 | Train Loss: 0.000304 | Validation Loss: 0.000326\n",
            "Epoch 542 | Train Loss: 0.000304 | Validation Loss: 0.000288\n",
            "Epoch 543 | Train Loss: 0.000342 | Validation Loss: 0.000300\n",
            "Epoch 544 | Train Loss: 0.000310 | Validation Loss: 0.000302\n",
            "Epoch 545 | Train Loss: 0.000313 | Validation Loss: 0.000301\n",
            "Epoch 546 | Train Loss: 0.000313 | Validation Loss: 0.000297\n",
            "Epoch 547 | Train Loss: 0.000332 | Validation Loss: 0.000310\n",
            "Epoch 548 | Train Loss: 0.000290 | Validation Loss: 0.000350\n",
            "Epoch 549 | Train Loss: 0.000286 | Validation Loss: 0.000316\n",
            "Epoch 550 | Train Loss: 0.000297 | Validation Loss: 0.000292\n",
            "Epoch 551 | Train Loss: 0.000310 | Validation Loss: 0.000335\n",
            "Epoch 552 | Train Loss: 0.000280 | Validation Loss: 0.000312\n",
            "Epoch 553 | Train Loss: 0.000306 | Validation Loss: 0.000308\n",
            "Epoch 554 | Train Loss: 0.000315 | Validation Loss: 0.000303\n",
            "Epoch 555 | Train Loss: 0.000281 | Validation Loss: 0.000297\n",
            "Epoch 556 | Train Loss: 0.000283 | Validation Loss: 0.000310\n",
            "Epoch 557 | Train Loss: 0.000312 | Validation Loss: 0.000317\n",
            "Epoch 558 | Train Loss: 0.000314 | Validation Loss: 0.000278\n",
            "Epoch 559 | Train Loss: 0.000292 | Validation Loss: 0.000294\n",
            "Epoch 560 | Train Loss: 0.000322 | Validation Loss: 0.000309\n",
            "Epoch 561 | Train Loss: 0.000314 | Validation Loss: 0.000302\n",
            "Epoch 562 | Train Loss: 0.000296 | Validation Loss: 0.000295\n",
            "Epoch 563 | Train Loss: 0.000306 | Validation Loss: 0.000299\n",
            "Epoch 564 | Train Loss: 0.000292 | Validation Loss: 0.000328\n",
            "Epoch 565 | Train Loss: 0.000322 | Validation Loss: 0.000292\n",
            "Epoch 566 | Train Loss: 0.000282 | Validation Loss: 0.000294\n",
            "Epoch 567 | Train Loss: 0.000286 | Validation Loss: 0.000292\n",
            "Epoch 568 | Train Loss: 0.000305 | Validation Loss: 0.000309\n",
            "Epoch 569 | Train Loss: 0.000330 | Validation Loss: 0.000298\n",
            "Epoch 570 | Train Loss: 0.000304 | Validation Loss: 0.000298\n",
            "Epoch 571 | Train Loss: 0.000286 | Validation Loss: 0.000283\n",
            "Epoch 572 | Train Loss: 0.000291 | Validation Loss: 0.000280\n",
            "Epoch 573 | Train Loss: 0.000300 | Validation Loss: 0.000287\n",
            "Epoch 574 | Train Loss: 0.000278 | Validation Loss: 0.000279\n",
            "Epoch 575 | Train Loss: 0.000271 | Validation Loss: 0.000281\n",
            "Epoch 576 | Train Loss: 0.000295 | Validation Loss: 0.000303\n",
            "Epoch 577 | Train Loss: 0.000286 | Validation Loss: 0.000288\n",
            "Epoch 578 | Train Loss: 0.000276 | Validation Loss: 0.000325\n",
            "Epoch 579 | Train Loss: 0.000335 | Validation Loss: 0.000288\n",
            "Epoch 580 | Train Loss: 0.000290 | Validation Loss: 0.000291\n",
            "Epoch 581 | Train Loss: 0.000295 | Validation Loss: 0.000275\n",
            "Epoch 582 | Train Loss: 0.000290 | Validation Loss: 0.000309\n",
            "Epoch 583 | Train Loss: 0.000293 | Validation Loss: 0.000295\n",
            "Epoch 584 | Train Loss: 0.000280 | Validation Loss: 0.000322\n",
            "Epoch 585 | Train Loss: 0.000325 | Validation Loss: 0.000352\n",
            "Epoch 586 | Train Loss: 0.000313 | Validation Loss: 0.000305\n",
            "Epoch 587 | Train Loss: 0.000298 | Validation Loss: 0.000284\n",
            "Epoch 588 | Train Loss: 0.000278 | Validation Loss: 0.000293\n",
            "Epoch 589 | Train Loss: 0.000279 | Validation Loss: 0.000285\n",
            "Epoch 590 | Train Loss: 0.000308 | Validation Loss: 0.000303\n",
            "Epoch 591 | Train Loss: 0.000318 | Validation Loss: 0.000325\n",
            "Epoch 592 | Train Loss: 0.000312 | Validation Loss: 0.000295\n",
            "Epoch 593 | Train Loss: 0.000321 | Validation Loss: 0.000349\n",
            "Epoch 594 | Train Loss: 0.000293 | Validation Loss: 0.000301\n",
            "Epoch 595 | Train Loss: 0.000318 | Validation Loss: 0.000287\n",
            "Epoch 596 | Train Loss: 0.000326 | Validation Loss: 0.000292\n",
            "Epoch 597 | Train Loss: 0.000316 | Validation Loss: 0.000290\n",
            "Epoch 598 | Train Loss: 0.000317 | Validation Loss: 0.000314\n",
            "Epoch 599 | Train Loss: 0.000315 | Validation Loss: 0.000292\n",
            "Epoch 600 | Train Loss: 0.000282 | Validation Loss: 0.000303\n",
            "Epoch 601 | Train Loss: 0.000276 | Validation Loss: 0.000289\n",
            "Epoch 602 | Train Loss: 0.000291 | Validation Loss: 0.000312\n",
            "Epoch 603 | Train Loss: 0.000286 | Validation Loss: 0.000286\n",
            "Epoch 604 | Train Loss: 0.000295 | Validation Loss: 0.000293\n",
            "Epoch 605 | Train Loss: 0.000282 | Validation Loss: 0.000282\n",
            "Epoch 606 | Train Loss: 0.000279 | Validation Loss: 0.000281\n",
            "Epoch 607 | Train Loss: 0.000258 | Validation Loss: 0.000279\n",
            "Epoch 608 | Train Loss: 0.000271 | Validation Loss: 0.000290\n",
            "Epoch 609 | Train Loss: 0.000315 | Validation Loss: 0.000485\n",
            "Epoch 610 | Train Loss: 0.000324 | Validation Loss: 0.000318\n",
            "Epoch 611 | Train Loss: 0.000316 | Validation Loss: 0.000316\n",
            "Epoch 612 | Train Loss: 0.000298 | Validation Loss: 0.000295\n",
            "Epoch 613 | Train Loss: 0.000284 | Validation Loss: 0.000335\n",
            "Epoch 614 | Train Loss: 0.000293 | Validation Loss: 0.000377\n",
            "Epoch 615 | Train Loss: 0.000283 | Validation Loss: 0.000336\n",
            "Epoch 616 | Train Loss: 0.000291 | Validation Loss: 0.000283\n",
            "Epoch 617 | Train Loss: 0.000284 | Validation Loss: 0.000307\n",
            "Epoch 618 | Train Loss: 0.000276 | Validation Loss: 0.000299\n",
            "Epoch 619 | Train Loss: 0.000290 | Validation Loss: 0.000405\n",
            "Epoch 620 | Train Loss: 0.000280 | Validation Loss: 0.000284\n",
            "Epoch 621 | Train Loss: 0.000284 | Validation Loss: 0.000318\n",
            "Epoch 622 | Train Loss: 0.000277 | Validation Loss: 0.000275\n",
            "Epoch 623 | Train Loss: 0.000299 | Validation Loss: 0.000326\n",
            "Epoch 624 | Train Loss: 0.000293 | Validation Loss: 0.000280\n",
            "Epoch 625 | Train Loss: 0.000312 | Validation Loss: 0.000329\n",
            "Epoch 626 | Train Loss: 0.000330 | Validation Loss: 0.000311\n",
            "Epoch 627 | Train Loss: 0.000337 | Validation Loss: 0.000294\n",
            "Epoch 628 | Train Loss: 0.000347 | Validation Loss: 0.000348\n",
            "Epoch 629 | Train Loss: 0.000285 | Validation Loss: 0.000343\n",
            "Epoch 630 | Train Loss: 0.000278 | Validation Loss: 0.000313\n",
            "Epoch 631 | Train Loss: 0.000289 | Validation Loss: 0.000409\n",
            "Epoch 632 | Train Loss: 0.000317 | Validation Loss: 0.000286\n",
            "Epoch 633 | Train Loss: 0.000294 | Validation Loss: 0.000299\n",
            "Epoch 634 | Train Loss: 0.000319 | Validation Loss: 0.000354\n",
            "Epoch 635 | Train Loss: 0.000333 | Validation Loss: 0.000311\n",
            "Epoch 636 | Train Loss: 0.000274 | Validation Loss: 0.000282\n",
            "Epoch 637 | Train Loss: 0.000280 | Validation Loss: 0.000310\n",
            "Epoch 638 | Train Loss: 0.000287 | Validation Loss: 0.000309\n",
            "Epoch 639 | Train Loss: 0.000289 | Validation Loss: 0.000287\n",
            "Epoch 640 | Train Loss: 0.000281 | Validation Loss: 0.000311\n",
            "Epoch 641 | Train Loss: 0.000291 | Validation Loss: 0.000278\n",
            "Epoch 642 | Train Loss: 0.000280 | Validation Loss: 0.000281\n",
            "Epoch 643 | Train Loss: 0.000282 | Validation Loss: 0.000314\n",
            "Epoch 644 | Train Loss: 0.000289 | Validation Loss: 0.000330\n",
            "Epoch 645 | Train Loss: 0.000305 | Validation Loss: 0.000301\n",
            "Epoch 646 | Train Loss: 0.000293 | Validation Loss: 0.000296\n",
            "Epoch 647 | Train Loss: 0.000277 | Validation Loss: 0.000285\n",
            "Epoch 648 | Train Loss: 0.000281 | Validation Loss: 0.000287\n",
            "Epoch 649 | Train Loss: 0.000282 | Validation Loss: 0.000294\n",
            "Epoch 650 | Train Loss: 0.000297 | Validation Loss: 0.000305\n",
            "Epoch 651 | Train Loss: 0.000285 | Validation Loss: 0.000293\n",
            "Epoch 652 | Train Loss: 0.000303 | Validation Loss: 0.000320\n",
            "Epoch 653 | Train Loss: 0.000275 | Validation Loss: 0.000289\n",
            "Epoch 654 | Train Loss: 0.000267 | Validation Loss: 0.000286\n",
            "Epoch 655 | Train Loss: 0.000288 | Validation Loss: 0.000291\n",
            "Epoch 656 | Train Loss: 0.000288 | Validation Loss: 0.000327\n",
            "Epoch 657 | Train Loss: 0.000286 | Validation Loss: 0.000306\n",
            "Epoch 658 | Train Loss: 0.000304 | Validation Loss: 0.000302\n",
            "Epoch 659 | Train Loss: 0.000294 | Validation Loss: 0.000287\n",
            "Epoch 660 | Train Loss: 0.000268 | Validation Loss: 0.000304\n",
            "Epoch 661 | Train Loss: 0.000273 | Validation Loss: 0.000294\n",
            "Epoch 662 | Train Loss: 0.000288 | Validation Loss: 0.000279\n",
            "Epoch 663 | Train Loss: 0.000292 | Validation Loss: 0.000303\n",
            "Epoch 664 | Train Loss: 0.000280 | Validation Loss: 0.000269\n",
            "Epoch 665 | Train Loss: 0.000310 | Validation Loss: 0.000344\n",
            "Epoch 666 | Train Loss: 0.000276 | Validation Loss: 0.000292\n",
            "Epoch 667 | Train Loss: 0.000286 | Validation Loss: 0.000285\n",
            "Epoch 668 | Train Loss: 0.000284 | Validation Loss: 0.000290\n",
            "Epoch 669 | Train Loss: 0.000281 | Validation Loss: 0.000300\n",
            "Epoch 670 | Train Loss: 0.000291 | Validation Loss: 0.000290\n",
            "Epoch 671 | Train Loss: 0.000278 | Validation Loss: 0.000271\n",
            "Epoch 672 | Train Loss: 0.000300 | Validation Loss: 0.000308\n",
            "Epoch 673 | Train Loss: 0.000283 | Validation Loss: 0.000281\n",
            "Epoch 674 | Train Loss: 0.000308 | Validation Loss: 0.000330\n",
            "Epoch 675 | Train Loss: 0.000303 | Validation Loss: 0.000292\n",
            "Epoch 676 | Train Loss: 0.000302 | Validation Loss: 0.000315\n",
            "Epoch 677 | Train Loss: 0.000294 | Validation Loss: 0.000308\n",
            "Epoch 678 | Train Loss: 0.000283 | Validation Loss: 0.000282\n",
            "Epoch 679 | Train Loss: 0.000284 | Validation Loss: 0.000278\n",
            "Epoch 680 | Train Loss: 0.000294 | Validation Loss: 0.000334\n",
            "Epoch 681 | Train Loss: 0.000312 | Validation Loss: 0.000326\n",
            "Epoch 682 | Train Loss: 0.000300 | Validation Loss: 0.000301\n",
            "Epoch 683 | Train Loss: 0.000285 | Validation Loss: 0.000311\n",
            "Epoch 684 | Train Loss: 0.000328 | Validation Loss: 0.000287\n",
            "Epoch 685 | Train Loss: 0.000286 | Validation Loss: 0.000299\n",
            "Epoch 686 | Train Loss: 0.000277 | Validation Loss: 0.000333\n",
            "Epoch 687 | Train Loss: 0.000311 | Validation Loss: 0.000303\n",
            "Epoch 688 | Train Loss: 0.000275 | Validation Loss: 0.000306\n",
            "Epoch 689 | Train Loss: 0.000287 | Validation Loss: 0.000291\n",
            "Epoch 690 | Train Loss: 0.000292 | Validation Loss: 0.000280\n",
            "Epoch 691 | Train Loss: 0.000284 | Validation Loss: 0.000291\n",
            "Epoch 692 | Train Loss: 0.000296 | Validation Loss: 0.000293\n",
            "Epoch 693 | Train Loss: 0.000274 | Validation Loss: 0.000284\n",
            "Epoch 694 | Train Loss: 0.000276 | Validation Loss: 0.000312\n",
            "Epoch 695 | Train Loss: 0.000285 | Validation Loss: 0.000284\n",
            "Epoch 696 | Train Loss: 0.000281 | Validation Loss: 0.000302\n",
            "Epoch 697 | Train Loss: 0.000272 | Validation Loss: 0.000297\n",
            "Epoch 698 | Train Loss: 0.000280 | Validation Loss: 0.000306\n",
            "Epoch 699 | Train Loss: 0.000311 | Validation Loss: 0.000284\n",
            "Epoch 700 | Train Loss: 0.000280 | Validation Loss: 0.000300\n",
            "Epoch 701 | Train Loss: 0.000292 | Validation Loss: 0.000314\n",
            "Epoch 702 | Train Loss: 0.000287 | Validation Loss: 0.000295\n",
            "Epoch 703 | Train Loss: 0.000286 | Validation Loss: 0.000308\n",
            "Epoch 704 | Train Loss: 0.000271 | Validation Loss: 0.000290\n",
            "Epoch 705 | Train Loss: 0.000272 | Validation Loss: 0.000288\n",
            "Epoch 706 | Train Loss: 0.000292 | Validation Loss: 0.000296\n",
            "Epoch 707 | Train Loss: 0.000291 | Validation Loss: 0.000383\n",
            "Epoch 708 | Train Loss: 0.000282 | Validation Loss: 0.000308\n",
            "Epoch 709 | Train Loss: 0.000292 | Validation Loss: 0.000299\n",
            "Epoch 710 | Train Loss: 0.000290 | Validation Loss: 0.000318\n",
            "Epoch 711 | Train Loss: 0.000275 | Validation Loss: 0.000309\n",
            "Epoch 712 | Train Loss: 0.000281 | Validation Loss: 0.000309\n",
            "Epoch 713 | Train Loss: 0.000274 | Validation Loss: 0.000318\n",
            "Epoch 714 | Train Loss: 0.000319 | Validation Loss: 0.000453\n",
            "Epoch 715 | Train Loss: 0.000295 | Validation Loss: 0.000467\n",
            "Epoch 716 | Train Loss: 0.000283 | Validation Loss: 0.000351\n",
            "Epoch 717 | Train Loss: 0.000287 | Validation Loss: 0.000312\n",
            "Epoch 718 | Train Loss: 0.000286 | Validation Loss: 0.000479\n",
            "Epoch 719 | Train Loss: 0.000313 | Validation Loss: 0.000331\n",
            "Epoch 720 | Train Loss: 0.000297 | Validation Loss: 0.000360\n",
            "Epoch 721 | Train Loss: 0.000346 | Validation Loss: 0.000376\n",
            "Epoch 722 | Train Loss: 0.000314 | Validation Loss: 0.000331\n",
            "Epoch 723 | Train Loss: 0.000290 | Validation Loss: 0.000321\n",
            "Epoch 724 | Train Loss: 0.000303 | Validation Loss: 0.000295\n",
            "Epoch 725 | Train Loss: 0.000290 | Validation Loss: 0.000303\n",
            "Epoch 726 | Train Loss: 0.000288 | Validation Loss: 0.000300\n",
            "Epoch 727 | Train Loss: 0.000305 | Validation Loss: 0.000289\n",
            "Epoch 728 | Train Loss: 0.000284 | Validation Loss: 0.000312\n",
            "Epoch 729 | Train Loss: 0.000302 | Validation Loss: 0.000295\n",
            "Epoch 730 | Train Loss: 0.000297 | Validation Loss: 0.000280\n",
            "Epoch 731 | Train Loss: 0.000283 | Validation Loss: 0.000286\n",
            "Epoch 732 | Train Loss: 0.000283 | Validation Loss: 0.000309\n",
            "Epoch 733 | Train Loss: 0.000286 | Validation Loss: 0.000297\n",
            "Epoch 734 | Train Loss: 0.000294 | Validation Loss: 0.000296\n",
            "Epoch 735 | Train Loss: 0.000302 | Validation Loss: 0.000314\n",
            "Epoch 736 | Train Loss: 0.000269 | Validation Loss: 0.000308\n",
            "Epoch 737 | Train Loss: 0.000274 | Validation Loss: 0.000285\n",
            "Epoch 738 | Train Loss: 0.000281 | Validation Loss: 0.000310\n",
            "Epoch 739 | Train Loss: 0.000261 | Validation Loss: 0.000288\n",
            "Epoch 740 | Train Loss: 0.000282 | Validation Loss: 0.000302\n",
            "Epoch 741 | Train Loss: 0.000277 | Validation Loss: 0.000287\n",
            "Epoch 742 | Train Loss: 0.000267 | Validation Loss: 0.000290\n",
            "Epoch 743 | Train Loss: 0.000279 | Validation Loss: 0.000285\n",
            "Epoch 744 | Train Loss: 0.000263 | Validation Loss: 0.000286\n",
            "Epoch 745 | Train Loss: 0.000296 | Validation Loss: 0.000308\n",
            "Epoch 746 | Train Loss: 0.000272 | Validation Loss: 0.000308\n",
            "Epoch 747 | Train Loss: 0.000312 | Validation Loss: 0.000319\n",
            "Epoch 748 | Train Loss: 0.000296 | Validation Loss: 0.000285\n",
            "Epoch 749 | Train Loss: 0.000310 | Validation Loss: 0.000293\n",
            "Epoch 750 | Train Loss: 0.000296 | Validation Loss: 0.000296\n",
            "Epoch 751 | Train Loss: 0.000276 | Validation Loss: 0.000267\n",
            "Epoch 752 | Train Loss: 0.000274 | Validation Loss: 0.000286\n",
            "Epoch 753 | Train Loss: 0.000266 | Validation Loss: 0.000271\n",
            "Epoch 754 | Train Loss: 0.000278 | Validation Loss: 0.000296\n",
            "Epoch 755 | Train Loss: 0.000286 | Validation Loss: 0.000293\n",
            "Epoch 756 | Train Loss: 0.000291 | Validation Loss: 0.000300\n",
            "Epoch 757 | Train Loss: 0.000277 | Validation Loss: 0.000279\n",
            "Epoch 758 | Train Loss: 0.000284 | Validation Loss: 0.000300\n",
            "Epoch 759 | Train Loss: 0.000287 | Validation Loss: 0.000320\n",
            "Epoch 760 | Train Loss: 0.000280 | Validation Loss: 0.000291\n",
            "Epoch 761 | Train Loss: 0.000321 | Validation Loss: 0.000337\n",
            "Epoch 762 | Train Loss: 0.000323 | Validation Loss: 0.000322\n",
            "Epoch 763 | Train Loss: 0.000294 | Validation Loss: 0.000306\n",
            "Epoch 764 | Train Loss: 0.000287 | Validation Loss: 0.000318\n",
            "Epoch 765 | Train Loss: 0.000295 | Validation Loss: 0.000299\n",
            "Epoch 766 | Train Loss: 0.000311 | Validation Loss: 0.000312\n",
            "Epoch 767 | Train Loss: 0.000315 | Validation Loss: 0.000315\n",
            "Epoch 768 | Train Loss: 0.000285 | Validation Loss: 0.000313\n",
            "Epoch 769 | Train Loss: 0.000291 | Validation Loss: 0.000293\n",
            "Epoch 770 | Train Loss: 0.000295 | Validation Loss: 0.000287\n",
            "Epoch 771 | Train Loss: 0.000278 | Validation Loss: 0.000305\n",
            "Epoch 772 | Train Loss: 0.000294 | Validation Loss: 0.000311\n",
            "Epoch 773 | Train Loss: 0.000281 | Validation Loss: 0.000287\n",
            "Epoch 774 | Train Loss: 0.000293 | Validation Loss: 0.000315\n",
            "Epoch 775 | Train Loss: 0.000276 | Validation Loss: 0.000292\n",
            "Epoch 776 | Train Loss: 0.000289 | Validation Loss: 0.000301\n",
            "Epoch 777 | Train Loss: 0.000278 | Validation Loss: 0.000287\n",
            "Epoch 778 | Train Loss: 0.000286 | Validation Loss: 0.000292\n",
            "Epoch 779 | Train Loss: 0.000276 | Validation Loss: 0.000278\n",
            "Epoch 780 | Train Loss: 0.000254 | Validation Loss: 0.000278\n",
            "Epoch 781 | Train Loss: 0.000271 | Validation Loss: 0.000303\n",
            "Epoch 782 | Train Loss: 0.000270 | Validation Loss: 0.000309\n",
            "Epoch 783 | Train Loss: 0.000274 | Validation Loss: 0.000274\n",
            "Epoch 784 | Train Loss: 0.000261 | Validation Loss: 0.000300\n",
            "Epoch 785 | Train Loss: 0.000262 | Validation Loss: 0.000296\n",
            "Epoch 786 | Train Loss: 0.000268 | Validation Loss: 0.000301\n",
            "Epoch 787 | Train Loss: 0.000273 | Validation Loss: 0.000290\n",
            "Epoch 788 | Train Loss: 0.000279 | Validation Loss: 0.000302\n",
            "Epoch 789 | Train Loss: 0.000316 | Validation Loss: 0.000305\n",
            "Epoch 790 | Train Loss: 0.000349 | Validation Loss: 0.000364\n",
            "Epoch 791 | Train Loss: 0.000311 | Validation Loss: 0.000297\n",
            "Epoch 792 | Train Loss: 0.000293 | Validation Loss: 0.000287\n",
            "Epoch 793 | Train Loss: 0.000291 | Validation Loss: 0.000345\n",
            "Epoch 794 | Train Loss: 0.000320 | Validation Loss: 0.000304\n",
            "Epoch 795 | Train Loss: 0.000307 | Validation Loss: 0.000309\n",
            "Epoch 796 | Train Loss: 0.000289 | Validation Loss: 0.000307\n",
            "Epoch 797 | Train Loss: 0.000281 | Validation Loss: 0.000287\n",
            "Epoch 798 | Train Loss: 0.000266 | Validation Loss: 0.000290\n",
            "Epoch 799 | Train Loss: 0.000270 | Validation Loss: 0.000298\n",
            "Epoch 800 | Train Loss: 0.000281 | Validation Loss: 0.000298\n",
            "Epoch 801 | Train Loss: 0.000267 | Validation Loss: 0.000283\n",
            "Epoch 802 | Train Loss: 0.000282 | Validation Loss: 0.000292\n",
            "Epoch 803 | Train Loss: 0.000280 | Validation Loss: 0.000299\n",
            "Epoch 804 | Train Loss: 0.000253 | Validation Loss: 0.000284\n",
            "Epoch 805 | Train Loss: 0.000259 | Validation Loss: 0.000270\n",
            "Epoch 806 | Train Loss: 0.000264 | Validation Loss: 0.000282\n",
            "Epoch 807 | Train Loss: 0.000266 | Validation Loss: 0.000299\n",
            "Epoch 808 | Train Loss: 0.000272 | Validation Loss: 0.000295\n",
            "Epoch 809 | Train Loss: 0.000280 | Validation Loss: 0.000303\n",
            "Epoch 810 | Train Loss: 0.000276 | Validation Loss: 0.000289\n",
            "Epoch 811 | Train Loss: 0.000265 | Validation Loss: 0.000271\n",
            "Epoch 812 | Train Loss: 0.000285 | Validation Loss: 0.000291\n",
            "Epoch 813 | Train Loss: 0.000277 | Validation Loss: 0.000288\n",
            "Epoch 814 | Train Loss: 0.000273 | Validation Loss: 0.000286\n",
            "Epoch 815 | Train Loss: 0.000298 | Validation Loss: 0.000296\n",
            "Epoch 816 | Train Loss: 0.000278 | Validation Loss: 0.000291\n",
            "Epoch 817 | Train Loss: 0.000281 | Validation Loss: 0.000306\n",
            "Epoch 818 | Train Loss: 0.000274 | Validation Loss: 0.000318\n",
            "Epoch 819 | Train Loss: 0.000286 | Validation Loss: 0.000292\n",
            "Epoch 820 | Train Loss: 0.000287 | Validation Loss: 0.000295\n",
            "Epoch 821 | Train Loss: 0.000278 | Validation Loss: 0.000307\n",
            "Epoch 822 | Train Loss: 0.000272 | Validation Loss: 0.000291\n",
            "Epoch 823 | Train Loss: 0.000298 | Validation Loss: 0.000334\n",
            "Epoch 824 | Train Loss: 0.000292 | Validation Loss: 0.000309\n",
            "Epoch 825 | Train Loss: 0.000277 | Validation Loss: 0.000306\n",
            "Epoch 826 | Train Loss: 0.000271 | Validation Loss: 0.000275\n",
            "Epoch 827 | Train Loss: 0.000261 | Validation Loss: 0.000298\n",
            "Epoch 828 | Train Loss: 0.000273 | Validation Loss: 0.000287\n",
            "Epoch 829 | Train Loss: 0.000289 | Validation Loss: 0.000299\n",
            "Epoch 830 | Train Loss: 0.000302 | Validation Loss: 0.000345\n",
            "Epoch 831 | Train Loss: 0.000298 | Validation Loss: 0.000284\n",
            "Epoch 832 | Train Loss: 0.000313 | Validation Loss: 0.000323\n",
            "Epoch 833 | Train Loss: 0.000264 | Validation Loss: 0.000322\n",
            "Epoch 834 | Train Loss: 0.000265 | Validation Loss: 0.000286\n",
            "Epoch 835 | Train Loss: 0.000286 | Validation Loss: 0.000307\n",
            "Epoch 836 | Train Loss: 0.000284 | Validation Loss: 0.000296\n",
            "Epoch 837 | Train Loss: 0.000267 | Validation Loss: 0.000293\n",
            "Epoch 838 | Train Loss: 0.000255 | Validation Loss: 0.000277\n",
            "Epoch 839 | Train Loss: 0.000264 | Validation Loss: 0.000286\n",
            "Epoch 840 | Train Loss: 0.000277 | Validation Loss: 0.000283\n",
            "Epoch 841 | Train Loss: 0.000272 | Validation Loss: 0.000279\n",
            "Epoch 842 | Train Loss: 0.000272 | Validation Loss: 0.000300\n",
            "Epoch 843 | Train Loss: 0.000261 | Validation Loss: 0.000308\n",
            "Epoch 844 | Train Loss: 0.000273 | Validation Loss: 0.000306\n",
            "Epoch 845 | Train Loss: 0.000263 | Validation Loss: 0.000291\n",
            "Epoch 846 | Train Loss: 0.000279 | Validation Loss: 0.000275\n",
            "Epoch 847 | Train Loss: 0.000277 | Validation Loss: 0.000284\n",
            "Epoch 848 | Train Loss: 0.000259 | Validation Loss: 0.000288\n",
            "Epoch 849 | Train Loss: 0.000270 | Validation Loss: 0.000284\n",
            "Epoch 850 | Train Loss: 0.000272 | Validation Loss: 0.000294\n",
            "Epoch 851 | Train Loss: 0.000294 | Validation Loss: 0.000312\n",
            "Epoch 852 | Train Loss: 0.000285 | Validation Loss: 0.000294\n",
            "Epoch 853 | Train Loss: 0.000287 | Validation Loss: 0.000290\n",
            "Epoch 854 | Train Loss: 0.000286 | Validation Loss: 0.000288\n",
            "Epoch 855 | Train Loss: 0.000279 | Validation Loss: 0.000316\n",
            "Epoch 856 | Train Loss: 0.000276 | Validation Loss: 0.000305\n",
            "Epoch 857 | Train Loss: 0.000274 | Validation Loss: 0.000307\n",
            "Epoch 858 | Train Loss: 0.000273 | Validation Loss: 0.000303\n",
            "Epoch 859 | Train Loss: 0.000275 | Validation Loss: 0.000296\n",
            "Epoch 860 | Train Loss: 0.000274 | Validation Loss: 0.000298\n",
            "Epoch 861 | Train Loss: 0.000290 | Validation Loss: 0.000313\n",
            "Epoch 862 | Train Loss: 0.000289 | Validation Loss: 0.000309\n",
            "Epoch 863 | Train Loss: 0.000268 | Validation Loss: 0.000297\n",
            "Epoch 864 | Train Loss: 0.000282 | Validation Loss: 0.000298\n",
            "Epoch 865 | Train Loss: 0.000262 | Validation Loss: 0.000277\n",
            "Epoch 866 | Train Loss: 0.000262 | Validation Loss: 0.000289\n",
            "Epoch 867 | Train Loss: 0.000278 | Validation Loss: 0.000302\n",
            "Epoch 868 | Train Loss: 0.000270 | Validation Loss: 0.000288\n",
            "Epoch 869 | Train Loss: 0.000279 | Validation Loss: 0.000302\n",
            "Epoch 870 | Train Loss: 0.000260 | Validation Loss: 0.000288\n",
            "Epoch 871 | Train Loss: 0.000270 | Validation Loss: 0.000288\n",
            "Epoch 872 | Train Loss: 0.000266 | Validation Loss: 0.000288\n",
            "Epoch 873 | Train Loss: 0.000277 | Validation Loss: 0.000298\n",
            "Epoch 874 | Train Loss: 0.000286 | Validation Loss: 0.000283\n",
            "Epoch 875 | Train Loss: 0.000267 | Validation Loss: 0.000280\n",
            "Epoch 876 | Train Loss: 0.000260 | Validation Loss: 0.000288\n",
            "Epoch 877 | Train Loss: 0.000269 | Validation Loss: 0.000291\n",
            "Epoch 878 | Train Loss: 0.000272 | Validation Loss: 0.000291\n",
            "Epoch 879 | Train Loss: 0.000266 | Validation Loss: 0.000298\n",
            "Epoch 880 | Train Loss: 0.000290 | Validation Loss: 0.000299\n",
            "Epoch 881 | Train Loss: 0.000276 | Validation Loss: 0.000290\n",
            "Epoch 882 | Train Loss: 0.000274 | Validation Loss: 0.000277\n",
            "Epoch 883 | Train Loss: 0.000279 | Validation Loss: 0.000306\n",
            "Epoch 884 | Train Loss: 0.000275 | Validation Loss: 0.000296\n",
            "Epoch 885 | Train Loss: 0.000279 | Validation Loss: 0.000315\n",
            "Epoch 886 | Train Loss: 0.000278 | Validation Loss: 0.000314\n",
            "Epoch 887 | Train Loss: 0.000256 | Validation Loss: 0.000278\n",
            "Epoch 888 | Train Loss: 0.000254 | Validation Loss: 0.000281\n",
            "Epoch 889 | Train Loss: 0.000253 | Validation Loss: 0.000291\n",
            "Epoch 890 | Train Loss: 0.000297 | Validation Loss: 0.000355\n",
            "Epoch 891 | Train Loss: 0.000294 | Validation Loss: 0.000319\n",
            "Epoch 892 | Train Loss: 0.000282 | Validation Loss: 0.000343\n",
            "Epoch 893 | Train Loss: 0.000327 | Validation Loss: 0.000317\n",
            "Epoch 894 | Train Loss: 0.000284 | Validation Loss: 0.000325\n",
            "Epoch 895 | Train Loss: 0.000275 | Validation Loss: 0.000302\n",
            "Epoch 896 | Train Loss: 0.000271 | Validation Loss: 0.000285\n",
            "Epoch 897 | Train Loss: 0.000292 | Validation Loss: 0.000308\n",
            "Epoch 898 | Train Loss: 0.000266 | Validation Loss: 0.000286\n",
            "Epoch 899 | Train Loss: 0.000262 | Validation Loss: 0.000282\n",
            "Epoch 900 | Train Loss: 0.000287 | Validation Loss: 0.000280\n",
            "Epoch 901 | Train Loss: 0.000268 | Validation Loss: 0.000283\n",
            "Epoch 902 | Train Loss: 0.000273 | Validation Loss: 0.000288\n",
            "Epoch 903 | Train Loss: 0.000280 | Validation Loss: 0.000289\n",
            "Epoch 904 | Train Loss: 0.000275 | Validation Loss: 0.000272\n",
            "Epoch 905 | Train Loss: 0.000268 | Validation Loss: 0.000280\n",
            "Epoch 906 | Train Loss: 0.000260 | Validation Loss: 0.000277\n",
            "Epoch 907 | Train Loss: 0.000260 | Validation Loss: 0.000303\n",
            "Epoch 908 | Train Loss: 0.000269 | Validation Loss: 0.000300\n",
            "Epoch 909 | Train Loss: 0.000270 | Validation Loss: 0.000309\n",
            "Epoch 910 | Train Loss: 0.000271 | Validation Loss: 0.000306\n",
            "Epoch 911 | Train Loss: 0.000267 | Validation Loss: 0.000282\n",
            "Epoch 912 | Train Loss: 0.000290 | Validation Loss: 0.000317\n",
            "Epoch 913 | Train Loss: 0.000306 | Validation Loss: 0.000308\n",
            "Epoch 914 | Train Loss: 0.000292 | Validation Loss: 0.000316\n",
            "Epoch 915 | Train Loss: 0.000283 | Validation Loss: 0.000283\n",
            "Epoch 916 | Train Loss: 0.000272 | Validation Loss: 0.000295\n",
            "Epoch 917 | Train Loss: 0.000307 | Validation Loss: 0.000304\n",
            "Epoch 918 | Train Loss: 0.000275 | Validation Loss: 0.000290\n",
            "Epoch 919 | Train Loss: 0.000286 | Validation Loss: 0.000310\n",
            "Epoch 920 | Train Loss: 0.000276 | Validation Loss: 0.000287\n",
            "Epoch 921 | Train Loss: 0.000278 | Validation Loss: 0.000287\n",
            "Epoch 922 | Train Loss: 0.000288 | Validation Loss: 0.000316\n",
            "Epoch 923 | Train Loss: 0.000266 | Validation Loss: 0.000289\n",
            "Epoch 924 | Train Loss: 0.000259 | Validation Loss: 0.000296\n",
            "Epoch 925 | Train Loss: 0.000253 | Validation Loss: 0.000284\n",
            "Epoch 926 | Train Loss: 0.000255 | Validation Loss: 0.000289\n",
            "Epoch 927 | Train Loss: 0.000269 | Validation Loss: 0.000275\n",
            "Epoch 928 | Train Loss: 0.000272 | Validation Loss: 0.000297\n",
            "Epoch 929 | Train Loss: 0.000278 | Validation Loss: 0.000307\n",
            "Epoch 930 | Train Loss: 0.000267 | Validation Loss: 0.000286\n",
            "Epoch 931 | Train Loss: 0.000263 | Validation Loss: 0.000294\n",
            "Epoch 932 | Train Loss: 0.000255 | Validation Loss: 0.000282\n",
            "Epoch 933 | Train Loss: 0.000272 | Validation Loss: 0.000293\n",
            "Epoch 934 | Train Loss: 0.000259 | Validation Loss: 0.000296\n",
            "Epoch 935 | Train Loss: 0.000254 | Validation Loss: 0.000270\n",
            "Epoch 936 | Train Loss: 0.000247 | Validation Loss: 0.000298\n",
            "Epoch 937 | Train Loss: 0.000268 | Validation Loss: 0.000304\n",
            "Epoch 938 | Train Loss: 0.000252 | Validation Loss: 0.000304\n",
            "Epoch 939 | Train Loss: 0.000294 | Validation Loss: 0.000294\n",
            "Epoch 940 | Train Loss: 0.000287 | Validation Loss: 0.000295\n",
            "Epoch 941 | Train Loss: 0.000274 | Validation Loss: 0.000294\n",
            "Epoch 942 | Train Loss: 0.000275 | Validation Loss: 0.000291\n",
            "Epoch 943 | Train Loss: 0.000274 | Validation Loss: 0.000296\n",
            "Epoch 944 | Train Loss: 0.000260 | Validation Loss: 0.000286\n",
            "Epoch 945 | Train Loss: 0.000264 | Validation Loss: 0.000291\n",
            "Epoch 946 | Train Loss: 0.000253 | Validation Loss: 0.000269\n",
            "Epoch 947 | Train Loss: 0.000264 | Validation Loss: 0.000301\n",
            "Epoch 948 | Train Loss: 0.000262 | Validation Loss: 0.000290\n",
            "Epoch 949 | Train Loss: 0.000265 | Validation Loss: 0.000304\n",
            "Epoch 950 | Train Loss: 0.000328 | Validation Loss: 0.000393\n",
            "Epoch 951 | Train Loss: 0.000342 | Validation Loss: 0.000364\n",
            "Epoch 952 | Train Loss: 0.000303 | Validation Loss: 0.000300\n",
            "Epoch 953 | Train Loss: 0.000286 | Validation Loss: 0.000318\n",
            "Epoch 954 | Train Loss: 0.000335 | Validation Loss: 0.000345\n",
            "Epoch 955 | Train Loss: 0.000280 | Validation Loss: 0.000293\n",
            "Epoch 956 | Train Loss: 0.000271 | Validation Loss: 0.000308\n",
            "Epoch 957 | Train Loss: 0.000290 | Validation Loss: 0.000280\n",
            "Epoch 958 | Train Loss: 0.000264 | Validation Loss: 0.000278\n",
            "Epoch 959 | Train Loss: 0.000303 | Validation Loss: 0.000325\n",
            "Epoch 960 | Train Loss: 0.000282 | Validation Loss: 0.000308\n",
            "Epoch 961 | Train Loss: 0.000269 | Validation Loss: 0.000288\n",
            "Epoch 962 | Train Loss: 0.000267 | Validation Loss: 0.000278\n",
            "Epoch 963 | Train Loss: 0.000264 | Validation Loss: 0.000300\n",
            "Epoch 964 | Train Loss: 0.000269 | Validation Loss: 0.000309\n",
            "Epoch 965 | Train Loss: 0.000274 | Validation Loss: 0.000301\n",
            "Epoch 966 | Train Loss: 0.000267 | Validation Loss: 0.000269\n",
            "Epoch 967 | Train Loss: 0.000265 | Validation Loss: 0.000285\n",
            "Epoch 968 | Train Loss: 0.000269 | Validation Loss: 0.000300\n",
            "Epoch 969 | Train Loss: 0.000253 | Validation Loss: 0.000292\n",
            "Epoch 970 | Train Loss: 0.000267 | Validation Loss: 0.000285\n",
            "Epoch 971 | Train Loss: 0.000268 | Validation Loss: 0.000294\n",
            "Epoch 972 | Train Loss: 0.000252 | Validation Loss: 0.000304\n",
            "Epoch 973 | Train Loss: 0.000277 | Validation Loss: 0.000303\n",
            "Epoch 974 | Train Loss: 0.000256 | Validation Loss: 0.000289\n",
            "Epoch 975 | Train Loss: 0.000275 | Validation Loss: 0.000304\n",
            "Epoch 976 | Train Loss: 0.000287 | Validation Loss: 0.000312\n",
            "Epoch 977 | Train Loss: 0.000315 | Validation Loss: 0.000302\n",
            "Epoch 978 | Train Loss: 0.000265 | Validation Loss: 0.000299\n",
            "Epoch 979 | Train Loss: 0.000270 | Validation Loss: 0.000294\n",
            "Epoch 980 | Train Loss: 0.000267 | Validation Loss: 0.000281\n",
            "Epoch 981 | Train Loss: 0.000252 | Validation Loss: 0.000296\n",
            "Epoch 982 | Train Loss: 0.000267 | Validation Loss: 0.000290\n",
            "Epoch 983 | Train Loss: 0.000279 | Validation Loss: 0.000313\n",
            "Epoch 984 | Train Loss: 0.000291 | Validation Loss: 0.000332\n",
            "Epoch 985 | Train Loss: 0.000337 | Validation Loss: 0.000323\n",
            "Epoch 986 | Train Loss: 0.000277 | Validation Loss: 0.000295\n",
            "Epoch 987 | Train Loss: 0.000262 | Validation Loss: 0.000299\n",
            "Epoch 988 | Train Loss: 0.000278 | Validation Loss: 0.000312\n",
            "Epoch 989 | Train Loss: 0.000296 | Validation Loss: 0.000303\n",
            "Epoch 990 | Train Loss: 0.000276 | Validation Loss: 0.000282\n",
            "Epoch 991 | Train Loss: 0.000265 | Validation Loss: 0.000304\n",
            "Epoch 992 | Train Loss: 0.000291 | Validation Loss: 0.000336\n",
            "Epoch 993 | Train Loss: 0.000318 | Validation Loss: 0.000314\n",
            "Epoch 994 | Train Loss: 0.000314 | Validation Loss: 0.000344\n",
            "Epoch 995 | Train Loss: 0.000305 | Validation Loss: 0.000352\n",
            "Epoch 996 | Train Loss: 0.000275 | Validation Loss: 0.000278\n",
            "Epoch 997 | Train Loss: 0.000267 | Validation Loss: 0.000284\n",
            "Epoch 998 | Train Loss: 0.000263 | Validation Loss: 0.000309\n",
            "Epoch 999 | Train Loss: 0.000288 | Validation Loss: 0.000311\n",
            "Epoch 1000 | Train Loss: 0.000265 | Validation Loss: 0.000305\n",
            "Epoch 1001 | Train Loss: 0.000269 | Validation Loss: 0.000313\n",
            "Epoch 1002 | Train Loss: 0.000275 | Validation Loss: 0.000326\n",
            "Epoch 1003 | Train Loss: 0.000280 | Validation Loss: 0.000309\n",
            "Epoch 1004 | Train Loss: 0.000257 | Validation Loss: 0.000302\n",
            "Epoch 1005 | Train Loss: 0.000265 | Validation Loss: 0.000276\n",
            "Epoch 1006 | Train Loss: 0.000274 | Validation Loss: 0.000294\n",
            "Epoch 1007 | Train Loss: 0.000287 | Validation Loss: 0.000286\n",
            "Epoch 1008 | Train Loss: 0.000284 | Validation Loss: 0.000322\n",
            "Epoch 1009 | Train Loss: 0.000257 | Validation Loss: 0.000292\n",
            "Epoch 1010 | Train Loss: 0.000278 | Validation Loss: 0.000296\n",
            "Epoch 1011 | Train Loss: 0.000268 | Validation Loss: 0.000288\n",
            "Epoch 1012 | Train Loss: 0.000259 | Validation Loss: 0.000296\n",
            "Epoch 1013 | Train Loss: 0.000257 | Validation Loss: 0.000304\n",
            "Epoch 1014 | Train Loss: 0.000263 | Validation Loss: 0.000290\n",
            "Epoch 1015 | Train Loss: 0.000249 | Validation Loss: 0.000292\n",
            "Epoch 1016 | Train Loss: 0.000254 | Validation Loss: 0.000303\n",
            "Epoch 1017 | Train Loss: 0.000275 | Validation Loss: 0.000296\n",
            "Epoch 1018 | Train Loss: 0.000267 | Validation Loss: 0.000293\n",
            "Epoch 1019 | Train Loss: 0.000260 | Validation Loss: 0.000287\n",
            "Epoch 1020 | Train Loss: 0.000256 | Validation Loss: 0.000299\n",
            "Epoch 1021 | Train Loss: 0.000260 | Validation Loss: 0.000283\n",
            "Epoch 1022 | Train Loss: 0.000280 | Validation Loss: 0.000305\n",
            "Epoch 1023 | Train Loss: 0.000264 | Validation Loss: 0.000291\n",
            "Epoch 1024 | Train Loss: 0.000249 | Validation Loss: 0.000292\n",
            "Epoch 1025 | Train Loss: 0.000267 | Validation Loss: 0.000295\n",
            "Epoch 1026 | Train Loss: 0.000289 | Validation Loss: 0.000299\n",
            "Epoch 1027 | Train Loss: 0.000279 | Validation Loss: 0.000325\n",
            "Epoch 1028 | Train Loss: 0.000296 | Validation Loss: 0.000301\n",
            "Epoch 1029 | Train Loss: 0.000282 | Validation Loss: 0.000291\n",
            "Epoch 1030 | Train Loss: 0.000273 | Validation Loss: 0.000302\n",
            "Epoch 1031 | Train Loss: 0.000259 | Validation Loss: 0.000294\n",
            "Epoch 1032 | Train Loss: 0.000262 | Validation Loss: 0.000280\n",
            "Epoch 1033 | Train Loss: 0.000254 | Validation Loss: 0.000317\n",
            "Epoch 1034 | Train Loss: 0.000269 | Validation Loss: 0.000291\n",
            "Epoch 1035 | Train Loss: 0.000260 | Validation Loss: 0.000288\n",
            "Epoch 1036 | Train Loss: 0.000286 | Validation Loss: 0.000334\n",
            "Epoch 1037 | Train Loss: 0.000282 | Validation Loss: 0.000292\n",
            "Epoch 1038 | Train Loss: 0.000267 | Validation Loss: 0.000313\n",
            "Epoch 1039 | Train Loss: 0.000258 | Validation Loss: 0.000275\n",
            "Epoch 1040 | Train Loss: 0.000261 | Validation Loss: 0.000298\n",
            "Epoch 1041 | Train Loss: 0.000265 | Validation Loss: 0.000292\n",
            "Epoch 1042 | Train Loss: 0.000265 | Validation Loss: 0.000293\n",
            "Epoch 1043 | Train Loss: 0.000267 | Validation Loss: 0.000289\n",
            "Epoch 1044 | Train Loss: 0.000260 | Validation Loss: 0.000281\n",
            "Epoch 1045 | Train Loss: 0.000254 | Validation Loss: 0.000291\n",
            "Epoch 1046 | Train Loss: 0.000267 | Validation Loss: 0.000284\n",
            "Epoch 1047 | Train Loss: 0.000260 | Validation Loss: 0.000297\n",
            "Epoch 1048 | Train Loss: 0.000253 | Validation Loss: 0.000292\n",
            "Epoch 1049 | Train Loss: 0.000242 | Validation Loss: 0.000296\n",
            "Epoch 1050 | Train Loss: 0.000282 | Validation Loss: 0.000311\n",
            "Epoch 1051 | Train Loss: 0.000274 | Validation Loss: 0.000305\n",
            "Epoch 1052 | Train Loss: 0.000270 | Validation Loss: 0.000309\n",
            "Epoch 1053 | Train Loss: 0.000266 | Validation Loss: 0.000296\n",
            "Epoch 1054 | Train Loss: 0.000254 | Validation Loss: 0.000285\n",
            "Epoch 1055 | Train Loss: 0.000262 | Validation Loss: 0.000289\n",
            "Epoch 1056 | Train Loss: 0.000271 | Validation Loss: 0.000317\n",
            "Epoch 1057 | Train Loss: 0.000271 | Validation Loss: 0.000297\n",
            "Epoch 1058 | Train Loss: 0.000318 | Validation Loss: 0.000330\n",
            "Epoch 1059 | Train Loss: 0.000277 | Validation Loss: 0.000329\n",
            "Epoch 1060 | Train Loss: 0.000263 | Validation Loss: 0.000307\n",
            "Epoch 1061 | Train Loss: 0.000269 | Validation Loss: 0.000285\n",
            "Epoch 1062 | Train Loss: 0.000292 | Validation Loss: 0.000331\n",
            "Epoch 1063 | Train Loss: 0.000276 | Validation Loss: 0.000290\n",
            "Epoch 1064 | Train Loss: 0.000274 | Validation Loss: 0.000324\n",
            "Epoch 1065 | Train Loss: 0.000306 | Validation Loss: 0.000311\n",
            "Epoch 1066 | Train Loss: 0.000267 | Validation Loss: 0.000303\n",
            "Epoch 1067 | Train Loss: 0.000266 | Validation Loss: 0.000327\n",
            "Epoch 1068 | Train Loss: 0.000291 | Validation Loss: 0.000347\n",
            "Epoch 1069 | Train Loss: 0.000315 | Validation Loss: 0.000337\n",
            "Epoch 1070 | Train Loss: 0.000265 | Validation Loss: 0.000315\n",
            "Epoch 1071 | Train Loss: 0.000274 | Validation Loss: 0.000311\n",
            "Epoch 1072 | Train Loss: 0.000255 | Validation Loss: 0.000275\n",
            "Epoch 1073 | Train Loss: 0.000259 | Validation Loss: 0.000297\n",
            "Epoch 1074 | Train Loss: 0.000255 | Validation Loss: 0.000298\n",
            "Epoch 1075 | Train Loss: 0.000261 | Validation Loss: 0.000292\n",
            "Epoch 1076 | Train Loss: 0.000262 | Validation Loss: 0.000283\n",
            "Epoch 1077 | Train Loss: 0.000244 | Validation Loss: 0.000292\n",
            "Epoch 1078 | Train Loss: 0.000285 | Validation Loss: 0.000298\n",
            "Epoch 1079 | Train Loss: 0.000267 | Validation Loss: 0.000308\n",
            "Epoch 1080 | Train Loss: 0.000271 | Validation Loss: 0.000312\n",
            "Epoch 1081 | Train Loss: 0.000250 | Validation Loss: 0.000300\n",
            "Epoch 1082 | Train Loss: 0.000247 | Validation Loss: 0.000297\n",
            "Epoch 1083 | Train Loss: 0.000256 | Validation Loss: 0.000294\n",
            "Epoch 1084 | Train Loss: 0.000254 | Validation Loss: 0.000307\n",
            "Epoch 1085 | Train Loss: 0.000251 | Validation Loss: 0.000285\n",
            "Epoch 1086 | Train Loss: 0.000254 | Validation Loss: 0.000293\n",
            "Epoch 1087 | Train Loss: 0.000263 | Validation Loss: 0.000292\n",
            "Epoch 1088 | Train Loss: 0.000248 | Validation Loss: 0.000287\n",
            "Epoch 1089 | Train Loss: 0.000261 | Validation Loss: 0.000302\n",
            "Epoch 1090 | Train Loss: 0.000253 | Validation Loss: 0.000287\n",
            "Epoch 1091 | Train Loss: 0.000248 | Validation Loss: 0.000298\n",
            "Epoch 1092 | Train Loss: 0.000264 | Validation Loss: 0.000314\n",
            "Epoch 1093 | Train Loss: 0.000280 | Validation Loss: 0.000341\n",
            "Epoch 1094 | Train Loss: 0.000271 | Validation Loss: 0.000315\n",
            "Epoch 1095 | Train Loss: 0.000255 | Validation Loss: 0.000310\n",
            "Epoch 1096 | Train Loss: 0.000261 | Validation Loss: 0.000304\n",
            "Epoch 1097 | Train Loss: 0.000277 | Validation Loss: 0.000328\n",
            "Epoch 1098 | Train Loss: 0.000259 | Validation Loss: 0.000304\n",
            "Epoch 1099 | Train Loss: 0.000266 | Validation Loss: 0.000331\n",
            "Epoch 1100 | Train Loss: 0.000273 | Validation Loss: 0.000312\n",
            "Epoch 1101 | Train Loss: 0.000274 | Validation Loss: 0.000300\n",
            "Epoch 1102 | Train Loss: 0.000285 | Validation Loss: 0.000308\n",
            "Epoch 1103 | Train Loss: 0.000271 | Validation Loss: 0.000297\n",
            "Epoch 1104 | Train Loss: 0.000273 | Validation Loss: 0.000333\n",
            "Epoch 1105 | Train Loss: 0.000281 | Validation Loss: 0.000323\n",
            "Epoch 1106 | Train Loss: 0.000276 | Validation Loss: 0.000295\n",
            "Epoch 1107 | Train Loss: 0.000258 | Validation Loss: 0.000310\n",
            "Epoch 1108 | Train Loss: 0.000253 | Validation Loss: 0.000294\n",
            "Epoch 1109 | Train Loss: 0.000273 | Validation Loss: 0.000307\n",
            "Epoch 1110 | Train Loss: 0.000253 | Validation Loss: 0.000285\n",
            "Epoch 1111 | Train Loss: 0.000261 | Validation Loss: 0.000300\n",
            "Epoch 1112 | Train Loss: 0.000265 | Validation Loss: 0.000290\n",
            "Epoch 1113 | Train Loss: 0.000260 | Validation Loss: 0.000305\n",
            "Epoch 1114 | Train Loss: 0.000260 | Validation Loss: 0.000293\n",
            "Epoch 1115 | Train Loss: 0.000247 | Validation Loss: 0.000314\n",
            "Epoch 1116 | Train Loss: 0.000264 | Validation Loss: 0.000315\n",
            "Epoch 1117 | Train Loss: 0.000269 | Validation Loss: 0.000281\n",
            "Epoch 1118 | Train Loss: 0.000268 | Validation Loss: 0.000291\n",
            "Epoch 1119 | Train Loss: 0.000253 | Validation Loss: 0.000296\n",
            "Epoch 1120 | Train Loss: 0.000256 | Validation Loss: 0.000304\n",
            "Epoch 1121 | Train Loss: 0.000259 | Validation Loss: 0.000307\n",
            "Epoch 1122 | Train Loss: 0.000272 | Validation Loss: 0.000318\n",
            "Epoch 1123 | Train Loss: 0.000260 | Validation Loss: 0.000315\n",
            "Epoch 1124 | Train Loss: 0.000268 | Validation Loss: 0.000314\n",
            "Epoch 1125 | Train Loss: 0.000255 | Validation Loss: 0.000305\n",
            "Epoch 1126 | Train Loss: 0.000307 | Validation Loss: 0.000331\n",
            "Epoch 1127 | Train Loss: 0.000267 | Validation Loss: 0.000319\n",
            "Epoch 1128 | Train Loss: 0.000277 | Validation Loss: 0.000318\n",
            "Epoch 1129 | Train Loss: 0.000256 | Validation Loss: 0.000289\n",
            "Epoch 1130 | Train Loss: 0.000260 | Validation Loss: 0.000304\n",
            "Epoch 1131 | Train Loss: 0.000257 | Validation Loss: 0.000304\n",
            "Epoch 1132 | Train Loss: 0.000261 | Validation Loss: 0.000293\n",
            "Epoch 1133 | Train Loss: 0.000259 | Validation Loss: 0.000293\n",
            "Epoch 1134 | Train Loss: 0.000250 | Validation Loss: 0.000294\n",
            "Epoch 1135 | Train Loss: 0.000255 | Validation Loss: 0.000288\n",
            "Epoch 1136 | Train Loss: 0.000265 | Validation Loss: 0.000289\n",
            "Epoch 1137 | Train Loss: 0.000263 | Validation Loss: 0.000306\n",
            "Epoch 1138 | Train Loss: 0.000259 | Validation Loss: 0.000298\n",
            "Epoch 1139 | Train Loss: 0.000260 | Validation Loss: 0.000288\n",
            "Epoch 1140 | Train Loss: 0.000267 | Validation Loss: 0.000289\n",
            "Epoch 1141 | Train Loss: 0.000258 | Validation Loss: 0.000290\n",
            "Epoch 1142 | Train Loss: 0.000272 | Validation Loss: 0.000314\n",
            "Epoch 1143 | Train Loss: 0.000281 | Validation Loss: 0.000314\n",
            "Epoch 1144 | Train Loss: 0.000293 | Validation Loss: 0.000312\n",
            "Epoch 1145 | Train Loss: 0.000278 | Validation Loss: 0.000300\n",
            "Epoch 1146 | Train Loss: 0.000251 | Validation Loss: 0.000300\n",
            "Epoch 1147 | Train Loss: 0.000250 | Validation Loss: 0.000293\n",
            "Epoch 1148 | Train Loss: 0.000255 | Validation Loss: 0.000301\n",
            "Epoch 1149 | Train Loss: 0.000267 | Validation Loss: 0.000297\n",
            "Epoch 1150 | Train Loss: 0.000249 | Validation Loss: 0.000300\n",
            "Epoch 1151 | Train Loss: 0.000266 | Validation Loss: 0.000282\n",
            "Epoch 1152 | Train Loss: 0.000254 | Validation Loss: 0.000283\n",
            "Epoch 1153 | Train Loss: 0.000254 | Validation Loss: 0.000318\n",
            "Epoch 1154 | Train Loss: 0.000248 | Validation Loss: 0.000304\n",
            "Epoch 1155 | Train Loss: 0.000258 | Validation Loss: 0.000276\n",
            "Epoch 1156 | Train Loss: 0.000267 | Validation Loss: 0.000311\n",
            "Epoch 1157 | Train Loss: 0.000261 | Validation Loss: 0.000323\n",
            "Epoch 1158 | Train Loss: 0.000278 | Validation Loss: 0.000321\n",
            "Epoch 1159 | Train Loss: 0.000264 | Validation Loss: 0.000314\n",
            "Epoch 1160 | Train Loss: 0.000250 | Validation Loss: 0.000290\n",
            "Epoch 1161 | Train Loss: 0.000251 | Validation Loss: 0.000288\n",
            "Epoch 1162 | Train Loss: 0.000245 | Validation Loss: 0.000296\n",
            "Epoch 1163 | Train Loss: 0.000242 | Validation Loss: 0.000293\n",
            "Epoch 1164 | Train Loss: 0.000240 | Validation Loss: 0.000287\n",
            "Epoch 1165 | Train Loss: 0.000243 | Validation Loss: 0.000313\n",
            "Epoch 1166 | Train Loss: 0.000261 | Validation Loss: 0.000330\n",
            "Epoch 1167 | Train Loss: 0.000266 | Validation Loss: 0.000293\n",
            "Epoch 1168 | Train Loss: 0.000245 | Validation Loss: 0.000307\n",
            "Epoch 1169 | Train Loss: 0.000270 | Validation Loss: 0.000321\n",
            "Epoch 1170 | Train Loss: 0.000269 | Validation Loss: 0.000321\n",
            "Epoch 1171 | Train Loss: 0.000280 | Validation Loss: 0.000318\n",
            "Epoch 1172 | Train Loss: 0.000259 | Validation Loss: 0.000297\n",
            "Epoch 1173 | Train Loss: 0.000252 | Validation Loss: 0.000305\n",
            "Epoch 1174 | Train Loss: 0.000268 | Validation Loss: 0.000325\n",
            "Epoch 1175 | Train Loss: 0.000297 | Validation Loss: 0.000304\n",
            "Epoch 1176 | Train Loss: 0.000266 | Validation Loss: 0.000303\n",
            "Epoch 1177 | Train Loss: 0.000268 | Validation Loss: 0.000322\n",
            "Epoch 1178 | Train Loss: 0.000244 | Validation Loss: 0.000283\n",
            "Epoch 1179 | Train Loss: 0.000255 | Validation Loss: 0.000295\n",
            "Epoch 1180 | Train Loss: 0.000266 | Validation Loss: 0.000322\n",
            "Epoch 1181 | Train Loss: 0.000280 | Validation Loss: 0.000308\n",
            "Epoch 1182 | Train Loss: 0.000280 | Validation Loss: 0.000300\n",
            "Epoch 1183 | Train Loss: 0.000276 | Validation Loss: 0.000290\n",
            "Epoch 1184 | Train Loss: 0.000290 | Validation Loss: 0.000332\n",
            "Epoch 1185 | Train Loss: 0.000271 | Validation Loss: 0.000322\n",
            "Epoch 1186 | Train Loss: 0.000280 | Validation Loss: 0.000305\n",
            "Epoch 1187 | Train Loss: 0.000249 | Validation Loss: 0.000295\n",
            "Epoch 1188 | Train Loss: 0.000253 | Validation Loss: 0.000300\n",
            "Epoch 1189 | Train Loss: 0.000276 | Validation Loss: 0.000336\n",
            "Epoch 1190 | Train Loss: 0.000281 | Validation Loss: 0.000293\n",
            "Epoch 1191 | Train Loss: 0.000259 | Validation Loss: 0.000302\n",
            "Epoch 1192 | Train Loss: 0.000271 | Validation Loss: 0.000292\n",
            "Epoch 1193 | Train Loss: 0.000267 | Validation Loss: 0.000316\n",
            "Epoch 1194 | Train Loss: 0.000256 | Validation Loss: 0.000312\n",
            "Epoch 1195 | Train Loss: 0.000273 | Validation Loss: 0.000330\n",
            "Epoch 1196 | Train Loss: 0.000272 | Validation Loss: 0.000303\n",
            "Epoch 1197 | Train Loss: 0.000262 | Validation Loss: 0.000316\n",
            "Epoch 1198 | Train Loss: 0.000254 | Validation Loss: 0.000300\n",
            "Epoch 1199 | Train Loss: 0.000246 | Validation Loss: 0.000288\n",
            "Epoch 1200 | Train Loss: 0.000260 | Validation Loss: 0.000312\n",
            "Epoch 1201 | Train Loss: 0.000248 | Validation Loss: 0.000304\n",
            "Epoch 1202 | Train Loss: 0.000246 | Validation Loss: 0.000308\n",
            "Epoch 1203 | Train Loss: 0.000270 | Validation Loss: 0.000295\n",
            "Epoch 1204 | Train Loss: 0.000267 | Validation Loss: 0.000327\n",
            "Epoch 1205 | Train Loss: 0.000262 | Validation Loss: 0.000317\n",
            "Epoch 1206 | Train Loss: 0.000253 | Validation Loss: 0.000300\n",
            "Epoch 1207 | Train Loss: 0.000251 | Validation Loss: 0.000278\n",
            "Epoch 1208 | Train Loss: 0.000267 | Validation Loss: 0.000316\n",
            "Epoch 1209 | Train Loss: 0.000258 | Validation Loss: 0.000298\n",
            "Epoch 1210 | Train Loss: 0.000273 | Validation Loss: 0.000302\n",
            "Epoch 1211 | Train Loss: 0.000265 | Validation Loss: 0.000309\n",
            "Epoch 1212 | Train Loss: 0.000287 | Validation Loss: 0.000319\n",
            "Epoch 1213 | Train Loss: 0.000286 | Validation Loss: 0.000290\n",
            "Epoch 1214 | Train Loss: 0.000266 | Validation Loss: 0.000295\n",
            "Epoch 1215 | Train Loss: 0.000251 | Validation Loss: 0.000288\n",
            "Epoch 1216 | Train Loss: 0.000260 | Validation Loss: 0.000298\n",
            "Epoch 1217 | Train Loss: 0.000272 | Validation Loss: 0.000336\n",
            "Epoch 1218 | Train Loss: 0.000263 | Validation Loss: 0.000317\n",
            "Epoch 1219 | Train Loss: 0.000274 | Validation Loss: 0.000311\n",
            "Epoch 1220 | Train Loss: 0.000253 | Validation Loss: 0.000300\n",
            "Epoch 1221 | Train Loss: 0.000264 | Validation Loss: 0.000337\n",
            "Epoch 1222 | Train Loss: 0.000295 | Validation Loss: 0.000344\n",
            "Epoch 1223 | Train Loss: 0.000278 | Validation Loss: 0.000331\n",
            "Epoch 1224 | Train Loss: 0.000246 | Validation Loss: 0.000292\n",
            "Epoch 1225 | Train Loss: 0.000262 | Validation Loss: 0.000313\n",
            "Epoch 1226 | Train Loss: 0.000263 | Validation Loss: 0.000296\n",
            "Epoch 1227 | Train Loss: 0.000279 | Validation Loss: 0.000312\n",
            "Epoch 1228 | Train Loss: 0.000263 | Validation Loss: 0.000280\n",
            "Epoch 1229 | Train Loss: 0.000246 | Validation Loss: 0.000306\n",
            "Epoch 1230 | Train Loss: 0.000245 | Validation Loss: 0.000306\n",
            "Epoch 1231 | Train Loss: 0.000270 | Validation Loss: 0.000301\n",
            "Epoch 1232 | Train Loss: 0.000255 | Validation Loss: 0.000319\n",
            "Epoch 1233 | Train Loss: 0.000267 | Validation Loss: 0.000310\n",
            "Epoch 1234 | Train Loss: 0.000254 | Validation Loss: 0.000300\n",
            "Epoch 1235 | Train Loss: 0.000269 | Validation Loss: 0.000297\n",
            "Epoch 1236 | Train Loss: 0.000264 | Validation Loss: 0.000296\n",
            "Epoch 1237 | Train Loss: 0.000252 | Validation Loss: 0.000282\n",
            "Epoch 1238 | Train Loss: 0.000249 | Validation Loss: 0.000295\n",
            "Epoch 1239 | Train Loss: 0.000259 | Validation Loss: 0.000294\n",
            "Epoch 1240 | Train Loss: 0.000260 | Validation Loss: 0.000297\n",
            "Epoch 1241 | Train Loss: 0.000249 | Validation Loss: 0.000286\n",
            "Epoch 1242 | Train Loss: 0.000249 | Validation Loss: 0.000319\n",
            "Epoch 1243 | Train Loss: 0.000268 | Validation Loss: 0.000305\n",
            "Epoch 1244 | Train Loss: 0.000257 | Validation Loss: 0.000287\n",
            "Epoch 1245 | Train Loss: 0.000256 | Validation Loss: 0.000286\n",
            "Epoch 1246 | Train Loss: 0.000245 | Validation Loss: 0.000296\n",
            "Epoch 1247 | Train Loss: 0.000253 | Validation Loss: 0.000285\n",
            "Epoch 1248 | Train Loss: 0.000241 | Validation Loss: 0.000283\n",
            "Epoch 1249 | Train Loss: 0.000247 | Validation Loss: 0.000291\n",
            "Epoch 1250 | Train Loss: 0.000245 | Validation Loss: 0.000278\n",
            "Epoch 1251 | Train Loss: 0.000239 | Validation Loss: 0.000304\n",
            "Epoch 1252 | Train Loss: 0.000255 | Validation Loss: 0.000324\n",
            "Epoch 1253 | Train Loss: 0.000267 | Validation Loss: 0.000322\n",
            "Epoch 1254 | Train Loss: 0.000259 | Validation Loss: 0.000327\n",
            "Epoch 1255 | Train Loss: 0.000268 | Validation Loss: 0.000299\n",
            "Epoch 1256 | Train Loss: 0.000258 | Validation Loss: 0.000324\n",
            "Epoch 1257 | Train Loss: 0.000258 | Validation Loss: 0.000306\n",
            "Epoch 1258 | Train Loss: 0.000261 | Validation Loss: 0.000303\n",
            "Epoch 1259 | Train Loss: 0.000250 | Validation Loss: 0.000288\n",
            "Epoch 1260 | Train Loss: 0.000249 | Validation Loss: 0.000298\n",
            "Epoch 1261 | Train Loss: 0.000245 | Validation Loss: 0.000290\n",
            "Epoch 1262 | Train Loss: 0.000266 | Validation Loss: 0.000334\n",
            "Epoch 1263 | Train Loss: 0.000279 | Validation Loss: 0.000316\n",
            "Epoch 1264 | Train Loss: 0.000260 | Validation Loss: 0.000305\n",
            "Epoch 1265 | Train Loss: 0.000241 | Validation Loss: 0.000289\n",
            "Epoch 1266 | Train Loss: 0.000258 | Validation Loss: 0.000293\n",
            "Epoch 1267 | Train Loss: 0.000252 | Validation Loss: 0.000316\n",
            "Epoch 1268 | Train Loss: 0.000268 | Validation Loss: 0.000326\n",
            "Epoch 1269 | Train Loss: 0.000268 | Validation Loss: 0.000324\n",
            "Epoch 1270 | Train Loss: 0.000287 | Validation Loss: 0.000298\n",
            "Epoch 1271 | Train Loss: 0.000258 | Validation Loss: 0.000305\n",
            "Epoch 1272 | Train Loss: 0.000275 | Validation Loss: 0.000316\n",
            "Epoch 1273 | Train Loss: 0.000252 | Validation Loss: 0.000318\n",
            "Epoch 1274 | Train Loss: 0.000268 | Validation Loss: 0.000304\n",
            "Epoch 1275 | Train Loss: 0.000272 | Validation Loss: 0.000306\n",
            "Epoch 1276 | Train Loss: 0.000268 | Validation Loss: 0.000331\n",
            "Epoch 1277 | Train Loss: 0.000269 | Validation Loss: 0.000304\n",
            "Epoch 1278 | Train Loss: 0.000263 | Validation Loss: 0.000304\n",
            "Epoch 1279 | Train Loss: 0.000249 | Validation Loss: 0.000300\n",
            "Epoch 1280 | Train Loss: 0.000250 | Validation Loss: 0.000306\n",
            "Epoch 1281 | Train Loss: 0.000244 | Validation Loss: 0.000292\n",
            "Epoch 1282 | Train Loss: 0.000232 | Validation Loss: 0.000284\n",
            "Epoch 1283 | Train Loss: 0.000245 | Validation Loss: 0.000298\n",
            "Epoch 1284 | Train Loss: 0.000250 | Validation Loss: 0.000318\n",
            "Epoch 1285 | Train Loss: 0.000261 | Validation Loss: 0.000308\n",
            "Epoch 1286 | Train Loss: 0.000246 | Validation Loss: 0.000304\n",
            "Epoch 1287 | Train Loss: 0.000252 | Validation Loss: 0.000302\n",
            "Epoch 1288 | Train Loss: 0.000255 | Validation Loss: 0.000293\n",
            "Epoch 1289 | Train Loss: 0.000247 | Validation Loss: 0.000298\n",
            "Epoch 1290 | Train Loss: 0.000251 | Validation Loss: 0.000305\n",
            "Epoch 1291 | Train Loss: 0.000256 | Validation Loss: 0.000297\n",
            "Epoch 1292 | Train Loss: 0.000236 | Validation Loss: 0.000293\n",
            "Epoch 1293 | Train Loss: 0.000249 | Validation Loss: 0.000309\n",
            "Epoch 1294 | Train Loss: 0.000264 | Validation Loss: 0.000308\n",
            "Epoch 1295 | Train Loss: 0.000256 | Validation Loss: 0.000292\n",
            "Epoch 1296 | Train Loss: 0.000259 | Validation Loss: 0.000309\n",
            "Epoch 1297 | Train Loss: 0.000256 | Validation Loss: 0.000339\n",
            "Epoch 1298 | Train Loss: 0.000280 | Validation Loss: 0.000311\n",
            "Epoch 1299 | Train Loss: 0.000248 | Validation Loss: 0.000293\n",
            "Epoch 1300 | Train Loss: 0.000251 | Validation Loss: 0.000291\n",
            "Epoch 1301 | Train Loss: 0.000257 | Validation Loss: 0.000306\n",
            "Epoch 1302 | Train Loss: 0.000256 | Validation Loss: 0.000317\n",
            "Epoch 1303 | Train Loss: 0.000251 | Validation Loss: 0.000305\n",
            "Epoch 1304 | Train Loss: 0.000253 | Validation Loss: 0.000289\n",
            "Epoch 1305 | Train Loss: 0.000252 | Validation Loss: 0.000304\n",
            "Epoch 1306 | Train Loss: 0.000258 | Validation Loss: 0.000338\n",
            "Epoch 1307 | Train Loss: 0.000236 | Validation Loss: 0.000428\n",
            "Epoch 1308 | Train Loss: 0.000242 | Validation Loss: 0.000520\n",
            "Epoch 1309 | Train Loss: 0.000246 | Validation Loss: 0.000309\n",
            "Epoch 1310 | Train Loss: 0.000257 | Validation Loss: 0.000313\n",
            "Epoch 1311 | Train Loss: 0.000272 | Validation Loss: 0.000304\n",
            "Epoch 1312 | Train Loss: 0.000260 | Validation Loss: 0.000316\n",
            "Epoch 1313 | Train Loss: 0.000290 | Validation Loss: 0.000329\n",
            "Epoch 1314 | Train Loss: 0.000278 | Validation Loss: 0.000332\n",
            "Epoch 1315 | Train Loss: 0.000262 | Validation Loss: 0.000299\n",
            "Epoch 1316 | Train Loss: 0.000250 | Validation Loss: 0.000307\n",
            "Epoch 1317 | Train Loss: 0.000264 | Validation Loss: 0.000299\n",
            "Epoch 1318 | Train Loss: 0.000255 | Validation Loss: 0.000299\n",
            "Epoch 1319 | Train Loss: 0.000254 | Validation Loss: 0.000293\n",
            "Epoch 1320 | Train Loss: 0.000253 | Validation Loss: 0.000292\n",
            "Epoch 1321 | Train Loss: 0.000247 | Validation Loss: 0.000292\n",
            "Epoch 1322 | Train Loss: 0.000254 | Validation Loss: 0.000313\n",
            "Epoch 1323 | Train Loss: 0.000268 | Validation Loss: 0.000291\n",
            "Epoch 1324 | Train Loss: 0.000274 | Validation Loss: 0.000284\n",
            "Epoch 1325 | Train Loss: 0.000249 | Validation Loss: 0.000298\n",
            "Epoch 1326 | Train Loss: 0.000233 | Validation Loss: 0.000294\n",
            "Epoch 1327 | Train Loss: 0.000242 | Validation Loss: 0.000295\n",
            "Epoch 1328 | Train Loss: 0.000253 | Validation Loss: 0.000320\n",
            "Epoch 1329 | Train Loss: 0.000251 | Validation Loss: 0.000307\n",
            "Epoch 1330 | Train Loss: 0.000252 | Validation Loss: 0.000291\n",
            "Epoch 1331 | Train Loss: 0.000249 | Validation Loss: 0.000313\n",
            "Epoch 1332 | Train Loss: 0.000244 | Validation Loss: 0.000280\n",
            "Epoch 1333 | Train Loss: 0.000248 | Validation Loss: 0.000296\n",
            "Epoch 1334 | Train Loss: 0.000235 | Validation Loss: 0.000303\n",
            "Epoch 1335 | Train Loss: 0.000245 | Validation Loss: 0.000284\n",
            "Epoch 1336 | Train Loss: 0.000249 | Validation Loss: 0.000298\n",
            "Epoch 1337 | Train Loss: 0.000248 | Validation Loss: 0.000299\n",
            "Epoch 1338 | Train Loss: 0.000241 | Validation Loss: 0.000285\n",
            "Epoch 1339 | Train Loss: 0.000235 | Validation Loss: 0.000282\n",
            "Epoch 1340 | Train Loss: 0.000245 | Validation Loss: 0.000304\n",
            "Epoch 1341 | Train Loss: 0.000272 | Validation Loss: 0.000307\n",
            "Epoch 1342 | Train Loss: 0.000273 | Validation Loss: 0.000321\n",
            "Epoch 1343 | Train Loss: 0.000241 | Validation Loss: 0.000303\n",
            "Epoch 1344 | Train Loss: 0.000237 | Validation Loss: 0.000301\n",
            "Epoch 1345 | Train Loss: 0.000233 | Validation Loss: 0.000290\n",
            "Epoch 1346 | Train Loss: 0.000249 | Validation Loss: 0.000306\n",
            "Epoch 1347 | Train Loss: 0.000240 | Validation Loss: 0.000296\n",
            "Epoch 1348 | Train Loss: 0.000256 | Validation Loss: 0.000291\n",
            "Epoch 1349 | Train Loss: 0.000243 | Validation Loss: 0.000280\n",
            "Epoch 1350 | Train Loss: 0.000250 | Validation Loss: 0.000292\n",
            "Epoch 1351 | Train Loss: 0.000240 | Validation Loss: 0.000287\n",
            "Epoch 1352 | Train Loss: 0.000241 | Validation Loss: 0.000290\n",
            "Epoch 1353 | Train Loss: 0.000240 | Validation Loss: 0.000293\n",
            "Epoch 1354 | Train Loss: 0.000242 | Validation Loss: 0.000297\n",
            "Epoch 1355 | Train Loss: 0.000251 | Validation Loss: 0.000304\n",
            "Epoch 1356 | Train Loss: 0.000279 | Validation Loss: 0.000299\n",
            "Epoch 1357 | Train Loss: 0.000285 | Validation Loss: 0.000317\n",
            "Epoch 1358 | Train Loss: 0.000242 | Validation Loss: 0.000300\n",
            "Epoch 1359 | Train Loss: 0.000251 | Validation Loss: 0.000312\n",
            "Epoch 1360 | Train Loss: 0.000248 | Validation Loss: 0.000309\n",
            "Epoch 1361 | Train Loss: 0.000256 | Validation Loss: 0.000306\n",
            "Epoch 1362 | Train Loss: 0.000278 | Validation Loss: 0.000336\n",
            "Epoch 1363 | Train Loss: 0.000274 | Validation Loss: 0.000307\n",
            "Epoch 1364 | Train Loss: 0.000273 | Validation Loss: 0.000382\n",
            "Epoch 1365 | Train Loss: 0.000282 | Validation Loss: 0.000310\n",
            "Epoch 1366 | Train Loss: 0.000264 | Validation Loss: 0.000299\n",
            "Epoch 1367 | Train Loss: 0.000249 | Validation Loss: 0.000325\n",
            "Epoch 1368 | Train Loss: 0.000251 | Validation Loss: 0.000299\n",
            "Epoch 1369 | Train Loss: 0.000246 | Validation Loss: 0.000305\n",
            "Epoch 1370 | Train Loss: 0.000240 | Validation Loss: 0.000305\n",
            "Epoch 1371 | Train Loss: 0.000244 | Validation Loss: 0.000309\n",
            "Epoch 1372 | Train Loss: 0.000255 | Validation Loss: 0.000323\n",
            "Epoch 1373 | Train Loss: 0.000253 | Validation Loss: 0.000288\n",
            "Epoch 1374 | Train Loss: 0.000268 | Validation Loss: 0.000321\n",
            "Epoch 1375 | Train Loss: 0.000275 | Validation Loss: 0.000336\n",
            "Epoch 1376 | Train Loss: 0.000277 | Validation Loss: 0.000313\n",
            "Epoch 1377 | Train Loss: 0.000294 | Validation Loss: 0.000317\n",
            "Epoch 1378 | Train Loss: 0.000298 | Validation Loss: 0.000327\n",
            "Epoch 1379 | Train Loss: 0.000288 | Validation Loss: 0.000358\n",
            "Epoch 1380 | Train Loss: 0.000282 | Validation Loss: 0.000382\n",
            "Epoch 1381 | Train Loss: 0.000282 | Validation Loss: 0.000299\n",
            "Epoch 1382 | Train Loss: 0.000259 | Validation Loss: 0.000303\n",
            "Epoch 1383 | Train Loss: 0.000259 | Validation Loss: 0.000298\n",
            "Epoch 1384 | Train Loss: 0.000265 | Validation Loss: 0.000321\n",
            "Epoch 1385 | Train Loss: 0.000276 | Validation Loss: 0.000289\n",
            "Epoch 1386 | Train Loss: 0.000268 | Validation Loss: 0.000317\n",
            "Epoch 1387 | Train Loss: 0.000248 | Validation Loss: 0.000297\n",
            "Epoch 1388 | Train Loss: 0.000285 | Validation Loss: 0.000344\n",
            "Epoch 1389 | Train Loss: 0.000276 | Validation Loss: 0.000304\n",
            "Epoch 1390 | Train Loss: 0.000255 | Validation Loss: 0.000302\n",
            "Epoch 1391 | Train Loss: 0.000262 | Validation Loss: 0.000326\n",
            "Epoch 1392 | Train Loss: 0.000277 | Validation Loss: 0.000329\n",
            "Epoch 1393 | Train Loss: 0.000251 | Validation Loss: 0.000304\n",
            "Epoch 1394 | Train Loss: 0.000244 | Validation Loss: 0.000289\n",
            "Epoch 1395 | Train Loss: 0.000252 | Validation Loss: 0.000315\n",
            "Epoch 1396 | Train Loss: 0.000256 | Validation Loss: 0.000301\n",
            "Epoch 1397 | Train Loss: 0.000262 | Validation Loss: 0.000307\n",
            "Epoch 1398 | Train Loss: 0.000246 | Validation Loss: 0.000287\n",
            "Epoch 1399 | Train Loss: 0.000257 | Validation Loss: 0.000304\n",
            "Epoch 1400 | Train Loss: 0.000245 | Validation Loss: 0.000300\n",
            "Epoch 1401 | Train Loss: 0.000241 | Validation Loss: 0.000302\n",
            "Epoch 1402 | Train Loss: 0.000262 | Validation Loss: 0.000311\n",
            "Epoch 1403 | Train Loss: 0.000252 | Validation Loss: 0.000318\n",
            "Epoch 1404 | Train Loss: 0.000246 | Validation Loss: 0.000308\n",
            "Epoch 1405 | Train Loss: 0.000284 | Validation Loss: 0.000305\n",
            "Epoch 1406 | Train Loss: 0.000280 | Validation Loss: 0.000301\n",
            "Epoch 1407 | Train Loss: 0.000257 | Validation Loss: 0.000285\n",
            "Epoch 1408 | Train Loss: 0.000271 | Validation Loss: 0.000329\n",
            "Epoch 1409 | Train Loss: 0.000266 | Validation Loss: 0.000322\n",
            "Epoch 1410 | Train Loss: 0.000250 | Validation Loss: 0.000314\n",
            "Epoch 1411 | Train Loss: 0.000264 | Validation Loss: 0.000321\n",
            "Epoch 1412 | Train Loss: 0.000297 | Validation Loss: 0.000295\n",
            "Epoch 1413 | Train Loss: 0.000273 | Validation Loss: 0.000322\n",
            "Epoch 1414 | Train Loss: 0.000265 | Validation Loss: 0.000320\n",
            "Epoch 1415 | Train Loss: 0.000251 | Validation Loss: 0.000302\n",
            "Epoch 1416 | Train Loss: 0.000263 | Validation Loss: 0.000295\n",
            "Epoch 1417 | Train Loss: 0.000258 | Validation Loss: 0.000283\n",
            "Epoch 1418 | Train Loss: 0.000255 | Validation Loss: 0.000322\n",
            "Epoch 1419 | Train Loss: 0.000254 | Validation Loss: 0.000307\n",
            "Epoch 1420 | Train Loss: 0.000244 | Validation Loss: 0.000309\n",
            "Epoch 1421 | Train Loss: 0.000260 | Validation Loss: 0.000309\n",
            "Epoch 1422 | Train Loss: 0.000250 | Validation Loss: 0.000312\n",
            "Epoch 1423 | Train Loss: 0.000253 | Validation Loss: 0.000316\n",
            "Epoch 1424 | Train Loss: 0.000250 | Validation Loss: 0.000306\n",
            "Epoch 1425 | Train Loss: 0.000243 | Validation Loss: 0.000292\n",
            "Epoch 1426 | Train Loss: 0.000253 | Validation Loss: 0.000314\n",
            "Epoch 1427 | Train Loss: 0.000259 | Validation Loss: 0.000292\n",
            "Epoch 1428 | Train Loss: 0.000263 | Validation Loss: 0.000308\n",
            "Epoch 1429 | Train Loss: 0.000249 | Validation Loss: 0.000308\n",
            "Epoch 1430 | Train Loss: 0.000268 | Validation Loss: 0.000307\n",
            "Epoch 1431 | Train Loss: 0.000272 | Validation Loss: 0.000314\n",
            "Epoch 1432 | Train Loss: 0.000268 | Validation Loss: 0.000305\n",
            "Epoch 1433 | Train Loss: 0.000246 | Validation Loss: 0.000302\n",
            "Epoch 1434 | Train Loss: 0.000257 | Validation Loss: 0.000306\n",
            "Epoch 1435 | Train Loss: 0.000247 | Validation Loss: 0.000298\n",
            "Epoch 1436 | Train Loss: 0.000262 | Validation Loss: 0.000286\n",
            "Epoch 1437 | Train Loss: 0.000253 | Validation Loss: 0.000315\n",
            "Epoch 1438 | Train Loss: 0.000263 | Validation Loss: 0.000294\n",
            "Epoch 1439 | Train Loss: 0.000266 | Validation Loss: 0.000324\n",
            "Epoch 1440 | Train Loss: 0.000294 | Validation Loss: 0.000327\n",
            "Epoch 1441 | Train Loss: 0.000257 | Validation Loss: 0.000314\n",
            "Epoch 1442 | Train Loss: 0.000247 | Validation Loss: 0.000312\n",
            "Epoch 1443 | Train Loss: 0.000231 | Validation Loss: 0.000288\n",
            "Epoch 1444 | Train Loss: 0.000245 | Validation Loss: 0.000305\n",
            "Epoch 1445 | Train Loss: 0.000250 | Validation Loss: 0.000319\n",
            "Epoch 1446 | Train Loss: 0.000272 | Validation Loss: 0.000313\n",
            "Epoch 1447 | Train Loss: 0.000258 | Validation Loss: 0.000323\n",
            "Epoch 1448 | Train Loss: 0.000270 | Validation Loss: 0.000310\n",
            "Epoch 1449 | Train Loss: 0.000257 | Validation Loss: 0.000303\n",
            "Epoch 1450 | Train Loss: 0.000258 | Validation Loss: 0.000296\n",
            "Epoch 1451 | Train Loss: 0.000253 | Validation Loss: 0.000303\n",
            "Epoch 1452 | Train Loss: 0.000242 | Validation Loss: 0.000306\n",
            "Epoch 1453 | Train Loss: 0.000238 | Validation Loss: 0.000302\n",
            "Epoch 1454 | Train Loss: 0.000255 | Validation Loss: 0.000303\n",
            "Epoch 1455 | Train Loss: 0.000252 | Validation Loss: 0.000298\n",
            "Epoch 1456 | Train Loss: 0.000254 | Validation Loss: 0.000303\n",
            "Epoch 1457 | Train Loss: 0.000253 | Validation Loss: 0.000309\n",
            "Epoch 1458 | Train Loss: 0.000256 | Validation Loss: 0.000300\n",
            "Epoch 1459 | Train Loss: 0.000250 | Validation Loss: 0.000305\n",
            "Epoch 1460 | Train Loss: 0.000259 | Validation Loss: 0.000316\n",
            "Epoch 1461 | Train Loss: 0.000264 | Validation Loss: 0.000331\n",
            "Epoch 1462 | Train Loss: 0.000264 | Validation Loss: 0.000296\n",
            "Epoch 1463 | Train Loss: 0.000251 | Validation Loss: 0.000309\n",
            "Epoch 1464 | Train Loss: 0.000244 | Validation Loss: 0.000322\n",
            "Epoch 1465 | Train Loss: 0.000256 | Validation Loss: 0.000339\n",
            "Epoch 1466 | Train Loss: 0.000258 | Validation Loss: 0.000341\n",
            "Epoch 1467 | Train Loss: 0.000281 | Validation Loss: 0.000316\n",
            "Epoch 1468 | Train Loss: 0.000246 | Validation Loss: 0.000313\n",
            "Epoch 1469 | Train Loss: 0.000249 | Validation Loss: 0.000297\n",
            "Epoch 1470 | Train Loss: 0.000243 | Validation Loss: 0.000305\n",
            "Epoch 1471 | Train Loss: 0.000253 | Validation Loss: 0.000291\n",
            "Epoch 1472 | Train Loss: 0.000241 | Validation Loss: 0.000298\n",
            "Epoch 1473 | Train Loss: 0.000268 | Validation Loss: 0.000291\n",
            "Epoch 1474 | Train Loss: 0.000252 | Validation Loss: 0.000300\n",
            "Epoch 1475 | Train Loss: 0.000239 | Validation Loss: 0.000298\n",
            "Epoch 1476 | Train Loss: 0.000243 | Validation Loss: 0.000308\n",
            "Epoch 1477 | Train Loss: 0.000258 | Validation Loss: 0.000334\n",
            "Epoch 1478 | Train Loss: 0.000276 | Validation Loss: 0.000350\n",
            "Epoch 1479 | Train Loss: 0.000250 | Validation Loss: 0.000301\n",
            "Epoch 1480 | Train Loss: 0.000243 | Validation Loss: 0.000295\n",
            "Epoch 1481 | Train Loss: 0.000253 | Validation Loss: 0.000328\n",
            "Epoch 1482 | Train Loss: 0.000258 | Validation Loss: 0.000320\n",
            "Epoch 1483 | Train Loss: 0.000255 | Validation Loss: 0.000308\n",
            "Epoch 1484 | Train Loss: 0.000254 | Validation Loss: 0.000310\n",
            "Epoch 1485 | Train Loss: 0.000265 | Validation Loss: 0.000321\n",
            "Epoch 1486 | Train Loss: 0.000256 | Validation Loss: 0.000312\n",
            "Epoch 1487 | Train Loss: 0.000249 | Validation Loss: 0.000309\n",
            "Epoch 1488 | Train Loss: 0.000238 | Validation Loss: 0.000297\n",
            "Epoch 1489 | Train Loss: 0.000249 | Validation Loss: 0.000318\n",
            "Epoch 1490 | Train Loss: 0.000278 | Validation Loss: 0.000337\n",
            "Epoch 1491 | Train Loss: 0.000236 | Validation Loss: 0.000312\n",
            "Epoch 1492 | Train Loss: 0.000251 | Validation Loss: 0.000305\n",
            "Epoch 1493 | Train Loss: 0.000264 | Validation Loss: 0.000347\n",
            "Epoch 1494 | Train Loss: 0.000265 | Validation Loss: 0.000321\n",
            "Epoch 1495 | Train Loss: 0.000246 | Validation Loss: 0.000304\n",
            "Epoch 1496 | Train Loss: 0.000259 | Validation Loss: 0.000293\n",
            "Epoch 1497 | Train Loss: 0.000258 | Validation Loss: 0.000322\n",
            "Epoch 1498 | Train Loss: 0.000235 | Validation Loss: 0.000287\n",
            "Epoch 1499 | Train Loss: 0.000242 | Validation Loss: 0.000276\n",
            "Epoch 1500 | Train Loss: 0.000233 | Validation Loss: 0.000292\n",
            "Epoch 1501 | Train Loss: 0.000245 | Validation Loss: 0.000301\n",
            "Epoch 1502 | Train Loss: 0.000234 | Validation Loss: 0.000292\n",
            "Epoch 1503 | Train Loss: 0.000233 | Validation Loss: 0.000305\n",
            "Epoch 1504 | Train Loss: 0.000247 | Validation Loss: 0.000317\n",
            "Epoch 1505 | Train Loss: 0.000248 | Validation Loss: 0.000321\n",
            "Epoch 1506 | Train Loss: 0.000260 | Validation Loss: 0.000298\n",
            "Epoch 1507 | Train Loss: 0.000253 | Validation Loss: 0.000336\n",
            "Epoch 1508 | Train Loss: 0.000243 | Validation Loss: 0.000340\n",
            "Epoch 1509 | Train Loss: 0.000244 | Validation Loss: 0.000319\n",
            "Epoch 1510 | Train Loss: 0.000245 | Validation Loss: 0.000296\n",
            "Epoch 1511 | Train Loss: 0.000235 | Validation Loss: 0.000297\n",
            "Epoch 1512 | Train Loss: 0.000239 | Validation Loss: 0.000292\n",
            "Epoch 1513 | Train Loss: 0.000235 | Validation Loss: 0.000302\n",
            "Epoch 1514 | Train Loss: 0.000236 | Validation Loss: 0.000291\n",
            "Epoch 1515 | Train Loss: 0.000239 | Validation Loss: 0.000307\n",
            "Epoch 1516 | Train Loss: 0.000239 | Validation Loss: 0.000299\n",
            "Epoch 1517 | Train Loss: 0.000257 | Validation Loss: 0.000297\n",
            "Epoch 1518 | Train Loss: 0.000251 | Validation Loss: 0.000300\n",
            "Epoch 1519 | Train Loss: 0.000234 | Validation Loss: 0.000311\n",
            "Epoch 1520 | Train Loss: 0.000238 | Validation Loss: 0.000302\n",
            "Epoch 1521 | Train Loss: 0.000246 | Validation Loss: 0.000286\n",
            "Epoch 1522 | Train Loss: 0.000254 | Validation Loss: 0.000291\n",
            "Epoch 1523 | Train Loss: 0.000235 | Validation Loss: 0.000296\n",
            "Epoch 1524 | Train Loss: 0.000236 | Validation Loss: 0.000289\n",
            "Epoch 1525 | Train Loss: 0.000244 | Validation Loss: 0.000300\n",
            "Epoch 1526 | Train Loss: 0.000233 | Validation Loss: 0.000309\n",
            "Epoch 1527 | Train Loss: 0.000259 | Validation Loss: 0.000318\n",
            "Epoch 1528 | Train Loss: 0.000254 | Validation Loss: 0.000291\n",
            "Epoch 1529 | Train Loss: 0.000254 | Validation Loss: 0.000300\n",
            "Epoch 1530 | Train Loss: 0.000250 | Validation Loss: 0.000307\n",
            "Epoch 1531 | Train Loss: 0.000242 | Validation Loss: 0.000283\n",
            "Epoch 1532 | Train Loss: 0.000233 | Validation Loss: 0.000305\n",
            "Epoch 1533 | Train Loss: 0.000260 | Validation Loss: 0.000341\n",
            "Epoch 1534 | Train Loss: 0.000250 | Validation Loss: 0.000305\n",
            "Epoch 1535 | Train Loss: 0.000241 | Validation Loss: 0.000313\n",
            "Epoch 1536 | Train Loss: 0.000231 | Validation Loss: 0.000299\n",
            "Epoch 1537 | Train Loss: 0.000255 | Validation Loss: 0.000320\n",
            "Epoch 1538 | Train Loss: 0.000245 | Validation Loss: 0.000294\n",
            "Epoch 1539 | Train Loss: 0.000248 | Validation Loss: 0.000302\n",
            "Epoch 1540 | Train Loss: 0.000247 | Validation Loss: 0.000291\n",
            "Epoch 1541 | Train Loss: 0.000251 | Validation Loss: 0.000288\n",
            "Epoch 1542 | Train Loss: 0.000233 | Validation Loss: 0.000278\n",
            "Epoch 1543 | Train Loss: 0.000235 | Validation Loss: 0.000303\n",
            "Epoch 1544 | Train Loss: 0.000230 | Validation Loss: 0.000291\n",
            "Epoch 1545 | Train Loss: 0.000240 | Validation Loss: 0.000311\n",
            "Epoch 1546 | Train Loss: 0.000238 | Validation Loss: 0.000288\n",
            "Epoch 1547 | Train Loss: 0.000224 | Validation Loss: 0.000285\n",
            "Epoch 1548 | Train Loss: 0.000234 | Validation Loss: 0.000292\n",
            "Epoch 1549 | Train Loss: 0.000254 | Validation Loss: 0.000315\n",
            "Epoch 1550 | Train Loss: 0.000244 | Validation Loss: 0.000292\n",
            "Epoch 1551 | Train Loss: 0.000265 | Validation Loss: 0.000315\n",
            "Epoch 1552 | Train Loss: 0.000274 | Validation Loss: 0.000304\n",
            "Epoch 1553 | Train Loss: 0.000248 | Validation Loss: 0.000317\n",
            "Epoch 1554 | Train Loss: 0.000254 | Validation Loss: 0.000296\n",
            "Epoch 1555 | Train Loss: 0.000250 | Validation Loss: 0.000301\n",
            "Epoch 1556 | Train Loss: 0.000253 | Validation Loss: 0.000308\n",
            "Epoch 1557 | Train Loss: 0.000237 | Validation Loss: 0.000294\n",
            "Epoch 1558 | Train Loss: 0.000240 | Validation Loss: 0.000293\n",
            "Epoch 1559 | Train Loss: 0.000248 | Validation Loss: 0.000286\n",
            "Epoch 1560 | Train Loss: 0.000256 | Validation Loss: 0.000291\n",
            "Epoch 1561 | Train Loss: 0.000243 | Validation Loss: 0.000298\n",
            "Epoch 1562 | Train Loss: 0.000230 | Validation Loss: 0.000304\n",
            "Epoch 1563 | Train Loss: 0.000257 | Validation Loss: 0.000312\n",
            "Epoch 1564 | Train Loss: 0.000248 | Validation Loss: 0.000312\n",
            "Epoch 1565 | Train Loss: 0.000239 | Validation Loss: 0.000328\n",
            "Epoch 1566 | Train Loss: 0.000277 | Validation Loss: 0.000326\n",
            "Epoch 1567 | Train Loss: 0.000253 | Validation Loss: 0.000307\n",
            "Epoch 1568 | Train Loss: 0.000241 | Validation Loss: 0.000291\n",
            "Epoch 1569 | Train Loss: 0.000239 | Validation Loss: 0.000281\n",
            "Epoch 1570 | Train Loss: 0.000237 | Validation Loss: 0.000302\n",
            "Epoch 1571 | Train Loss: 0.000227 | Validation Loss: 0.000303\n",
            "Epoch 1572 | Train Loss: 0.000238 | Validation Loss: 0.000323\n",
            "Epoch 1573 | Train Loss: 0.000239 | Validation Loss: 0.000320\n",
            "Epoch 1574 | Train Loss: 0.000260 | Validation Loss: 0.000334\n",
            "Epoch 1575 | Train Loss: 0.000251 | Validation Loss: 0.000313\n",
            "Epoch 1576 | Train Loss: 0.000239 | Validation Loss: 0.000302\n",
            "Epoch 1577 | Train Loss: 0.000228 | Validation Loss: 0.000289\n",
            "Epoch 1578 | Train Loss: 0.000236 | Validation Loss: 0.000284\n",
            "Epoch 1579 | Train Loss: 0.000246 | Validation Loss: 0.000283\n",
            "Epoch 1580 | Train Loss: 0.000235 | Validation Loss: 0.000304\n",
            "Epoch 1581 | Train Loss: 0.000240 | Validation Loss: 0.000289\n",
            "Epoch 1582 | Train Loss: 0.000242 | Validation Loss: 0.000314\n",
            "Epoch 1583 | Train Loss: 0.000247 | Validation Loss: 0.000286\n",
            "Epoch 1584 | Train Loss: 0.000235 | Validation Loss: 0.000294\n",
            "Epoch 1585 | Train Loss: 0.000239 | Validation Loss: 0.000293\n",
            "Epoch 1586 | Train Loss: 0.000243 | Validation Loss: 0.000299\n",
            "Epoch 1587 | Train Loss: 0.000236 | Validation Loss: 0.000285\n",
            "Epoch 1588 | Train Loss: 0.000244 | Validation Loss: 0.000288\n",
            "Epoch 1589 | Train Loss: 0.000237 | Validation Loss: 0.000286\n",
            "Epoch 1590 | Train Loss: 0.000256 | Validation Loss: 0.000300\n",
            "Epoch 1591 | Train Loss: 0.000257 | Validation Loss: 0.000306\n",
            "Epoch 1592 | Train Loss: 0.000258 | Validation Loss: 0.000296\n",
            "Epoch 1593 | Train Loss: 0.000250 | Validation Loss: 0.000277\n",
            "Epoch 1594 | Train Loss: 0.000246 | Validation Loss: 0.000308\n",
            "Epoch 1595 | Train Loss: 0.000251 | Validation Loss: 0.000295\n",
            "Epoch 1596 | Train Loss: 0.000236 | Validation Loss: 0.000299\n",
            "Epoch 1597 | Train Loss: 0.000236 | Validation Loss: 0.000291\n",
            "Epoch 1598 | Train Loss: 0.000225 | Validation Loss: 0.000296\n",
            "Epoch 1599 | Train Loss: 0.000251 | Validation Loss: 0.000302\n",
            "Epoch 1600 | Train Loss: 0.000239 | Validation Loss: 0.000292\n",
            "Epoch 1601 | Train Loss: 0.000246 | Validation Loss: 0.000310\n",
            "Epoch 1602 | Train Loss: 0.000268 | Validation Loss: 0.000322\n",
            "Epoch 1603 | Train Loss: 0.000256 | Validation Loss: 0.000324\n",
            "Epoch 1604 | Train Loss: 0.000256 | Validation Loss: 0.000297\n",
            "Epoch 1605 | Train Loss: 0.000259 | Validation Loss: 0.000289\n",
            "Epoch 1606 | Train Loss: 0.000237 | Validation Loss: 0.000294\n",
            "Epoch 1607 | Train Loss: 0.000234 | Validation Loss: 0.000286\n",
            "Epoch 1608 | Train Loss: 0.000237 | Validation Loss: 0.000281\n",
            "Epoch 1609 | Train Loss: 0.000237 | Validation Loss: 0.000295\n",
            "Epoch 1610 | Train Loss: 0.000249 | Validation Loss: 0.000285\n",
            "Epoch 1611 | Train Loss: 0.000231 | Validation Loss: 0.000292\n",
            "Epoch 1612 | Train Loss: 0.000250 | Validation Loss: 0.000291\n",
            "Epoch 1613 | Train Loss: 0.000240 | Validation Loss: 0.000304\n",
            "Epoch 1614 | Train Loss: 0.000264 | Validation Loss: 0.000286\n",
            "Epoch 1615 | Train Loss: 0.000247 | Validation Loss: 0.000326\n",
            "Epoch 1616 | Train Loss: 0.000240 | Validation Loss: 0.000290\n",
            "Epoch 1617 | Train Loss: 0.000240 | Validation Loss: 0.000283\n",
            "Epoch 1618 | Train Loss: 0.000232 | Validation Loss: 0.000277\n",
            "Epoch 1619 | Train Loss: 0.000232 | Validation Loss: 0.000292\n",
            "Epoch 1620 | Train Loss: 0.000233 | Validation Loss: 0.000286\n",
            "Epoch 1621 | Train Loss: 0.000236 | Validation Loss: 0.000282\n",
            "Epoch 1622 | Train Loss: 0.000228 | Validation Loss: 0.000289\n",
            "Epoch 1623 | Train Loss: 0.000237 | Validation Loss: 0.000294\n",
            "Epoch 1624 | Train Loss: 0.000260 | Validation Loss: 0.000312\n",
            "Epoch 1625 | Train Loss: 0.000238 | Validation Loss: 0.000297\n",
            "Epoch 1626 | Train Loss: 0.000237 | Validation Loss: 0.000293\n",
            "Epoch 1627 | Train Loss: 0.000227 | Validation Loss: 0.000292\n",
            "Epoch 1628 | Train Loss: 0.000237 | Validation Loss: 0.000294\n",
            "Epoch 1629 | Train Loss: 0.000240 | Validation Loss: 0.000285\n",
            "Epoch 1630 | Train Loss: 0.000250 | Validation Loss: 0.000298\n",
            "Epoch 1631 | Train Loss: 0.000244 | Validation Loss: 0.000286\n",
            "Epoch 1632 | Train Loss: 0.000239 | Validation Loss: 0.000314\n",
            "Epoch 1633 | Train Loss: 0.000234 | Validation Loss: 0.000300\n",
            "Epoch 1634 | Train Loss: 0.000236 | Validation Loss: 0.000300\n",
            "Epoch 1635 | Train Loss: 0.000253 | Validation Loss: 0.000286\n",
            "Epoch 1636 | Train Loss: 0.000245 | Validation Loss: 0.000296\n",
            "Epoch 1637 | Train Loss: 0.000238 | Validation Loss: 0.000306\n",
            "Epoch 1638 | Train Loss: 0.000228 | Validation Loss: 0.000293\n",
            "Epoch 1639 | Train Loss: 0.000236 | Validation Loss: 0.000283\n",
            "Epoch 1640 | Train Loss: 0.000240 | Validation Loss: 0.000308\n",
            "Epoch 1641 | Train Loss: 0.000236 | Validation Loss: 0.000283\n",
            "Epoch 1642 | Train Loss: 0.000246 | Validation Loss: 0.000307\n",
            "Epoch 1643 | Train Loss: 0.000232 | Validation Loss: 0.000295\n",
            "Epoch 1644 | Train Loss: 0.000249 | Validation Loss: 0.000287\n",
            "Epoch 1645 | Train Loss: 0.000270 | Validation Loss: 0.000292\n",
            "Epoch 1646 | Train Loss: 0.000263 | Validation Loss: 0.000329\n",
            "Epoch 1647 | Train Loss: 0.000277 | Validation Loss: 0.000282\n",
            "Epoch 1648 | Train Loss: 0.000263 | Validation Loss: 0.000335\n",
            "Epoch 1649 | Train Loss: 0.000265 | Validation Loss: 0.000298\n",
            "Epoch 1650 | Train Loss: 0.000262 | Validation Loss: 0.000310\n",
            "Epoch 1651 | Train Loss: 0.000249 | Validation Loss: 0.000330\n",
            "Epoch 1652 | Train Loss: 0.000243 | Validation Loss: 0.000306\n",
            "Epoch 1653 | Train Loss: 0.000240 | Validation Loss: 0.000290\n",
            "Epoch 1654 | Train Loss: 0.000252 | Validation Loss: 0.000294\n",
            "Epoch 1655 | Train Loss: 0.000249 | Validation Loss: 0.000300\n",
            "Epoch 1656 | Train Loss: 0.000245 | Validation Loss: 0.000310\n",
            "Epoch 1657 | Train Loss: 0.000254 | Validation Loss: 0.000300\n",
            "Epoch 1658 | Train Loss: 0.000225 | Validation Loss: 0.000307\n",
            "Epoch 1659 | Train Loss: 0.000254 | Validation Loss: 0.000300\n",
            "Epoch 1660 | Train Loss: 0.000245 | Validation Loss: 0.000291\n",
            "Epoch 1661 | Train Loss: 0.000250 | Validation Loss: 0.000283\n",
            "Epoch 1662 | Train Loss: 0.000224 | Validation Loss: 0.000298\n",
            "Epoch 1663 | Train Loss: 0.000239 | Validation Loss: 0.000322\n",
            "Epoch 1664 | Train Loss: 0.000230 | Validation Loss: 0.000314\n",
            "Epoch 1665 | Train Loss: 0.000226 | Validation Loss: 0.000291\n",
            "Epoch 1666 | Train Loss: 0.000240 | Validation Loss: 0.000298\n",
            "Epoch 1667 | Train Loss: 0.000241 | Validation Loss: 0.000285\n",
            "Epoch 1668 | Train Loss: 0.000256 | Validation Loss: 0.000304\n",
            "Epoch 1669 | Train Loss: 0.000249 | Validation Loss: 0.000294\n",
            "Epoch 1670 | Train Loss: 0.000236 | Validation Loss: 0.000296\n",
            "Epoch 1671 | Train Loss: 0.000255 | Validation Loss: 0.000298\n",
            "Epoch 1672 | Train Loss: 0.000247 | Validation Loss: 0.000296\n",
            "Epoch 1673 | Train Loss: 0.000252 | Validation Loss: 0.000299\n",
            "Epoch 1674 | Train Loss: 0.000269 | Validation Loss: 0.000318\n",
            "Epoch 1675 | Train Loss: 0.000238 | Validation Loss: 0.000298\n",
            "Epoch 1676 | Train Loss: 0.000232 | Validation Loss: 0.000277\n",
            "Epoch 1677 | Train Loss: 0.000257 | Validation Loss: 0.000288\n",
            "Epoch 1678 | Train Loss: 0.000271 | Validation Loss: 0.000301\n",
            "Epoch 1679 | Train Loss: 0.000229 | Validation Loss: 0.000285\n",
            "Epoch 1680 | Train Loss: 0.000231 | Validation Loss: 0.000285\n",
            "Epoch 1681 | Train Loss: 0.000232 | Validation Loss: 0.000292\n",
            "Epoch 1682 | Train Loss: 0.000228 | Validation Loss: 0.000292\n",
            "Epoch 1683 | Train Loss: 0.000226 | Validation Loss: 0.000284\n",
            "Epoch 1684 | Train Loss: 0.000227 | Validation Loss: 0.000292\n",
            "Epoch 1685 | Train Loss: 0.000246 | Validation Loss: 0.000275\n",
            "Epoch 1686 | Train Loss: 0.000232 | Validation Loss: 0.000300\n",
            "Epoch 1687 | Train Loss: 0.000242 | Validation Loss: 0.000295\n",
            "Epoch 1688 | Train Loss: 0.000244 | Validation Loss: 0.000310\n",
            "Epoch 1689 | Train Loss: 0.000250 | Validation Loss: 0.000311\n",
            "Epoch 1690 | Train Loss: 0.000243 | Validation Loss: 0.000293\n",
            "Epoch 1691 | Train Loss: 0.000246 | Validation Loss: 0.000279\n",
            "Epoch 1692 | Train Loss: 0.000229 | Validation Loss: 0.000308\n",
            "Epoch 1693 | Train Loss: 0.000231 | Validation Loss: 0.000294\n",
            "Epoch 1694 | Train Loss: 0.000242 | Validation Loss: 0.000314\n",
            "Epoch 1695 | Train Loss: 0.000229 | Validation Loss: 0.000292\n",
            "Epoch 1696 | Train Loss: 0.000241 | Validation Loss: 0.000296\n",
            "Epoch 1697 | Train Loss: 0.000249 | Validation Loss: 0.000309\n",
            "Epoch 1698 | Train Loss: 0.000241 | Validation Loss: 0.000288\n",
            "Epoch 1699 | Train Loss: 0.000233 | Validation Loss: 0.000311\n",
            "Epoch 1700 | Train Loss: 0.000236 | Validation Loss: 0.000280\n",
            "Epoch 1701 | Train Loss: 0.000238 | Validation Loss: 0.000299\n",
            "Epoch 1702 | Train Loss: 0.000224 | Validation Loss: 0.000297\n",
            "Epoch 1703 | Train Loss: 0.000238 | Validation Loss: 0.000288\n",
            "Epoch 1704 | Train Loss: 0.000237 | Validation Loss: 0.000290\n",
            "Epoch 1705 | Train Loss: 0.000236 | Validation Loss: 0.000306\n",
            "Epoch 1706 | Train Loss: 0.000242 | Validation Loss: 0.000296\n",
            "Epoch 1707 | Train Loss: 0.000240 | Validation Loss: 0.000309\n",
            "Epoch 1708 | Train Loss: 0.000251 | Validation Loss: 0.000320\n",
            "Epoch 1709 | Train Loss: 0.000237 | Validation Loss: 0.000277\n",
            "Epoch 1710 | Train Loss: 0.000248 | Validation Loss: 0.000321\n",
            "Epoch 1711 | Train Loss: 0.000244 | Validation Loss: 0.000268\n",
            "Epoch 1712 | Train Loss: 0.000230 | Validation Loss: 0.000294\n",
            "Epoch 1713 | Train Loss: 0.000253 | Validation Loss: 0.000325\n",
            "Epoch 1714 | Train Loss: 0.000257 | Validation Loss: 0.000299\n",
            "Epoch 1715 | Train Loss: 0.000262 | Validation Loss: 0.000319\n",
            "Epoch 1716 | Train Loss: 0.000249 | Validation Loss: 0.000269\n",
            "Epoch 1717 | Train Loss: 0.000233 | Validation Loss: 0.000304\n",
            "Epoch 1718 | Train Loss: 0.000245 | Validation Loss: 0.000332\n",
            "Epoch 1719 | Train Loss: 0.000250 | Validation Loss: 0.000301\n",
            "Epoch 1720 | Train Loss: 0.000233 | Validation Loss: 0.000279\n",
            "Epoch 1721 | Train Loss: 0.000232 | Validation Loss: 0.000290\n",
            "Epoch 1722 | Train Loss: 0.000231 | Validation Loss: 0.000293\n",
            "Epoch 1723 | Train Loss: 0.000262 | Validation Loss: 0.000299\n",
            "Epoch 1724 | Train Loss: 0.000234 | Validation Loss: 0.000305\n",
            "Epoch 1725 | Train Loss: 0.000253 | Validation Loss: 0.000304\n",
            "Epoch 1726 | Train Loss: 0.000261 | Validation Loss: 0.000300\n",
            "Epoch 1727 | Train Loss: 0.000227 | Validation Loss: 0.000303\n",
            "Epoch 1728 | Train Loss: 0.000226 | Validation Loss: 0.000287\n",
            "Epoch 1729 | Train Loss: 0.000247 | Validation Loss: 0.000294\n",
            "Epoch 1730 | Train Loss: 0.000226 | Validation Loss: 0.000330\n",
            "Epoch 1731 | Train Loss: 0.000252 | Validation Loss: 0.000318\n",
            "Epoch 1732 | Train Loss: 0.000242 | Validation Loss: 0.000308\n",
            "Epoch 1733 | Train Loss: 0.000232 | Validation Loss: 0.000300\n",
            "Epoch 1734 | Train Loss: 0.000236 | Validation Loss: 0.000322\n",
            "Epoch 1735 | Train Loss: 0.000235 | Validation Loss: 0.000308\n",
            "Epoch 1736 | Train Loss: 0.000236 | Validation Loss: 0.000296\n",
            "Epoch 1737 | Train Loss: 0.000233 | Validation Loss: 0.000281\n",
            "Epoch 1738 | Train Loss: 0.000228 | Validation Loss: 0.000303\n",
            "Epoch 1739 | Train Loss: 0.000234 | Validation Loss: 0.000294\n",
            "Epoch 1740 | Train Loss: 0.000231 | Validation Loss: 0.000294\n",
            "Epoch 1741 | Train Loss: 0.000230 | Validation Loss: 0.000286\n",
            "Epoch 1742 | Train Loss: 0.000232 | Validation Loss: 0.000273\n",
            "Epoch 1743 | Train Loss: 0.000224 | Validation Loss: 0.000283\n",
            "Epoch 1744 | Train Loss: 0.000239 | Validation Loss: 0.000323\n",
            "Epoch 1745 | Train Loss: 0.000264 | Validation Loss: 0.000333\n",
            "Epoch 1746 | Train Loss: 0.000247 | Validation Loss: 0.000303\n",
            "Epoch 1747 | Train Loss: 0.000254 | Validation Loss: 0.000292\n",
            "Epoch 1748 | Train Loss: 0.000251 | Validation Loss: 0.000299\n",
            "Epoch 1749 | Train Loss: 0.000255 | Validation Loss: 0.000325\n",
            "Epoch 1750 | Train Loss: 0.000235 | Validation Loss: 0.000305\n",
            "Epoch 1751 | Train Loss: 0.000234 | Validation Loss: 0.000300\n",
            "Epoch 1752 | Train Loss: 0.000234 | Validation Loss: 0.000292\n",
            "Epoch 1753 | Train Loss: 0.000232 | Validation Loss: 0.000303\n",
            "Epoch 1754 | Train Loss: 0.000219 | Validation Loss: 0.000302\n",
            "Epoch 1755 | Train Loss: 0.000246 | Validation Loss: 0.000291\n",
            "Epoch 1756 | Train Loss: 0.000241 | Validation Loss: 0.000280\n",
            "Epoch 1757 | Train Loss: 0.000246 | Validation Loss: 0.000322\n",
            "Epoch 1758 | Train Loss: 0.000267 | Validation Loss: 0.000313\n",
            "Epoch 1759 | Train Loss: 0.000243 | Validation Loss: 0.000286\n",
            "Epoch 1760 | Train Loss: 0.000234 | Validation Loss: 0.000292\n",
            "Epoch 1761 | Train Loss: 0.000224 | Validation Loss: 0.000286\n",
            "Epoch 1762 | Train Loss: 0.000246 | Validation Loss: 0.000312\n",
            "Epoch 1763 | Train Loss: 0.000256 | Validation Loss: 0.000312\n",
            "Epoch 1764 | Train Loss: 0.000250 | Validation Loss: 0.000311\n",
            "Epoch 1765 | Train Loss: 0.000255 | Validation Loss: 0.000323\n",
            "Epoch 1766 | Train Loss: 0.000231 | Validation Loss: 0.000309\n",
            "Epoch 1767 | Train Loss: 0.000242 | Validation Loss: 0.000301\n",
            "Epoch 1768 | Train Loss: 0.000231 | Validation Loss: 0.000294\n",
            "Epoch 1769 | Train Loss: 0.000225 | Validation Loss: 0.000304\n",
            "Epoch 1770 | Train Loss: 0.000235 | Validation Loss: 0.000303\n",
            "Epoch 1771 | Train Loss: 0.000258 | Validation Loss: 0.000314\n",
            "Epoch 1772 | Train Loss: 0.000250 | Validation Loss: 0.000305\n",
            "Epoch 1773 | Train Loss: 0.000230 | Validation Loss: 0.000297\n",
            "Epoch 1774 | Train Loss: 0.000224 | Validation Loss: 0.000320\n",
            "Epoch 1775 | Train Loss: 0.000227 | Validation Loss: 0.000309\n",
            "Epoch 1776 | Train Loss: 0.000232 | Validation Loss: 0.000305\n",
            "Epoch 1777 | Train Loss: 0.000221 | Validation Loss: 0.000300\n",
            "Epoch 1778 | Train Loss: 0.000223 | Validation Loss: 0.000295\n",
            "Epoch 1779 | Train Loss: 0.000217 | Validation Loss: 0.000293\n",
            "Epoch 1780 | Train Loss: 0.000238 | Validation Loss: 0.000295\n",
            "Epoch 1781 | Train Loss: 0.000230 | Validation Loss: 0.000303\n",
            "Epoch 1782 | Train Loss: 0.000232 | Validation Loss: 0.000294\n",
            "Epoch 1783 | Train Loss: 0.000242 | Validation Loss: 0.000302\n",
            "Epoch 1784 | Train Loss: 0.000238 | Validation Loss: 0.000290\n",
            "Epoch 1785 | Train Loss: 0.000248 | Validation Loss: 0.000305\n",
            "Epoch 1786 | Train Loss: 0.000229 | Validation Loss: 0.000291\n",
            "Epoch 1787 | Train Loss: 0.000232 | Validation Loss: 0.000283\n",
            "Epoch 1788 | Train Loss: 0.000235 | Validation Loss: 0.000298\n",
            "Epoch 1789 | Train Loss: 0.000242 | Validation Loss: 0.000291\n",
            "Epoch 1790 | Train Loss: 0.000224 | Validation Loss: 0.000290\n",
            "Epoch 1791 | Train Loss: 0.000228 | Validation Loss: 0.000293\n",
            "Epoch 1792 | Train Loss: 0.000228 | Validation Loss: 0.000275\n",
            "Epoch 1793 | Train Loss: 0.000232 | Validation Loss: 0.000299\n",
            "Epoch 1794 | Train Loss: 0.000228 | Validation Loss: 0.000298\n",
            "Epoch 1795 | Train Loss: 0.000236 | Validation Loss: 0.000283\n",
            "Epoch 1796 | Train Loss: 0.000239 | Validation Loss: 0.000290\n",
            "Epoch 1797 | Train Loss: 0.000250 | Validation Loss: 0.000305\n",
            "Epoch 1798 | Train Loss: 0.000239 | Validation Loss: 0.000292\n",
            "Epoch 1799 | Train Loss: 0.000229 | Validation Loss: 0.000285\n",
            "Epoch 1800 | Train Loss: 0.000229 | Validation Loss: 0.000288\n",
            "Epoch 1801 | Train Loss: 0.000233 | Validation Loss: 0.000324\n",
            "Epoch 1802 | Train Loss: 0.000243 | Validation Loss: 0.000310\n",
            "Epoch 1803 | Train Loss: 0.000250 | Validation Loss: 0.000300\n",
            "Epoch 1804 | Train Loss: 0.000240 | Validation Loss: 0.000334\n",
            "Epoch 1805 | Train Loss: 0.000236 | Validation Loss: 0.000308\n",
            "Epoch 1806 | Train Loss: 0.000231 | Validation Loss: 0.000326\n",
            "Epoch 1807 | Train Loss: 0.000249 | Validation Loss: 0.000310\n",
            "Epoch 1808 | Train Loss: 0.000246 | Validation Loss: 0.000310\n",
            "Epoch 1809 | Train Loss: 0.000241 | Validation Loss: 0.000291\n",
            "Epoch 1810 | Train Loss: 0.000222 | Validation Loss: 0.000287\n",
            "Epoch 1811 | Train Loss: 0.000218 | Validation Loss: 0.000271\n",
            "Epoch 1812 | Train Loss: 0.000221 | Validation Loss: 0.000281\n",
            "Epoch 1813 | Train Loss: 0.000226 | Validation Loss: 0.000285\n",
            "Epoch 1814 | Train Loss: 0.000235 | Validation Loss: 0.000273\n",
            "Epoch 1815 | Train Loss: 0.000234 | Validation Loss: 0.000297\n",
            "Epoch 1816 | Train Loss: 0.000252 | Validation Loss: 0.000307\n",
            "Epoch 1817 | Train Loss: 0.000239 | Validation Loss: 0.000304\n",
            "Epoch 1818 | Train Loss: 0.000236 | Validation Loss: 0.000287\n",
            "Epoch 1819 | Train Loss: 0.000230 | Validation Loss: 0.000277\n",
            "Epoch 1820 | Train Loss: 0.000220 | Validation Loss: 0.000282\n",
            "Epoch 1821 | Train Loss: 0.000228 | Validation Loss: 0.000288\n",
            "Epoch 1822 | Train Loss: 0.000232 | Validation Loss: 0.000291\n",
            "Epoch 1823 | Train Loss: 0.000240 | Validation Loss: 0.000306\n",
            "Epoch 1824 | Train Loss: 0.000228 | Validation Loss: 0.000283\n",
            "Epoch 1825 | Train Loss: 0.000238 | Validation Loss: 0.000308\n",
            "Epoch 1826 | Train Loss: 0.000236 | Validation Loss: 0.000278\n",
            "Epoch 1827 | Train Loss: 0.000226 | Validation Loss: 0.000293\n",
            "Epoch 1828 | Train Loss: 0.000227 | Validation Loss: 0.000289\n",
            "Epoch 1829 | Train Loss: 0.000220 | Validation Loss: 0.000296\n",
            "Epoch 1830 | Train Loss: 0.000245 | Validation Loss: 0.000322\n",
            "Epoch 1831 | Train Loss: 0.000245 | Validation Loss: 0.000311\n",
            "Epoch 1832 | Train Loss: 0.000268 | Validation Loss: 0.000303\n",
            "Epoch 1833 | Train Loss: 0.000237 | Validation Loss: 0.000310\n",
            "Epoch 1834 | Train Loss: 0.000248 | Validation Loss: 0.000299\n",
            "Epoch 1835 | Train Loss: 0.000229 | Validation Loss: 0.000299\n",
            "Epoch 1836 | Train Loss: 0.000233 | Validation Loss: 0.000291\n",
            "Epoch 1837 | Train Loss: 0.000221 | Validation Loss: 0.000283\n",
            "Epoch 1838 | Train Loss: 0.000227 | Validation Loss: 0.000290\n",
            "Epoch 1839 | Train Loss: 0.000237 | Validation Loss: 0.000299\n",
            "Epoch 1840 | Train Loss: 0.000228 | Validation Loss: 0.000305\n",
            "Epoch 1841 | Train Loss: 0.000245 | Validation Loss: 0.000309\n",
            "Epoch 1842 | Train Loss: 0.000232 | Validation Loss: 0.000299\n",
            "Epoch 1843 | Train Loss: 0.000233 | Validation Loss: 0.000287\n",
            "Epoch 1844 | Train Loss: 0.000235 | Validation Loss: 0.000282\n",
            "Epoch 1845 | Train Loss: 0.000227 | Validation Loss: 0.000292\n",
            "Epoch 1846 | Train Loss: 0.000234 | Validation Loss: 0.000282\n",
            "Epoch 1847 | Train Loss: 0.000241 | Validation Loss: 0.000298\n",
            "Epoch 1848 | Train Loss: 0.000240 | Validation Loss: 0.000303\n",
            "Epoch 1849 | Train Loss: 0.000258 | Validation Loss: 0.000294\n",
            "Epoch 1850 | Train Loss: 0.000238 | Validation Loss: 0.000294\n",
            "Epoch 1851 | Train Loss: 0.000235 | Validation Loss: 0.000300\n",
            "Epoch 1852 | Train Loss: 0.000236 | Validation Loss: 0.000299\n",
            "Epoch 1853 | Train Loss: 0.000242 | Validation Loss: 0.000317\n",
            "Epoch 1854 | Train Loss: 0.000244 | Validation Loss: 0.000288\n",
            "Epoch 1855 | Train Loss: 0.000238 | Validation Loss: 0.000294\n",
            "Epoch 1856 | Train Loss: 0.000237 | Validation Loss: 0.000303\n",
            "Epoch 1857 | Train Loss: 0.000230 | Validation Loss: 0.000293\n",
            "Epoch 1858 | Train Loss: 0.000226 | Validation Loss: 0.000311\n",
            "Epoch 1859 | Train Loss: 0.000221 | Validation Loss: 0.000296\n",
            "Epoch 1860 | Train Loss: 0.000235 | Validation Loss: 0.000297\n",
            "Epoch 1861 | Train Loss: 0.000232 | Validation Loss: 0.000304\n",
            "Epoch 1862 | Train Loss: 0.000225 | Validation Loss: 0.000296\n",
            "Epoch 1863 | Train Loss: 0.000225 | Validation Loss: 0.000305\n",
            "Epoch 1864 | Train Loss: 0.000233 | Validation Loss: 0.000301\n",
            "Epoch 1865 | Train Loss: 0.000231 | Validation Loss: 0.000289\n",
            "Epoch 1866 | Train Loss: 0.000238 | Validation Loss: 0.000299\n",
            "Epoch 1867 | Train Loss: 0.000235 | Validation Loss: 0.000304\n",
            "Epoch 1868 | Train Loss: 0.000226 | Validation Loss: 0.000305\n",
            "Epoch 1869 | Train Loss: 0.000232 | Validation Loss: 0.000293\n",
            "Epoch 1870 | Train Loss: 0.000228 | Validation Loss: 0.000287\n",
            "Epoch 1871 | Train Loss: 0.000219 | Validation Loss: 0.000280\n",
            "Epoch 1872 | Train Loss: 0.000226 | Validation Loss: 0.000277\n",
            "Epoch 1873 | Train Loss: 0.000214 | Validation Loss: 0.000307\n",
            "Epoch 1874 | Train Loss: 0.000234 | Validation Loss: 0.000319\n",
            "Epoch 1875 | Train Loss: 0.000260 | Validation Loss: 0.000318\n",
            "Epoch 1876 | Train Loss: 0.000241 | Validation Loss: 0.000289\n",
            "Epoch 1877 | Train Loss: 0.000241 | Validation Loss: 0.000306\n",
            "Epoch 1878 | Train Loss: 0.000224 | Validation Loss: 0.000310\n",
            "Epoch 1879 | Train Loss: 0.000224 | Validation Loss: 0.000292\n",
            "Epoch 1880 | Train Loss: 0.000231 | Validation Loss: 0.000297\n",
            "Epoch 1881 | Train Loss: 0.000230 | Validation Loss: 0.000296\n",
            "Epoch 1882 | Train Loss: 0.000230 | Validation Loss: 0.000282\n",
            "Epoch 1883 | Train Loss: 0.000236 | Validation Loss: 0.000323\n",
            "Epoch 1884 | Train Loss: 0.000236 | Validation Loss: 0.000279\n",
            "Epoch 1885 | Train Loss: 0.000250 | Validation Loss: 0.000298\n",
            "Epoch 1886 | Train Loss: 0.000241 | Validation Loss: 0.000294\n",
            "Epoch 1887 | Train Loss: 0.000226 | Validation Loss: 0.000288\n",
            "Epoch 1888 | Train Loss: 0.000232 | Validation Loss: 0.000280\n",
            "Epoch 1889 | Train Loss: 0.000230 | Validation Loss: 0.000301\n",
            "Epoch 1890 | Train Loss: 0.000238 | Validation Loss: 0.000283\n",
            "Epoch 1891 | Train Loss: 0.000237 | Validation Loss: 0.000314\n",
            "Epoch 1892 | Train Loss: 0.000229 | Validation Loss: 0.000292\n",
            "Epoch 1893 | Train Loss: 0.000227 | Validation Loss: 0.000283\n",
            "Epoch 1894 | Train Loss: 0.000235 | Validation Loss: 0.000295\n",
            "Epoch 1895 | Train Loss: 0.000251 | Validation Loss: 0.000319\n",
            "Epoch 1896 | Train Loss: 0.000245 | Validation Loss: 0.000306\n",
            "Epoch 1897 | Train Loss: 0.000234 | Validation Loss: 0.000295\n",
            "Epoch 1898 | Train Loss: 0.000231 | Validation Loss: 0.000295\n",
            "Epoch 1899 | Train Loss: 0.000235 | Validation Loss: 0.000308\n",
            "Epoch 1900 | Train Loss: 0.000232 | Validation Loss: 0.000306\n",
            "Epoch 1901 | Train Loss: 0.000231 | Validation Loss: 0.000280\n",
            "Epoch 1902 | Train Loss: 0.000224 | Validation Loss: 0.000297\n",
            "Epoch 1903 | Train Loss: 0.000226 | Validation Loss: 0.000294\n",
            "Epoch 1904 | Train Loss: 0.000232 | Validation Loss: 0.000291\n",
            "Epoch 1905 | Train Loss: 0.000258 | Validation Loss: 0.000281\n",
            "Epoch 1906 | Train Loss: 0.000236 | Validation Loss: 0.000288\n",
            "Epoch 1907 | Train Loss: 0.000236 | Validation Loss: 0.000296\n",
            "Epoch 1908 | Train Loss: 0.000236 | Validation Loss: 0.000294\n",
            "Epoch 1909 | Train Loss: 0.000240 | Validation Loss: 0.000293\n",
            "Epoch 1910 | Train Loss: 0.000231 | Validation Loss: 0.000319\n",
            "Epoch 1911 | Train Loss: 0.000234 | Validation Loss: 0.000287\n",
            "Epoch 1912 | Train Loss: 0.000220 | Validation Loss: 0.000303\n",
            "Epoch 1913 | Train Loss: 0.000231 | Validation Loss: 0.000285\n",
            "Epoch 1914 | Train Loss: 0.000223 | Validation Loss: 0.000299\n",
            "Epoch 1915 | Train Loss: 0.000221 | Validation Loss: 0.000295\n",
            "Epoch 1916 | Train Loss: 0.000213 | Validation Loss: 0.000276\n",
            "Epoch 1917 | Train Loss: 0.000244 | Validation Loss: 0.000294\n",
            "Epoch 1918 | Train Loss: 0.000241 | Validation Loss: 0.000308\n",
            "Epoch 1919 | Train Loss: 0.000229 | Validation Loss: 0.000300\n",
            "Epoch 1920 | Train Loss: 0.000225 | Validation Loss: 0.000291\n",
            "Epoch 1921 | Train Loss: 0.000221 | Validation Loss: 0.000272\n",
            "Epoch 1922 | Train Loss: 0.000216 | Validation Loss: 0.000272\n",
            "Epoch 1923 | Train Loss: 0.000222 | Validation Loss: 0.000279\n",
            "Epoch 1924 | Train Loss: 0.000225 | Validation Loss: 0.000283\n",
            "Epoch 1925 | Train Loss: 0.000250 | Validation Loss: 0.000318\n",
            "Epoch 1926 | Train Loss: 0.000240 | Validation Loss: 0.000318\n",
            "Epoch 1927 | Train Loss: 0.000245 | Validation Loss: 0.000307\n",
            "Epoch 1928 | Train Loss: 0.000236 | Validation Loss: 0.000288\n",
            "Epoch 1929 | Train Loss: 0.000231 | Validation Loss: 0.000267\n",
            "Epoch 1930 | Train Loss: 0.000231 | Validation Loss: 0.000319\n",
            "Epoch 1931 | Train Loss: 0.000252 | Validation Loss: 0.000309\n",
            "Epoch 1932 | Train Loss: 0.000239 | Validation Loss: 0.000301\n",
            "Epoch 1933 | Train Loss: 0.000231 | Validation Loss: 0.000288\n",
            "Epoch 1934 | Train Loss: 0.000227 | Validation Loss: 0.000300\n",
            "Epoch 1935 | Train Loss: 0.000236 | Validation Loss: 0.000315\n",
            "Epoch 1936 | Train Loss: 0.000236 | Validation Loss: 0.000274\n",
            "Epoch 1937 | Train Loss: 0.000239 | Validation Loss: 0.000295\n",
            "Epoch 1938 | Train Loss: 0.000216 | Validation Loss: 0.000276\n",
            "Epoch 1939 | Train Loss: 0.000230 | Validation Loss: 0.000292\n",
            "Epoch 1940 | Train Loss: 0.000213 | Validation Loss: 0.000277\n",
            "Epoch 1941 | Train Loss: 0.000217 | Validation Loss: 0.000297\n",
            "Epoch 1942 | Train Loss: 0.000221 | Validation Loss: 0.000285\n",
            "Epoch 1943 | Train Loss: 0.000234 | Validation Loss: 0.000285\n",
            "Epoch 1944 | Train Loss: 0.000237 | Validation Loss: 0.000291\n",
            "Epoch 1945 | Train Loss: 0.000238 | Validation Loss: 0.000284\n",
            "Epoch 1946 | Train Loss: 0.000230 | Validation Loss: 0.000299\n",
            "Epoch 1947 | Train Loss: 0.000249 | Validation Loss: 0.000303\n",
            "Epoch 1948 | Train Loss: 0.000230 | Validation Loss: 0.000291\n",
            "Epoch 1949 | Train Loss: 0.000241 | Validation Loss: 0.000275\n",
            "Epoch 1950 | Train Loss: 0.000223 | Validation Loss: 0.000294\n",
            "Epoch 1951 | Train Loss: 0.000228 | Validation Loss: 0.000299\n",
            "Epoch 1952 | Train Loss: 0.000224 | Validation Loss: 0.000286\n",
            "Epoch 1953 | Train Loss: 0.000216 | Validation Loss: 0.000293\n",
            "Epoch 1954 | Train Loss: 0.000220 | Validation Loss: 0.000294\n",
            "Epoch 1955 | Train Loss: 0.000226 | Validation Loss: 0.000293\n",
            "Epoch 1956 | Train Loss: 0.000227 | Validation Loss: 0.000283\n",
            "Epoch 1957 | Train Loss: 0.000227 | Validation Loss: 0.000274\n",
            "Epoch 1958 | Train Loss: 0.000233 | Validation Loss: 0.000304\n",
            "Epoch 1959 | Train Loss: 0.000235 | Validation Loss: 0.000297\n",
            "Epoch 1960 | Train Loss: 0.000221 | Validation Loss: 0.000279\n",
            "Epoch 1961 | Train Loss: 0.000227 | Validation Loss: 0.000274\n",
            "Epoch 1962 | Train Loss: 0.000234 | Validation Loss: 0.000287\n",
            "Epoch 1963 | Train Loss: 0.000246 | Validation Loss: 0.000296\n",
            "Epoch 1964 | Train Loss: 0.000226 | Validation Loss: 0.000274\n",
            "Epoch 1965 | Train Loss: 0.000226 | Validation Loss: 0.000295\n",
            "Epoch 1966 | Train Loss: 0.000230 | Validation Loss: 0.000276\n",
            "Epoch 1967 | Train Loss: 0.000239 | Validation Loss: 0.000285\n",
            "Epoch 1968 | Train Loss: 0.000218 | Validation Loss: 0.000283\n",
            "Epoch 1969 | Train Loss: 0.000221 | Validation Loss: 0.000295\n",
            "Epoch 1970 | Train Loss: 0.000221 | Validation Loss: 0.000285\n",
            "Epoch 1971 | Train Loss: 0.000230 | Validation Loss: 0.000301\n",
            "Epoch 1972 | Train Loss: 0.000229 | Validation Loss: 0.000290\n",
            "Epoch 1973 | Train Loss: 0.000225 | Validation Loss: 0.000288\n",
            "Epoch 1974 | Train Loss: 0.000236 | Validation Loss: 0.000299\n",
            "Epoch 1975 | Train Loss: 0.000235 | Validation Loss: 0.000324\n",
            "Epoch 1976 | Train Loss: 0.000234 | Validation Loss: 0.000315\n",
            "Epoch 1977 | Train Loss: 0.000232 | Validation Loss: 0.000298\n",
            "Epoch 1978 | Train Loss: 0.000228 | Validation Loss: 0.000311\n",
            "Epoch 1979 | Train Loss: 0.000231 | Validation Loss: 0.000287\n",
            "Epoch 1980 | Train Loss: 0.000226 | Validation Loss: 0.000281\n",
            "Epoch 1981 | Train Loss: 0.000221 | Validation Loss: 0.000274\n",
            "Epoch 1982 | Train Loss: 0.000219 | Validation Loss: 0.000291\n",
            "Epoch 1983 | Train Loss: 0.000220 | Validation Loss: 0.000287\n",
            "Epoch 1984 | Train Loss: 0.000243 | Validation Loss: 0.000296\n",
            "Epoch 1985 | Train Loss: 0.000237 | Validation Loss: 0.000285\n",
            "Epoch 1986 | Train Loss: 0.000221 | Validation Loss: 0.000271\n",
            "Epoch 1987 | Train Loss: 0.000230 | Validation Loss: 0.000296\n",
            "Epoch 1988 | Train Loss: 0.000223 | Validation Loss: 0.000295\n",
            "Epoch 1989 | Train Loss: 0.000220 | Validation Loss: 0.000287\n",
            "Epoch 1990 | Train Loss: 0.000218 | Validation Loss: 0.000278\n",
            "Epoch 1991 | Train Loss: 0.000229 | Validation Loss: 0.000282\n",
            "Epoch 1992 | Train Loss: 0.000229 | Validation Loss: 0.000309\n",
            "Epoch 1993 | Train Loss: 0.000248 | Validation Loss: 0.000318\n",
            "Epoch 1994 | Train Loss: 0.000223 | Validation Loss: 0.000316\n",
            "Epoch 1995 | Train Loss: 0.000227 | Validation Loss: 0.000288\n",
            "Epoch 1996 | Train Loss: 0.000218 | Validation Loss: 0.000285\n",
            "Epoch 1997 | Train Loss: 0.000225 | Validation Loss: 0.000276\n",
            "Epoch 1998 | Train Loss: 0.000220 | Validation Loss: 0.000288\n",
            "Epoch 1999 | Train Loss: 0.000229 | Validation Loss: 0.000295\n",
            "Epoch 2000 | Train Loss: 0.000235 | Validation Loss: 0.000292\n",
            "Epoch 2001 | Train Loss: 0.000219 | Validation Loss: 0.000285\n",
            "Epoch 2002 | Train Loss: 0.000229 | Validation Loss: 0.000301\n",
            "Epoch 2003 | Train Loss: 0.000226 | Validation Loss: 0.000287\n",
            "Epoch 2004 | Train Loss: 0.000233 | Validation Loss: 0.000293\n",
            "Epoch 2005 | Train Loss: 0.000233 | Validation Loss: 0.000306\n",
            "Epoch 2006 | Train Loss: 0.000247 | Validation Loss: 0.000317\n",
            "Epoch 2007 | Train Loss: 0.000251 | Validation Loss: 0.000323\n",
            "Epoch 2008 | Train Loss: 0.000227 | Validation Loss: 0.000300\n",
            "Epoch 2009 | Train Loss: 0.000227 | Validation Loss: 0.000279\n",
            "Epoch 2010 | Train Loss: 0.000218 | Validation Loss: 0.000291\n",
            "Epoch 2011 | Train Loss: 0.000239 | Validation Loss: 0.000341\n",
            "Epoch 2012 | Train Loss: 0.000229 | Validation Loss: 0.000321\n",
            "Epoch 2013 | Train Loss: 0.000221 | Validation Loss: 0.000303\n",
            "Epoch 2014 | Train Loss: 0.000237 | Validation Loss: 0.000280\n",
            "Epoch 2015 | Train Loss: 0.000237 | Validation Loss: 0.000289\n",
            "Epoch 2016 | Train Loss: 0.000229 | Validation Loss: 0.000309\n",
            "Epoch 2017 | Train Loss: 0.000241 | Validation Loss: 0.000328\n",
            "Epoch 2018 | Train Loss: 0.000233 | Validation Loss: 0.000318\n",
            "Epoch 2019 | Train Loss: 0.000227 | Validation Loss: 0.000299\n",
            "Epoch 2020 | Train Loss: 0.000225 | Validation Loss: 0.000307\n",
            "Epoch 2021 | Train Loss: 0.000220 | Validation Loss: 0.000303\n",
            "Epoch 2022 | Train Loss: 0.000218 | Validation Loss: 0.000310\n",
            "Epoch 2023 | Train Loss: 0.000219 | Validation Loss: 0.000287\n",
            "Epoch 2024 | Train Loss: 0.000212 | Validation Loss: 0.000292\n",
            "Epoch 2025 | Train Loss: 0.000237 | Validation Loss: 0.000275\n",
            "Epoch 2026 | Train Loss: 0.000237 | Validation Loss: 0.000312\n",
            "Epoch 2027 | Train Loss: 0.000237 | Validation Loss: 0.000297\n",
            "Epoch 2028 | Train Loss: 0.000231 | Validation Loss: 0.000282\n",
            "Epoch 2029 | Train Loss: 0.000229 | Validation Loss: 0.000281\n",
            "Epoch 2030 | Train Loss: 0.000226 | Validation Loss: 0.000289\n",
            "Epoch 2031 | Train Loss: 0.000234 | Validation Loss: 0.000302\n",
            "Epoch 2032 | Train Loss: 0.000235 | Validation Loss: 0.000322\n",
            "Epoch 2033 | Train Loss: 0.000233 | Validation Loss: 0.000299\n",
            "Epoch 2034 | Train Loss: 0.000224 | Validation Loss: 0.000289\n",
            "Epoch 2035 | Train Loss: 0.000247 | Validation Loss: 0.000302\n",
            "Epoch 2036 | Train Loss: 0.000248 | Validation Loss: 0.000310\n",
            "Epoch 2037 | Train Loss: 0.000236 | Validation Loss: 0.000275\n",
            "Epoch 2038 | Train Loss: 0.000233 | Validation Loss: 0.000298\n",
            "Epoch 2039 | Train Loss: 0.000239 | Validation Loss: 0.000303\n",
            "Epoch 2040 | Train Loss: 0.000229 | Validation Loss: 0.000324\n",
            "Epoch 2041 | Train Loss: 0.000224 | Validation Loss: 0.000300\n",
            "Epoch 2042 | Train Loss: 0.000230 | Validation Loss: 0.000310\n",
            "Epoch 2043 | Train Loss: 0.000227 | Validation Loss: 0.000301\n",
            "Epoch 2044 | Train Loss: 0.000221 | Validation Loss: 0.000291\n",
            "Epoch 2045 | Train Loss: 0.000228 | Validation Loss: 0.000298\n",
            "Epoch 2046 | Train Loss: 0.000207 | Validation Loss: 0.000304\n",
            "Epoch 2047 | Train Loss: 0.000236 | Validation Loss: 0.000288\n",
            "Epoch 2048 | Train Loss: 0.000229 | Validation Loss: 0.000318\n",
            "Epoch 2049 | Train Loss: 0.000228 | Validation Loss: 0.000278\n",
            "Epoch 2050 | Train Loss: 0.000222 | Validation Loss: 0.000287\n",
            "Epoch 2051 | Train Loss: 0.000223 | Validation Loss: 0.000293\n",
            "Epoch 2052 | Train Loss: 0.000221 | Validation Loss: 0.000275\n",
            "Epoch 2053 | Train Loss: 0.000235 | Validation Loss: 0.000313\n",
            "Epoch 2054 | Train Loss: 0.000226 | Validation Loss: 0.000276\n",
            "Epoch 2055 | Train Loss: 0.000219 | Validation Loss: 0.000274\n",
            "Epoch 2056 | Train Loss: 0.000227 | Validation Loss: 0.000279\n",
            "Epoch 2057 | Train Loss: 0.000242 | Validation Loss: 0.000305\n",
            "Epoch 2058 | Train Loss: 0.000222 | Validation Loss: 0.000292\n",
            "Epoch 2059 | Train Loss: 0.000231 | Validation Loss: 0.000293\n",
            "Epoch 2060 | Train Loss: 0.000238 | Validation Loss: 0.000291\n",
            "Epoch 2061 | Train Loss: 0.000221 | Validation Loss: 0.000308\n",
            "Epoch 2062 | Train Loss: 0.000213 | Validation Loss: 0.000293\n",
            "Epoch 2063 | Train Loss: 0.000217 | Validation Loss: 0.000285\n",
            "Epoch 2064 | Train Loss: 0.000225 | Validation Loss: 0.000304\n",
            "Epoch 2065 | Train Loss: 0.000227 | Validation Loss: 0.000295\n",
            "Epoch 2066 | Train Loss: 0.000226 | Validation Loss: 0.000299\n",
            "Epoch 2067 | Train Loss: 0.000227 | Validation Loss: 0.000296\n",
            "Epoch 2068 | Train Loss: 0.000221 | Validation Loss: 0.000283\n",
            "Epoch 2069 | Train Loss: 0.000216 | Validation Loss: 0.000302\n",
            "Epoch 2070 | Train Loss: 0.000228 | Validation Loss: 0.000285\n",
            "Epoch 2071 | Train Loss: 0.000238 | Validation Loss: 0.000308\n",
            "Epoch 2072 | Train Loss: 0.000256 | Validation Loss: 0.000318\n",
            "Epoch 2073 | Train Loss: 0.000246 | Validation Loss: 0.000321\n",
            "Epoch 2074 | Train Loss: 0.000231 | Validation Loss: 0.000306\n",
            "Epoch 2075 | Train Loss: 0.000232 | Validation Loss: 0.000275\n",
            "Epoch 2076 | Train Loss: 0.000224 | Validation Loss: 0.000301\n",
            "Epoch 2077 | Train Loss: 0.000218 | Validation Loss: 0.000304\n",
            "Epoch 2078 | Train Loss: 0.000221 | Validation Loss: 0.000278\n",
            "Epoch 2079 | Train Loss: 0.000225 | Validation Loss: 0.000280\n",
            "Epoch 2080 | Train Loss: 0.000233 | Validation Loss: 0.000287\n",
            "Epoch 2081 | Train Loss: 0.000228 | Validation Loss: 0.000290\n",
            "Epoch 2082 | Train Loss: 0.000220 | Validation Loss: 0.000293\n",
            "Epoch 2083 | Train Loss: 0.000230 | Validation Loss: 0.000299\n",
            "Epoch 2084 | Train Loss: 0.000238 | Validation Loss: 0.000290\n",
            "Epoch 2085 | Train Loss: 0.000233 | Validation Loss: 0.000292\n",
            "Epoch 2086 | Train Loss: 0.000253 | Validation Loss: 0.000295\n",
            "Epoch 2087 | Train Loss: 0.000223 | Validation Loss: 0.000279\n",
            "Epoch 2088 | Train Loss: 0.000231 | Validation Loss: 0.000291\n",
            "Epoch 2089 | Train Loss: 0.000220 | Validation Loss: 0.000294\n",
            "Epoch 2090 | Train Loss: 0.000234 | Validation Loss: 0.000297\n",
            "Epoch 2091 | Train Loss: 0.000224 | Validation Loss: 0.000296\n",
            "Epoch 2092 | Train Loss: 0.000257 | Validation Loss: 0.000336\n",
            "Epoch 2093 | Train Loss: 0.000267 | Validation Loss: 0.000320\n",
            "Epoch 2094 | Train Loss: 0.000250 | Validation Loss: 0.000322\n",
            "Epoch 2095 | Train Loss: 0.000244 | Validation Loss: 0.000304\n",
            "Epoch 2096 | Train Loss: 0.000245 | Validation Loss: 0.000294\n",
            "Epoch 2097 | Train Loss: 0.000228 | Validation Loss: 0.000293\n",
            "Epoch 2098 | Train Loss: 0.000223 | Validation Loss: 0.000280\n",
            "Epoch 2099 | Train Loss: 0.000219 | Validation Loss: 0.000285\n",
            "Epoch 2100 | Train Loss: 0.000225 | Validation Loss: 0.000304\n",
            "Epoch 2101 | Train Loss: 0.000231 | Validation Loss: 0.000291\n",
            "Epoch 2102 | Train Loss: 0.000224 | Validation Loss: 0.000298\n",
            "Epoch 2103 | Train Loss: 0.000252 | Validation Loss: 0.000328\n",
            "Epoch 2104 | Train Loss: 0.000238 | Validation Loss: 0.000307\n",
            "Epoch 2105 | Train Loss: 0.000233 | Validation Loss: 0.000295\n",
            "Epoch 2106 | Train Loss: 0.000234 | Validation Loss: 0.000275\n",
            "Epoch 2107 | Train Loss: 0.000220 | Validation Loss: 0.000299\n",
            "Epoch 2108 | Train Loss: 0.000217 | Validation Loss: 0.000287\n",
            "Epoch 2109 | Train Loss: 0.000230 | Validation Loss: 0.000315\n",
            "Epoch 2110 | Train Loss: 0.000243 | Validation Loss: 0.000296\n",
            "Epoch 2111 | Train Loss: 0.000237 | Validation Loss: 0.000276\n",
            "Epoch 2112 | Train Loss: 0.000233 | Validation Loss: 0.000298\n",
            "Epoch 2113 | Train Loss: 0.000237 | Validation Loss: 0.000286\n",
            "Epoch 2114 | Train Loss: 0.000235 | Validation Loss: 0.000286\n",
            "Epoch 2115 | Train Loss: 0.000223 | Validation Loss: 0.000288\n",
            "Epoch 2116 | Train Loss: 0.000213 | Validation Loss: 0.000300\n",
            "Epoch 2117 | Train Loss: 0.000234 | Validation Loss: 0.000296\n",
            "Epoch 2118 | Train Loss: 0.000238 | Validation Loss: 0.000308\n",
            "Epoch 2119 | Train Loss: 0.000246 | Validation Loss: 0.000333\n",
            "Epoch 2120 | Train Loss: 0.000236 | Validation Loss: 0.000306\n",
            "Epoch 2121 | Train Loss: 0.000227 | Validation Loss: 0.000277\n",
            "Epoch 2122 | Train Loss: 0.000219 | Validation Loss: 0.000296\n",
            "Epoch 2123 | Train Loss: 0.000210 | Validation Loss: 0.000274\n",
            "Epoch 2124 | Train Loss: 0.000216 | Validation Loss: 0.000281\n",
            "Epoch 2125 | Train Loss: 0.000226 | Validation Loss: 0.000298\n",
            "Epoch 2126 | Train Loss: 0.000220 | Validation Loss: 0.000282\n",
            "Epoch 2127 | Train Loss: 0.000247 | Validation Loss: 0.000301\n",
            "Epoch 2128 | Train Loss: 0.000236 | Validation Loss: 0.000287\n",
            "Epoch 2129 | Train Loss: 0.000236 | Validation Loss: 0.000317\n",
            "Epoch 2130 | Train Loss: 0.000243 | Validation Loss: 0.000294\n",
            "Epoch 2131 | Train Loss: 0.000222 | Validation Loss: 0.000278\n",
            "Epoch 2132 | Train Loss: 0.000218 | Validation Loss: 0.000286\n",
            "Epoch 2133 | Train Loss: 0.000223 | Validation Loss: 0.000300\n",
            "Epoch 2134 | Train Loss: 0.000221 | Validation Loss: 0.000283\n",
            "Epoch 2135 | Train Loss: 0.000217 | Validation Loss: 0.000300\n",
            "Epoch 2136 | Train Loss: 0.000217 | Validation Loss: 0.000311\n",
            "Epoch 2137 | Train Loss: 0.000242 | Validation Loss: 0.000299\n",
            "Epoch 2138 | Train Loss: 0.000246 | Validation Loss: 0.000315\n",
            "Epoch 2139 | Train Loss: 0.000222 | Validation Loss: 0.000302\n",
            "Epoch 2140 | Train Loss: 0.000238 | Validation Loss: 0.000287\n",
            "Epoch 2141 | Train Loss: 0.000222 | Validation Loss: 0.000279\n",
            "Epoch 2142 | Train Loss: 0.000226 | Validation Loss: 0.000299\n",
            "Epoch 2143 | Train Loss: 0.000223 | Validation Loss: 0.000298\n",
            "Epoch 2144 | Train Loss: 0.000211 | Validation Loss: 0.000288\n",
            "Epoch 2145 | Train Loss: 0.000215 | Validation Loss: 0.000283\n",
            "Epoch 2146 | Train Loss: 0.000228 | Validation Loss: 0.000295\n",
            "Epoch 2147 | Train Loss: 0.000222 | Validation Loss: 0.000292\n",
            "Epoch 2148 | Train Loss: 0.000223 | Validation Loss: 0.000287\n",
            "saved!\n",
            "Epoch 2149 | Train Loss: 0.000230 | Validation Loss: 0.000260\n",
            "Epoch 2150 | Train Loss: 0.000225 | Validation Loss: 0.000300\n",
            "Epoch 2151 | Train Loss: 0.000230 | Validation Loss: 0.000288\n",
            "Epoch 2152 | Train Loss: 0.000223 | Validation Loss: 0.000284\n",
            "Epoch 2153 | Train Loss: 0.000223 | Validation Loss: 0.000307\n",
            "Epoch 2154 | Train Loss: 0.000216 | Validation Loss: 0.000285\n",
            "Epoch 2155 | Train Loss: 0.000213 | Validation Loss: 0.000293\n",
            "Epoch 2156 | Train Loss: 0.000213 | Validation Loss: 0.000297\n",
            "Epoch 2157 | Train Loss: 0.000223 | Validation Loss: 0.000292\n",
            "Epoch 2158 | Train Loss: 0.000220 | Validation Loss: 0.000314\n",
            "Epoch 2159 | Train Loss: 0.000226 | Validation Loss: 0.000315\n",
            "Epoch 2160 | Train Loss: 0.000213 | Validation Loss: 0.000293\n",
            "Epoch 2161 | Train Loss: 0.000231 | Validation Loss: 0.000280\n",
            "Epoch 2162 | Train Loss: 0.000224 | Validation Loss: 0.000295\n",
            "Epoch 2163 | Train Loss: 0.000217 | Validation Loss: 0.000308\n",
            "Epoch 2164 | Train Loss: 0.000217 | Validation Loss: 0.000299\n",
            "Epoch 2165 | Train Loss: 0.000220 | Validation Loss: 0.000287\n",
            "Epoch 2166 | Train Loss: 0.000216 | Validation Loss: 0.000290\n",
            "Epoch 2167 | Train Loss: 0.000218 | Validation Loss: 0.000289\n",
            "Epoch 2168 | Train Loss: 0.000224 | Validation Loss: 0.000298\n",
            "Epoch 2169 | Train Loss: 0.000242 | Validation Loss: 0.000292\n",
            "Epoch 2170 | Train Loss: 0.000229 | Validation Loss: 0.000307\n",
            "Epoch 2171 | Train Loss: 0.000230 | Validation Loss: 0.000299\n",
            "Epoch 2172 | Train Loss: 0.000246 | Validation Loss: 0.000288\n",
            "Epoch 2173 | Train Loss: 0.000228 | Validation Loss: 0.000278\n",
            "Epoch 2174 | Train Loss: 0.000227 | Validation Loss: 0.000286\n",
            "Epoch 2175 | Train Loss: 0.000224 | Validation Loss: 0.000273\n",
            "Epoch 2176 | Train Loss: 0.000214 | Validation Loss: 0.000270\n",
            "Epoch 2177 | Train Loss: 0.000237 | Validation Loss: 0.000340\n",
            "Epoch 2178 | Train Loss: 0.000230 | Validation Loss: 0.000282\n",
            "Epoch 2179 | Train Loss: 0.000216 | Validation Loss: 0.000280\n",
            "Epoch 2180 | Train Loss: 0.000215 | Validation Loss: 0.000284\n",
            "Epoch 2181 | Train Loss: 0.000215 | Validation Loss: 0.000311\n",
            "Epoch 2182 | Train Loss: 0.000248 | Validation Loss: 0.000311\n",
            "Epoch 2183 | Train Loss: 0.000254 | Validation Loss: 0.000302\n",
            "Epoch 2184 | Train Loss: 0.000224 | Validation Loss: 0.000298\n",
            "Epoch 2185 | Train Loss: 0.000221 | Validation Loss: 0.000308\n",
            "Epoch 2186 | Train Loss: 0.000226 | Validation Loss: 0.000282\n",
            "Epoch 2187 | Train Loss: 0.000213 | Validation Loss: 0.000283\n",
            "Epoch 2188 | Train Loss: 0.000209 | Validation Loss: 0.000295\n",
            "Epoch 2189 | Train Loss: 0.000218 | Validation Loss: 0.000280\n",
            "Epoch 2190 | Train Loss: 0.000205 | Validation Loss: 0.000279\n",
            "Epoch 2191 | Train Loss: 0.000226 | Validation Loss: 0.000288\n",
            "Epoch 2192 | Train Loss: 0.000231 | Validation Loss: 0.000293\n",
            "Epoch 2193 | Train Loss: 0.000225 | Validation Loss: 0.000290\n",
            "Epoch 2194 | Train Loss: 0.000234 | Validation Loss: 0.000283\n",
            "Epoch 2195 | Train Loss: 0.000213 | Validation Loss: 0.000303\n",
            "Epoch 2196 | Train Loss: 0.000217 | Validation Loss: 0.000306\n",
            "Epoch 2197 | Train Loss: 0.000212 | Validation Loss: 0.000263\n",
            "Epoch 2198 | Train Loss: 0.000209 | Validation Loss: 0.000294\n",
            "Epoch 2199 | Train Loss: 0.000215 | Validation Loss: 0.000293\n",
            "Epoch 2200 | Train Loss: 0.000242 | Validation Loss: 0.000294\n",
            "Epoch 2201 | Train Loss: 0.000229 | Validation Loss: 0.000299\n",
            "Epoch 2202 | Train Loss: 0.000225 | Validation Loss: 0.000274\n",
            "Epoch 2203 | Train Loss: 0.000247 | Validation Loss: 0.000303\n",
            "Epoch 2204 | Train Loss: 0.000210 | Validation Loss: 0.000270\n",
            "Epoch 2205 | Train Loss: 0.000218 | Validation Loss: 0.000289\n",
            "Epoch 2206 | Train Loss: 0.000217 | Validation Loss: 0.000307\n",
            "Epoch 2207 | Train Loss: 0.000230 | Validation Loss: 0.000277\n",
            "Epoch 2208 | Train Loss: 0.000214 | Validation Loss: 0.000288\n",
            "Epoch 2209 | Train Loss: 0.000215 | Validation Loss: 0.000299\n",
            "Epoch 2210 | Train Loss: 0.000224 | Validation Loss: 0.000271\n",
            "Epoch 2211 | Train Loss: 0.000218 | Validation Loss: 0.000293\n",
            "Epoch 2212 | Train Loss: 0.000211 | Validation Loss: 0.000287\n",
            "Epoch 2213 | Train Loss: 0.000224 | Validation Loss: 0.000283\n",
            "Epoch 2214 | Train Loss: 0.000211 | Validation Loss: 0.000279\n",
            "Epoch 2215 | Train Loss: 0.000223 | Validation Loss: 0.000289\n",
            "Epoch 2216 | Train Loss: 0.000225 | Validation Loss: 0.000311\n",
            "Epoch 2217 | Train Loss: 0.000218 | Validation Loss: 0.000285\n",
            "Epoch 2218 | Train Loss: 0.000225 | Validation Loss: 0.000274\n",
            "Epoch 2219 | Train Loss: 0.000220 | Validation Loss: 0.000292\n",
            "Epoch 2220 | Train Loss: 0.000217 | Validation Loss: 0.000286\n",
            "Epoch 2221 | Train Loss: 0.000209 | Validation Loss: 0.000274\n",
            "Epoch 2222 | Train Loss: 0.000212 | Validation Loss: 0.000287\n",
            "Epoch 2223 | Train Loss: 0.000219 | Validation Loss: 0.000285\n",
            "Epoch 2224 | Train Loss: 0.000215 | Validation Loss: 0.000289\n",
            "Epoch 2225 | Train Loss: 0.000237 | Validation Loss: 0.000320\n",
            "Epoch 2226 | Train Loss: 0.000235 | Validation Loss: 0.000285\n",
            "Epoch 2227 | Train Loss: 0.000230 | Validation Loss: 0.000296\n",
            "Epoch 2228 | Train Loss: 0.000226 | Validation Loss: 0.000284\n",
            "Epoch 2229 | Train Loss: 0.000222 | Validation Loss: 0.000289\n",
            "Epoch 2230 | Train Loss: 0.000210 | Validation Loss: 0.000282\n",
            "Epoch 2231 | Train Loss: 0.000239 | Validation Loss: 0.000293\n",
            "Epoch 2232 | Train Loss: 0.000228 | Validation Loss: 0.000286\n",
            "Epoch 2233 | Train Loss: 0.000224 | Validation Loss: 0.000292\n",
            "Epoch 2234 | Train Loss: 0.000229 | Validation Loss: 0.000277\n",
            "Epoch 2235 | Train Loss: 0.000220 | Validation Loss: 0.000273\n",
            "Epoch 2236 | Train Loss: 0.000232 | Validation Loss: 0.000291\n",
            "Epoch 2237 | Train Loss: 0.000229 | Validation Loss: 0.000286\n",
            "Epoch 2238 | Train Loss: 0.000224 | Validation Loss: 0.000268\n",
            "Epoch 2239 | Train Loss: 0.000219 | Validation Loss: 0.000284\n",
            "Epoch 2240 | Train Loss: 0.000223 | Validation Loss: 0.000283\n",
            "Epoch 2241 | Train Loss: 0.000230 | Validation Loss: 0.000292\n",
            "Epoch 2242 | Train Loss: 0.000225 | Validation Loss: 0.000291\n",
            "Epoch 2243 | Train Loss: 0.000221 | Validation Loss: 0.000274\n",
            "Epoch 2244 | Train Loss: 0.000211 | Validation Loss: 0.000274\n",
            "Epoch 2245 | Train Loss: 0.000209 | Validation Loss: 0.000321\n",
            "Epoch 2246 | Train Loss: 0.000222 | Validation Loss: 0.000296\n",
            "Epoch 2247 | Train Loss: 0.000238 | Validation Loss: 0.000271\n",
            "Epoch 2248 | Train Loss: 0.000238 | Validation Loss: 0.000312\n",
            "Epoch 2249 | Train Loss: 0.000228 | Validation Loss: 0.000279\n",
            "Epoch 2250 | Train Loss: 0.000220 | Validation Loss: 0.000288\n",
            "Epoch 2251 | Train Loss: 0.000223 | Validation Loss: 0.000300\n",
            "Epoch 2252 | Train Loss: 0.000226 | Validation Loss: 0.000281\n",
            "Epoch 2253 | Train Loss: 0.000208 | Validation Loss: 0.000321\n",
            "Epoch 2254 | Train Loss: 0.000232 | Validation Loss: 0.000305\n",
            "Epoch 2255 | Train Loss: 0.000212 | Validation Loss: 0.000283\n",
            "Epoch 2256 | Train Loss: 0.000213 | Validation Loss: 0.000274\n",
            "Epoch 2257 | Train Loss: 0.000228 | Validation Loss: 0.000267\n",
            "Epoch 2258 | Train Loss: 0.000216 | Validation Loss: 0.000291\n",
            "Epoch 2259 | Train Loss: 0.000219 | Validation Loss: 0.000291\n",
            "Epoch 2260 | Train Loss: 0.000222 | Validation Loss: 0.000294\n",
            "Epoch 2261 | Train Loss: 0.000233 | Validation Loss: 0.000280\n",
            "Epoch 2262 | Train Loss: 0.000214 | Validation Loss: 0.000283\n",
            "Epoch 2263 | Train Loss: 0.000215 | Validation Loss: 0.000283\n",
            "Epoch 2264 | Train Loss: 0.000228 | Validation Loss: 0.000285\n",
            "Epoch 2265 | Train Loss: 0.000222 | Validation Loss: 0.000289\n",
            "Epoch 2266 | Train Loss: 0.000243 | Validation Loss: 0.000314\n",
            "Epoch 2267 | Train Loss: 0.000233 | Validation Loss: 0.000286\n",
            "Epoch 2268 | Train Loss: 0.000227 | Validation Loss: 0.000293\n",
            "Epoch 2269 | Train Loss: 0.000224 | Validation Loss: 0.000293\n",
            "Epoch 2270 | Train Loss: 0.000217 | Validation Loss: 0.000292\n",
            "Epoch 2271 | Train Loss: 0.000218 | Validation Loss: 0.000285\n",
            "Epoch 2272 | Train Loss: 0.000226 | Validation Loss: 0.000299\n",
            "Epoch 2273 | Train Loss: 0.000228 | Validation Loss: 0.000291\n",
            "Epoch 2274 | Train Loss: 0.000214 | Validation Loss: 0.000310\n",
            "Epoch 2275 | Train Loss: 0.000214 | Validation Loss: 0.000287\n",
            "Epoch 2276 | Train Loss: 0.000210 | Validation Loss: 0.000299\n",
            "Epoch 2277 | Train Loss: 0.000213 | Validation Loss: 0.000276\n",
            "Epoch 2278 | Train Loss: 0.000208 | Validation Loss: 0.000278\n",
            "Epoch 2279 | Train Loss: 0.000221 | Validation Loss: 0.000307\n",
            "Epoch 2280 | Train Loss: 0.000246 | Validation Loss: 0.000315\n",
            "Epoch 2281 | Train Loss: 0.000230 | Validation Loss: 0.000284\n",
            "Epoch 2282 | Train Loss: 0.000224 | Validation Loss: 0.000281\n",
            "Epoch 2283 | Train Loss: 0.000221 | Validation Loss: 0.000275\n",
            "Epoch 2284 | Train Loss: 0.000209 | Validation Loss: 0.000289\n",
            "Epoch 2285 | Train Loss: 0.000220 | Validation Loss: 0.000287\n",
            "Epoch 2286 | Train Loss: 0.000214 | Validation Loss: 0.000278\n",
            "Epoch 2287 | Train Loss: 0.000211 | Validation Loss: 0.000284\n",
            "Epoch 2288 | Train Loss: 0.000213 | Validation Loss: 0.000292\n",
            "Epoch 2289 | Train Loss: 0.000240 | Validation Loss: 0.000297\n",
            "Epoch 2290 | Train Loss: 0.000249 | Validation Loss: 0.000291\n",
            "Epoch 2291 | Train Loss: 0.000248 | Validation Loss: 0.000304\n",
            "Epoch 2292 | Train Loss: 0.000234 | Validation Loss: 0.000308\n",
            "Epoch 2293 | Train Loss: 0.000223 | Validation Loss: 0.000283\n",
            "Epoch 2294 | Train Loss: 0.000226 | Validation Loss: 0.000309\n",
            "Epoch 2295 | Train Loss: 0.000223 | Validation Loss: 0.000295\n",
            "Epoch 2296 | Train Loss: 0.000223 | Validation Loss: 0.000278\n",
            "Epoch 2297 | Train Loss: 0.000227 | Validation Loss: 0.000301\n",
            "Epoch 2298 | Train Loss: 0.000226 | Validation Loss: 0.000284\n",
            "Epoch 2299 | Train Loss: 0.000229 | Validation Loss: 0.000279\n",
            "Epoch 2300 | Train Loss: 0.000218 | Validation Loss: 0.000286\n",
            "Epoch 2301 | Train Loss: 0.000220 | Validation Loss: 0.000307\n",
            "Epoch 2302 | Train Loss: 0.000224 | Validation Loss: 0.000281\n",
            "Epoch 2303 | Train Loss: 0.000215 | Validation Loss: 0.000285\n",
            "Epoch 2304 | Train Loss: 0.000229 | Validation Loss: 0.000288\n",
            "Epoch 2305 | Train Loss: 0.000215 | Validation Loss: 0.000292\n",
            "Epoch 2306 | Train Loss: 0.000229 | Validation Loss: 0.000309\n",
            "Epoch 2307 | Train Loss: 0.000246 | Validation Loss: 0.000766\n",
            "Epoch 2308 | Train Loss: 0.000226 | Validation Loss: 0.000753\n",
            "Epoch 2309 | Train Loss: 0.000217 | Validation Loss: 0.000739\n",
            "Epoch 2310 | Train Loss: 0.000220 | Validation Loss: 0.000525\n",
            "Epoch 2311 | Train Loss: 0.000211 | Validation Loss: 0.000283\n",
            "Epoch 2312 | Train Loss: 0.000215 | Validation Loss: 0.000280\n",
            "Epoch 2313 | Train Loss: 0.000220 | Validation Loss: 0.000295\n",
            "Epoch 2314 | Train Loss: 0.000217 | Validation Loss: 0.000293\n",
            "Epoch 2315 | Train Loss: 0.000210 | Validation Loss: 0.000297\n",
            "Epoch 2316 | Train Loss: 0.000220 | Validation Loss: 0.000290\n",
            "Epoch 2317 | Train Loss: 0.000216 | Validation Loss: 0.000294\n",
            "Epoch 2318 | Train Loss: 0.000237 | Validation Loss: 0.000289\n",
            "Epoch 2319 | Train Loss: 0.000238 | Validation Loss: 0.000280\n",
            "Epoch 2320 | Train Loss: 0.000236 | Validation Loss: 0.000766\n",
            "Epoch 2321 | Train Loss: 0.000221 | Validation Loss: 0.000752\n",
            "Epoch 2322 | Train Loss: 0.000220 | Validation Loss: 0.000721\n",
            "Epoch 2323 | Train Loss: 0.000205 | Validation Loss: 0.000285\n",
            "Epoch 2324 | Train Loss: 0.000228 | Validation Loss: 0.000286\n",
            "Epoch 2325 | Train Loss: 0.000219 | Validation Loss: 0.000299\n",
            "Epoch 2326 | Train Loss: 0.000205 | Validation Loss: 0.000290\n",
            "Epoch 2327 | Train Loss: 0.000222 | Validation Loss: 0.000288\n",
            "Epoch 2328 | Train Loss: 0.000216 | Validation Loss: 0.000285\n",
            "Epoch 2329 | Train Loss: 0.000215 | Validation Loss: 0.000763\n",
            "Epoch 2330 | Train Loss: 0.000216 | Validation Loss: 0.000520\n",
            "Epoch 2331 | Train Loss: 0.000212 | Validation Loss: 0.000290\n",
            "Epoch 2332 | Train Loss: 0.000242 | Validation Loss: 0.000520\n",
            "Epoch 2333 | Train Loss: 0.000227 | Validation Loss: 0.000315\n",
            "Epoch 2334 | Train Loss: 0.000215 | Validation Loss: 0.000561\n",
            "Epoch 2335 | Train Loss: 0.000226 | Validation Loss: 0.000289\n",
            "Epoch 2336 | Train Loss: 0.000227 | Validation Loss: 0.000290\n",
            "Epoch 2337 | Train Loss: 0.000211 | Validation Loss: 0.000268\n",
            "Epoch 2338 | Train Loss: 0.000209 | Validation Loss: 0.000744\n",
            "Epoch 2339 | Train Loss: 0.000214 | Validation Loss: 0.000502\n",
            "Epoch 2340 | Train Loss: 0.000232 | Validation Loss: 0.000770\n",
            "Epoch 2341 | Train Loss: 0.000209 | Validation Loss: 0.000272\n",
            "Epoch 2342 | Train Loss: 0.000213 | Validation Loss: 0.000282\n",
            "Epoch 2343 | Train Loss: 0.000216 | Validation Loss: 0.000273\n",
            "Epoch 2344 | Train Loss: 0.000211 | Validation Loss: 0.000278\n",
            "Epoch 2345 | Train Loss: 0.000217 | Validation Loss: 0.000279\n",
            "Epoch 2346 | Train Loss: 0.000217 | Validation Loss: 0.000281\n",
            "Epoch 2347 | Train Loss: 0.000208 | Validation Loss: 0.000769\n",
            "Epoch 2348 | Train Loss: 0.000212 | Validation Loss: 0.000758\n",
            "Epoch 2349 | Train Loss: 0.000217 | Validation Loss: 0.000766\n",
            "Epoch 2350 | Train Loss: 0.000213 | Validation Loss: 0.000390\n",
            "Epoch 2351 | Train Loss: 0.000213 | Validation Loss: 0.000268\n",
            "Epoch 2352 | Train Loss: 0.000211 | Validation Loss: 0.000274\n",
            "Epoch 2353 | Train Loss: 0.000222 | Validation Loss: 0.000287\n",
            "Epoch 2354 | Train Loss: 0.000241 | Validation Loss: 0.000296\n",
            "Epoch 2355 | Train Loss: 0.000230 | Validation Loss: 0.000298\n",
            "Epoch 2356 | Train Loss: 0.000263 | Validation Loss: 0.000319\n",
            "Epoch 2357 | Train Loss: 0.000244 | Validation Loss: 0.000289\n",
            "Epoch 2358 | Train Loss: 0.000233 | Validation Loss: 0.000282\n",
            "Epoch 2359 | Train Loss: 0.000220 | Validation Loss: 0.000476\n",
            "Epoch 2360 | Train Loss: 0.000226 | Validation Loss: 0.000296\n",
            "Epoch 2361 | Train Loss: 0.000212 | Validation Loss: 0.000286\n",
            "Epoch 2362 | Train Loss: 0.000219 | Validation Loss: 0.000289\n",
            "Epoch 2363 | Train Loss: 0.000220 | Validation Loss: 0.000287\n",
            "Epoch 2364 | Train Loss: 0.000213 | Validation Loss: 0.000283\n",
            "Epoch 2365 | Train Loss: 0.000227 | Validation Loss: 0.000295\n",
            "Epoch 2366 | Train Loss: 0.000217 | Validation Loss: 0.000314\n",
            "Epoch 2367 | Train Loss: 0.000219 | Validation Loss: 0.000294\n",
            "Epoch 2368 | Train Loss: 0.000232 | Validation Loss: 0.000330\n",
            "Epoch 2369 | Train Loss: 0.000235 | Validation Loss: 0.000304\n",
            "Epoch 2370 | Train Loss: 0.000229 | Validation Loss: 0.000312\n",
            "Epoch 2371 | Train Loss: 0.000228 | Validation Loss: 0.000285\n",
            "Epoch 2372 | Train Loss: 0.000228 | Validation Loss: 0.000275\n",
            "Epoch 2373 | Train Loss: 0.000223 | Validation Loss: 0.000307\n",
            "Epoch 2374 | Train Loss: 0.000218 | Validation Loss: 0.000286\n",
            "Epoch 2375 | Train Loss: 0.000237 | Validation Loss: 0.000302\n",
            "Epoch 2376 | Train Loss: 0.000210 | Validation Loss: 0.000494\n",
            "Epoch 2377 | Train Loss: 0.000212 | Validation Loss: 0.000290\n",
            "Epoch 2378 | Train Loss: 0.000211 | Validation Loss: 0.000410\n",
            "Epoch 2379 | Train Loss: 0.000209 | Validation Loss: 0.000270\n",
            "Epoch 2380 | Train Loss: 0.000207 | Validation Loss: 0.000280\n",
            "Epoch 2381 | Train Loss: 0.000206 | Validation Loss: 0.000286\n",
            "Epoch 2382 | Train Loss: 0.000223 | Validation Loss: 0.000270\n",
            "Epoch 2383 | Train Loss: 0.000222 | Validation Loss: 0.000282\n",
            "Epoch 2384 | Train Loss: 0.000222 | Validation Loss: 0.000286\n",
            "Epoch 2385 | Train Loss: 0.000220 | Validation Loss: 0.000314\n",
            "Epoch 2386 | Train Loss: 0.000248 | Validation Loss: 0.000290\n",
            "Epoch 2387 | Train Loss: 0.000218 | Validation Loss: 0.000295\n",
            "Epoch 2388 | Train Loss: 0.000216 | Validation Loss: 0.000272\n",
            "Epoch 2389 | Train Loss: 0.000213 | Validation Loss: 0.000349\n",
            "Epoch 2390 | Train Loss: 0.000209 | Validation Loss: 0.000279\n",
            "Epoch 2391 | Train Loss: 0.000216 | Validation Loss: 0.000536\n",
            "Epoch 2392 | Train Loss: 0.000223 | Validation Loss: 0.000291\n",
            "Epoch 2393 | Train Loss: 0.000232 | Validation Loss: 0.000281\n",
            "Epoch 2394 | Train Loss: 0.000211 | Validation Loss: 0.000287\n",
            "Epoch 2395 | Train Loss: 0.000228 | Validation Loss: 0.000290\n",
            "Epoch 2396 | Train Loss: 0.000214 | Validation Loss: 0.000283\n",
            "Epoch 2397 | Train Loss: 0.000217 | Validation Loss: 0.000280\n",
            "Epoch 2398 | Train Loss: 0.000224 | Validation Loss: 0.000287\n",
            "Epoch 2399 | Train Loss: 0.000224 | Validation Loss: 0.000306\n",
            "Epoch 2400 | Train Loss: 0.000234 | Validation Loss: 0.000277\n",
            "Epoch 2401 | Train Loss: 0.000212 | Validation Loss: 0.000281\n",
            "Epoch 2402 | Train Loss: 0.000232 | Validation Loss: 0.000300\n",
            "Epoch 2403 | Train Loss: 0.000227 | Validation Loss: 0.000283\n",
            "Epoch 2404 | Train Loss: 0.000228 | Validation Loss: 0.000293\n",
            "Epoch 2405 | Train Loss: 0.000224 | Validation Loss: 0.000306\n",
            "Epoch 2406 | Train Loss: 0.000220 | Validation Loss: 0.000300\n",
            "Epoch 2407 | Train Loss: 0.000221 | Validation Loss: 0.000291\n",
            "Epoch 2408 | Train Loss: 0.000231 | Validation Loss: 0.000280\n",
            "Epoch 2409 | Train Loss: 0.000216 | Validation Loss: 0.000276\n",
            "Epoch 2410 | Train Loss: 0.000208 | Validation Loss: 0.000289\n",
            "Epoch 2411 | Train Loss: 0.000204 | Validation Loss: 0.000285\n",
            "Epoch 2412 | Train Loss: 0.000216 | Validation Loss: 0.000275\n",
            "Epoch 2413 | Train Loss: 0.000224 | Validation Loss: 0.000537\n",
            "Epoch 2414 | Train Loss: 0.000212 | Validation Loss: 0.000284\n",
            "Epoch 2415 | Train Loss: 0.000219 | Validation Loss: 0.000302\n",
            "Epoch 2416 | Train Loss: 0.000208 | Validation Loss: 0.000513\n",
            "Epoch 2417 | Train Loss: 0.000214 | Validation Loss: 0.000500\n",
            "Epoch 2418 | Train Loss: 0.000217 | Validation Loss: 0.000737\n",
            "Epoch 2419 | Train Loss: 0.000217 | Validation Loss: 0.000516\n",
            "Epoch 2420 | Train Loss: 0.000216 | Validation Loss: 0.000501\n",
            "Epoch 2421 | Train Loss: 0.000211 | Validation Loss: 0.000744\n",
            "Epoch 2422 | Train Loss: 0.000235 | Validation Loss: 0.000726\n",
            "Epoch 2423 | Train Loss: 0.000213 | Validation Loss: 0.000512\n",
            "Epoch 2424 | Train Loss: 0.000216 | Validation Loss: 0.000727\n",
            "Epoch 2425 | Train Loss: 0.000212 | Validation Loss: 0.000724\n",
            "Epoch 2426 | Train Loss: 0.000218 | Validation Loss: 0.000520\n",
            "Epoch 2427 | Train Loss: 0.000225 | Validation Loss: 0.000284\n",
            "Epoch 2428 | Train Loss: 0.000221 | Validation Loss: 0.000271\n",
            "Epoch 2429 | Train Loss: 0.000226 | Validation Loss: 0.000721\n",
            "Epoch 2430 | Train Loss: 0.000218 | Validation Loss: 0.000543\n",
            "Epoch 2431 | Train Loss: 0.000220 | Validation Loss: 0.000504\n",
            "Epoch 2432 | Train Loss: 0.000206 | Validation Loss: 0.000501\n",
            "Epoch 2433 | Train Loss: 0.000202 | Validation Loss: 0.000728\n",
            "Epoch 2434 | Train Loss: 0.000204 | Validation Loss: 0.000774\n",
            "Epoch 2435 | Train Loss: 0.000214 | Validation Loss: 0.000728\n",
            "Epoch 2436 | Train Loss: 0.000213 | Validation Loss: 0.000726\n",
            "Epoch 2437 | Train Loss: 0.000209 | Validation Loss: 0.000730\n",
            "Epoch 2438 | Train Loss: 0.000215 | Validation Loss: 0.000499\n",
            "Epoch 2439 | Train Loss: 0.000207 | Validation Loss: 0.000748\n",
            "Epoch 2440 | Train Loss: 0.000219 | Validation Loss: 0.000711\n",
            "Epoch 2441 | Train Loss: 0.000205 | Validation Loss: 0.000733\n",
            "Epoch 2442 | Train Loss: 0.000214 | Validation Loss: 0.000733\n",
            "Epoch 2443 | Train Loss: 0.000217 | Validation Loss: 0.000735\n",
            "Epoch 2444 | Train Loss: 0.000235 | Validation Loss: 0.000292\n",
            "Epoch 2445 | Train Loss: 0.000218 | Validation Loss: 0.000289\n",
            "Epoch 2446 | Train Loss: 0.000211 | Validation Loss: 0.000288\n",
            "Epoch 2447 | Train Loss: 0.000212 | Validation Loss: 0.000476\n",
            "Epoch 2448 | Train Loss: 0.000223 | Validation Loss: 0.000723\n",
            "Epoch 2449 | Train Loss: 0.000217 | Validation Loss: 0.000516\n",
            "Epoch 2450 | Train Loss: 0.000218 | Validation Loss: 0.000274\n",
            "Epoch 2451 | Train Loss: 0.000227 | Validation Loss: 0.000283\n",
            "Epoch 2452 | Train Loss: 0.000223 | Validation Loss: 0.000287\n",
            "Epoch 2453 | Train Loss: 0.000222 | Validation Loss: 0.000294\n",
            "Epoch 2454 | Train Loss: 0.000223 | Validation Loss: 0.000290\n",
            "Epoch 2455 | Train Loss: 0.000217 | Validation Loss: 0.000280\n",
            "Epoch 2456 | Train Loss: 0.000219 | Validation Loss: 0.000268\n",
            "Epoch 2457 | Train Loss: 0.000219 | Validation Loss: 0.000303\n",
            "Epoch 2458 | Train Loss: 0.000222 | Validation Loss: 0.000287\n",
            "Epoch 2459 | Train Loss: 0.000223 | Validation Loss: 0.000278\n",
            "Epoch 2460 | Train Loss: 0.000211 | Validation Loss: 0.000281\n",
            "Epoch 2461 | Train Loss: 0.000223 | Validation Loss: 0.000297\n",
            "Epoch 2462 | Train Loss: 0.000217 | Validation Loss: 0.000284\n",
            "Epoch 2463 | Train Loss: 0.000213 | Validation Loss: 0.000285\n",
            "Epoch 2464 | Train Loss: 0.000221 | Validation Loss: 0.000283\n",
            "Epoch 2465 | Train Loss: 0.000225 | Validation Loss: 0.000287\n",
            "Epoch 2466 | Train Loss: 0.000222 | Validation Loss: 0.000291\n",
            "Epoch 2467 | Train Loss: 0.000219 | Validation Loss: 0.000275\n",
            "Epoch 2468 | Train Loss: 0.000214 | Validation Loss: 0.000295\n",
            "Epoch 2469 | Train Loss: 0.000207 | Validation Loss: 0.000281\n",
            "Epoch 2470 | Train Loss: 0.000207 | Validation Loss: 0.000278\n",
            "Epoch 2471 | Train Loss: 0.000220 | Validation Loss: 0.000290\n",
            "Epoch 2472 | Train Loss: 0.000216 | Validation Loss: 0.000302\n",
            "Epoch 2473 | Train Loss: 0.000225 | Validation Loss: 0.000310\n",
            "Epoch 2474 | Train Loss: 0.000213 | Validation Loss: 0.000282\n",
            "Epoch 2475 | Train Loss: 0.000221 | Validation Loss: 0.000300\n",
            "Epoch 2476 | Train Loss: 0.000215 | Validation Loss: 0.000292\n",
            "Epoch 2477 | Train Loss: 0.000235 | Validation Loss: 0.000302\n",
            "Epoch 2478 | Train Loss: 0.000246 | Validation Loss: 0.000284\n",
            "Epoch 2479 | Train Loss: 0.000236 | Validation Loss: 0.000262\n",
            "Epoch 2480 | Train Loss: 0.000217 | Validation Loss: 0.000294\n",
            "Epoch 2481 | Train Loss: 0.000218 | Validation Loss: 0.000270\n",
            "Epoch 2482 | Train Loss: 0.000214 | Validation Loss: 0.000270\n",
            "Epoch 2483 | Train Loss: 0.000224 | Validation Loss: 0.000310\n",
            "Epoch 2484 | Train Loss: 0.000234 | Validation Loss: 0.000304\n",
            "Epoch 2485 | Train Loss: 0.000218 | Validation Loss: 0.000292\n",
            "Epoch 2486 | Train Loss: 0.000236 | Validation Loss: 0.000289\n",
            "Epoch 2487 | Train Loss: 0.000237 | Validation Loss: 0.000278\n",
            "Epoch 2488 | Train Loss: 0.000216 | Validation Loss: 0.000287\n",
            "Epoch 2489 | Train Loss: 0.000222 | Validation Loss: 0.000264\n",
            "Epoch 2490 | Train Loss: 0.000230 | Validation Loss: 0.000281\n",
            "Epoch 2491 | Train Loss: 0.000238 | Validation Loss: 0.000283\n",
            "Epoch 2492 | Train Loss: 0.000217 | Validation Loss: 0.000284\n",
            "Epoch 2493 | Train Loss: 0.000221 | Validation Loss: 0.000293\n",
            "Epoch 2494 | Train Loss: 0.000211 | Validation Loss: 0.000321\n",
            "Epoch 2495 | Train Loss: 0.000230 | Validation Loss: 0.000291\n",
            "Epoch 2496 | Train Loss: 0.000229 | Validation Loss: 0.000290\n",
            "Epoch 2497 | Train Loss: 0.000219 | Validation Loss: 0.000295\n",
            "Epoch 2498 | Train Loss: 0.000220 | Validation Loss: 0.000286\n",
            "Epoch 2499 | Train Loss: 0.000242 | Validation Loss: 0.000281\n",
            "Epoch 2500 | Train Loss: 0.000213 | Validation Loss: 0.000301\n",
            "Epoch 2501 | Train Loss: 0.000217 | Validation Loss: 0.000279\n",
            "Epoch 2502 | Train Loss: 0.000224 | Validation Loss: 0.000273\n",
            "Epoch 2503 | Train Loss: 0.000216 | Validation Loss: 0.000293\n",
            "Epoch 2504 | Train Loss: 0.000219 | Validation Loss: 0.000287\n",
            "Epoch 2505 | Train Loss: 0.000212 | Validation Loss: 0.000285\n",
            "Epoch 2506 | Train Loss: 0.000220 | Validation Loss: 0.000289\n",
            "Epoch 2507 | Train Loss: 0.000219 | Validation Loss: 0.000283\n",
            "Epoch 2508 | Train Loss: 0.000212 | Validation Loss: 0.000289\n",
            "Epoch 2509 | Train Loss: 0.000221 | Validation Loss: 0.000282\n",
            "Epoch 2510 | Train Loss: 0.000212 | Validation Loss: 0.000299\n",
            "Epoch 2511 | Train Loss: 0.000226 | Validation Loss: 0.000294\n",
            "Epoch 2512 | Train Loss: 0.000207 | Validation Loss: 0.000290\n",
            "Epoch 2513 | Train Loss: 0.000202 | Validation Loss: 0.000274\n",
            "Epoch 2514 | Train Loss: 0.000210 | Validation Loss: 0.000292\n",
            "Epoch 2515 | Train Loss: 0.000227 | Validation Loss: 0.000287\n",
            "Epoch 2516 | Train Loss: 0.000241 | Validation Loss: 0.000304\n",
            "Epoch 2517 | Train Loss: 0.000225 | Validation Loss: 0.000282\n",
            "Epoch 2518 | Train Loss: 0.000217 | Validation Loss: 0.000280\n",
            "Epoch 2519 | Train Loss: 0.000214 | Validation Loss: 0.000287\n",
            "Epoch 2520 | Train Loss: 0.000224 | Validation Loss: 0.000548\n",
            "Epoch 2521 | Train Loss: 0.000218 | Validation Loss: 0.000286\n",
            "Epoch 2522 | Train Loss: 0.000217 | Validation Loss: 0.000723\n",
            "Epoch 2523 | Train Loss: 0.000216 | Validation Loss: 0.000741\n",
            "Epoch 2524 | Train Loss: 0.000224 | Validation Loss: 0.000732\n",
            "Epoch 2525 | Train Loss: 0.000244 | Validation Loss: 0.000728\n",
            "Epoch 2526 | Train Loss: 0.000213 | Validation Loss: 0.000515\n",
            "Epoch 2527 | Train Loss: 0.000210 | Validation Loss: 0.000522\n",
            "Epoch 2528 | Train Loss: 0.000240 | Validation Loss: 0.000298\n",
            "Epoch 2529 | Train Loss: 0.000216 | Validation Loss: 0.000293\n",
            "Epoch 2530 | Train Loss: 0.000208 | Validation Loss: 0.000508\n",
            "Epoch 2531 | Train Loss: 0.000217 | Validation Loss: 0.000287\n",
            "Epoch 2532 | Train Loss: 0.000243 | Validation Loss: 0.000331\n",
            "Epoch 2533 | Train Loss: 0.000252 | Validation Loss: 0.000317\n",
            "Epoch 2534 | Train Loss: 0.000238 | Validation Loss: 0.000300\n",
            "Epoch 2535 | Train Loss: 0.000223 | Validation Loss: 0.000315\n",
            "Epoch 2536 | Train Loss: 0.000217 | Validation Loss: 0.000291\n",
            "Epoch 2537 | Train Loss: 0.000222 | Validation Loss: 0.000277\n",
            "Epoch 2538 | Train Loss: 0.000212 | Validation Loss: 0.000288\n",
            "Epoch 2539 | Train Loss: 0.000231 | Validation Loss: 0.000299\n",
            "Epoch 2540 | Train Loss: 0.000212 | Validation Loss: 0.000283\n",
            "Epoch 2541 | Train Loss: 0.000226 | Validation Loss: 0.000280\n",
            "Epoch 2542 | Train Loss: 0.000230 | Validation Loss: 0.000303\n",
            "Epoch 2543 | Train Loss: 0.000224 | Validation Loss: 0.000282\n",
            "Epoch 2544 | Train Loss: 0.000216 | Validation Loss: 0.000285\n",
            "Epoch 2545 | Train Loss: 0.000216 | Validation Loss: 0.000289\n",
            "Epoch 2546 | Train Loss: 0.000208 | Validation Loss: 0.000266\n",
            "Epoch 2547 | Train Loss: 0.000204 | Validation Loss: 0.000275\n",
            "Epoch 2548 | Train Loss: 0.000222 | Validation Loss: 0.000277\n",
            "Epoch 2549 | Train Loss: 0.000238 | Validation Loss: 0.000301\n",
            "Epoch 2550 | Train Loss: 0.000255 | Validation Loss: 0.000303\n",
            "Epoch 2551 | Train Loss: 0.000240 | Validation Loss: 0.000305\n",
            "Epoch 2552 | Train Loss: 0.000238 | Validation Loss: 0.000537\n",
            "Epoch 2553 | Train Loss: 0.000234 | Validation Loss: 0.000726\n",
            "Epoch 2554 | Train Loss: 0.000213 | Validation Loss: 0.000720\n",
            "Epoch 2555 | Train Loss: 0.000212 | Validation Loss: 0.000508\n",
            "Epoch 2556 | Train Loss: 0.000211 | Validation Loss: 0.000743\n",
            "Epoch 2557 | Train Loss: 0.000208 | Validation Loss: 0.000771\n",
            "Epoch 2558 | Train Loss: 0.000222 | Validation Loss: 0.000452\n",
            "Epoch 2559 | Train Loss: 0.000220 | Validation Loss: 0.000795\n",
            "Epoch 2560 | Train Loss: 0.000232 | Validation Loss: 0.000291\n",
            "Epoch 2561 | Train Loss: 0.000207 | Validation Loss: 0.000307\n",
            "Epoch 2562 | Train Loss: 0.000212 | Validation Loss: 0.000717\n",
            "Epoch 2563 | Train Loss: 0.000210 | Validation Loss: 0.000264\n",
            "Epoch 2564 | Train Loss: 0.000212 | Validation Loss: 0.000741\n",
            "Epoch 2565 | Train Loss: 0.000216 | Validation Loss: 0.000552\n",
            "Epoch 2566 | Train Loss: 0.000226 | Validation Loss: 0.000268\n",
            "Epoch 2567 | Train Loss: 0.000211 | Validation Loss: 0.000279\n",
            "Epoch 2568 | Train Loss: 0.000216 | Validation Loss: 0.000671\n",
            "Epoch 2569 | Train Loss: 0.000206 | Validation Loss: 0.000283\n",
            "Epoch 2570 | Train Loss: 0.000222 | Validation Loss: 0.000733\n",
            "Epoch 2571 | Train Loss: 0.000230 | Validation Loss: 0.000279\n",
            "Epoch 2572 | Train Loss: 0.000226 | Validation Loss: 0.000291\n",
            "Epoch 2573 | Train Loss: 0.000223 | Validation Loss: 0.000278\n",
            "Epoch 2574 | Train Loss: 0.000206 | Validation Loss: 0.000377\n",
            "Epoch 2575 | Train Loss: 0.000205 | Validation Loss: 0.000283\n",
            "Epoch 2576 | Train Loss: 0.000208 | Validation Loss: 0.000290\n",
            "Epoch 2577 | Train Loss: 0.000205 | Validation Loss: 0.000283\n",
            "Epoch 2578 | Train Loss: 0.000210 | Validation Loss: 0.000279\n",
            "Epoch 2579 | Train Loss: 0.000213 | Validation Loss: 0.000298\n",
            "Epoch 2580 | Train Loss: 0.000222 | Validation Loss: 0.000298\n",
            "Epoch 2581 | Train Loss: 0.000220 | Validation Loss: 0.000295\n",
            "Epoch 2582 | Train Loss: 0.000220 | Validation Loss: 0.000287\n",
            "Epoch 2583 | Train Loss: 0.000212 | Validation Loss: 0.000282\n",
            "Epoch 2584 | Train Loss: 0.000215 | Validation Loss: 0.000303\n",
            "Epoch 2585 | Train Loss: 0.000226 | Validation Loss: 0.000285\n",
            "Epoch 2586 | Train Loss: 0.000215 | Validation Loss: 0.000495\n",
            "Epoch 2587 | Train Loss: 0.000218 | Validation Loss: 0.000512\n",
            "Epoch 2588 | Train Loss: 0.000230 | Validation Loss: 0.000515\n",
            "Epoch 2589 | Train Loss: 0.000216 | Validation Loss: 0.000780\n",
            "Epoch 2590 | Train Loss: 0.000208 | Validation Loss: 0.000300\n",
            "Epoch 2591 | Train Loss: 0.000219 | Validation Loss: 0.000292\n",
            "Epoch 2592 | Train Loss: 0.000207 | Validation Loss: 0.000285\n",
            "Epoch 2593 | Train Loss: 0.000212 | Validation Loss: 0.000286\n",
            "Epoch 2594 | Train Loss: 0.000205 | Validation Loss: 0.000271\n",
            "Epoch 2595 | Train Loss: 0.000221 | Validation Loss: 0.000293\n",
            "Epoch 2596 | Train Loss: 0.000224 | Validation Loss: 0.000289\n",
            "Epoch 2597 | Train Loss: 0.000210 | Validation Loss: 0.000290\n",
            "Epoch 2598 | Train Loss: 0.000214 | Validation Loss: 0.000282\n",
            "Epoch 2599 | Train Loss: 0.000211 | Validation Loss: 0.000277\n",
            "Epoch 2600 | Train Loss: 0.000207 | Validation Loss: 0.000280\n",
            "Epoch 2601 | Train Loss: 0.000215 | Validation Loss: 0.000287\n",
            "Epoch 2602 | Train Loss: 0.000227 | Validation Loss: 0.000291\n",
            "Epoch 2603 | Train Loss: 0.000220 | Validation Loss: 0.000289\n",
            "Epoch 2604 | Train Loss: 0.000206 | Validation Loss: 0.000275\n",
            "Epoch 2605 | Train Loss: 0.000210 | Validation Loss: 0.000271\n",
            "Epoch 2606 | Train Loss: 0.000236 | Validation Loss: 0.000282\n",
            "Epoch 2607 | Train Loss: 0.000237 | Validation Loss: 0.000293\n",
            "Epoch 2608 | Train Loss: 0.000245 | Validation Loss: 0.000290\n",
            "Epoch 2609 | Train Loss: 0.000230 | Validation Loss: 0.000291\n",
            "Epoch 2610 | Train Loss: 0.000219 | Validation Loss: 0.000283\n",
            "Epoch 2611 | Train Loss: 0.000206 | Validation Loss: 0.000278\n",
            "Epoch 2612 | Train Loss: 0.000209 | Validation Loss: 0.000298\n",
            "Epoch 2613 | Train Loss: 0.000225 | Validation Loss: 0.000290\n",
            "Epoch 2614 | Train Loss: 0.000235 | Validation Loss: 0.000289\n",
            "Epoch 2615 | Train Loss: 0.000231 | Validation Loss: 0.000303\n",
            "Epoch 2616 | Train Loss: 0.000232 | Validation Loss: 0.000279\n",
            "Epoch 2617 | Train Loss: 0.000209 | Validation Loss: 0.000289\n",
            "Epoch 2618 | Train Loss: 0.000213 | Validation Loss: 0.000278\n",
            "Epoch 2619 | Train Loss: 0.000223 | Validation Loss: 0.000293\n",
            "Epoch 2620 | Train Loss: 0.000215 | Validation Loss: 0.000299\n",
            "Epoch 2621 | Train Loss: 0.000212 | Validation Loss: 0.000296\n",
            "Epoch 2622 | Train Loss: 0.000232 | Validation Loss: 0.000300\n",
            "Epoch 2623 | Train Loss: 0.000216 | Validation Loss: 0.000293\n",
            "Epoch 2624 | Train Loss: 0.000219 | Validation Loss: 0.000301\n",
            "Epoch 2625 | Train Loss: 0.000214 | Validation Loss: 0.000282\n",
            "Epoch 2626 | Train Loss: 0.000206 | Validation Loss: 0.000306\n",
            "Epoch 2627 | Train Loss: 0.000226 | Validation Loss: 0.000279\n",
            "Epoch 2628 | Train Loss: 0.000210 | Validation Loss: 0.000288\n",
            "Epoch 2629 | Train Loss: 0.000207 | Validation Loss: 0.000292\n",
            "Epoch 2630 | Train Loss: 0.000211 | Validation Loss: 0.000283\n",
            "Epoch 2631 | Train Loss: 0.000203 | Validation Loss: 0.000284\n",
            "Epoch 2632 | Train Loss: 0.000207 | Validation Loss: 0.000277\n",
            "Epoch 2633 | Train Loss: 0.000218 | Validation Loss: 0.000300\n",
            "Epoch 2634 | Train Loss: 0.000204 | Validation Loss: 0.000300\n",
            "Epoch 2635 | Train Loss: 0.000216 | Validation Loss: 0.000279\n",
            "Epoch 2636 | Train Loss: 0.000220 | Validation Loss: 0.000273\n",
            "Epoch 2637 | Train Loss: 0.000198 | Validation Loss: 0.000267\n",
            "Epoch 2638 | Train Loss: 0.000213 | Validation Loss: 0.000284\n",
            "Epoch 2639 | Train Loss: 0.000217 | Validation Loss: 0.000273\n",
            "Epoch 2640 | Train Loss: 0.000208 | Validation Loss: 0.000279\n",
            "Epoch 2641 | Train Loss: 0.000204 | Validation Loss: 0.000280\n",
            "Epoch 2642 | Train Loss: 0.000208 | Validation Loss: 0.000294\n",
            "Epoch 2643 | Train Loss: 0.000213 | Validation Loss: 0.000305\n",
            "Epoch 2644 | Train Loss: 0.000219 | Validation Loss: 0.000284\n",
            "Epoch 2645 | Train Loss: 0.000235 | Validation Loss: 0.000308\n",
            "Epoch 2646 | Train Loss: 0.000211 | Validation Loss: 0.000291\n",
            "Epoch 2647 | Train Loss: 0.000225 | Validation Loss: 0.000278\n",
            "Epoch 2648 | Train Loss: 0.000217 | Validation Loss: 0.000276\n",
            "Epoch 2649 | Train Loss: 0.000204 | Validation Loss: 0.000270\n",
            "Epoch 2650 | Train Loss: 0.000208 | Validation Loss: 0.000278\n",
            "Epoch 2651 | Train Loss: 0.000203 | Validation Loss: 0.000290\n",
            "Epoch 2652 | Train Loss: 0.000210 | Validation Loss: 0.000292\n",
            "Epoch 2653 | Train Loss: 0.000221 | Validation Loss: 0.000274\n",
            "Epoch 2654 | Train Loss: 0.000221 | Validation Loss: 0.000291\n",
            "Epoch 2655 | Train Loss: 0.000217 | Validation Loss: 0.000291\n",
            "Epoch 2656 | Train Loss: 0.000217 | Validation Loss: 0.000297\n",
            "Epoch 2657 | Train Loss: 0.000210 | Validation Loss: 0.000277\n",
            "Epoch 2658 | Train Loss: 0.000217 | Validation Loss: 0.000277\n",
            "Epoch 2659 | Train Loss: 0.000205 | Validation Loss: 0.000285\n",
            "Epoch 2660 | Train Loss: 0.000224 | Validation Loss: 0.000286\n",
            "Epoch 2661 | Train Loss: 0.000221 | Validation Loss: 0.000288\n",
            "Epoch 2662 | Train Loss: 0.000208 | Validation Loss: 0.000266\n",
            "Epoch 2663 | Train Loss: 0.000207 | Validation Loss: 0.000294\n",
            "Epoch 2664 | Train Loss: 0.000229 | Validation Loss: 0.000283\n",
            "Epoch 2665 | Train Loss: 0.000219 | Validation Loss: 0.000286\n",
            "Epoch 2666 | Train Loss: 0.000225 | Validation Loss: 0.000275\n",
            "Epoch 2667 | Train Loss: 0.000225 | Validation Loss: 0.000274\n",
            "Epoch 2668 | Train Loss: 0.000212 | Validation Loss: 0.000302\n",
            "Epoch 2669 | Train Loss: 0.000219 | Validation Loss: 0.000279\n",
            "Epoch 2670 | Train Loss: 0.000214 | Validation Loss: 0.000285\n",
            "Epoch 2671 | Train Loss: 0.000215 | Validation Loss: 0.000284\n",
            "Epoch 2672 | Train Loss: 0.000209 | Validation Loss: 0.000270\n",
            "Epoch 2673 | Train Loss: 0.000206 | Validation Loss: 0.000268\n",
            "Epoch 2674 | Train Loss: 0.000205 | Validation Loss: 0.000266\n",
            "Epoch 2675 | Train Loss: 0.000211 | Validation Loss: 0.000269\n",
            "Epoch 2676 | Train Loss: 0.000202 | Validation Loss: 0.000278\n",
            "Epoch 2677 | Train Loss: 0.000208 | Validation Loss: 0.000287\n",
            "Epoch 2678 | Train Loss: 0.000226 | Validation Loss: 0.000479\n",
            "Epoch 2679 | Train Loss: 0.000240 | Validation Loss: 0.000762\n",
            "Epoch 2680 | Train Loss: 0.000212 | Validation Loss: 0.000765\n",
            "Epoch 2681 | Train Loss: 0.000212 | Validation Loss: 0.000741\n",
            "Epoch 2682 | Train Loss: 0.000216 | Validation Loss: 0.000758\n",
            "Epoch 2683 | Train Loss: 0.000216 | Validation Loss: 0.000754\n",
            "Epoch 2684 | Train Loss: 0.000206 | Validation Loss: 0.000740\n",
            "Epoch 2685 | Train Loss: 0.000205 | Validation Loss: 0.000525\n",
            "Epoch 2686 | Train Loss: 0.000218 | Validation Loss: 0.000505\n",
            "Epoch 2687 | Train Loss: 0.000214 | Validation Loss: 0.000764\n",
            "Epoch 2688 | Train Loss: 0.000212 | Validation Loss: 0.000771\n",
            "Epoch 2689 | Train Loss: 0.000208 | Validation Loss: 0.000760\n",
            "Epoch 2690 | Train Loss: 0.000204 | Validation Loss: 0.000510\n",
            "Epoch 2691 | Train Loss: 0.000203 | Validation Loss: 0.000505\n",
            "Epoch 2692 | Train Loss: 0.000216 | Validation Loss: 0.000498\n",
            "Epoch 2693 | Train Loss: 0.000209 | Validation Loss: 0.000523\n",
            "Epoch 2694 | Train Loss: 0.000225 | Validation Loss: 0.000530\n",
            "Epoch 2695 | Train Loss: 0.000223 | Validation Loss: 0.000759\n",
            "Epoch 2696 | Train Loss: 0.000219 | Validation Loss: 0.000762\n",
            "Epoch 2697 | Train Loss: 0.000219 | Validation Loss: 0.000281\n",
            "Epoch 2698 | Train Loss: 0.000221 | Validation Loss: 0.000503\n",
            "Epoch 2699 | Train Loss: 0.000221 | Validation Loss: 0.000770\n",
            "Epoch 2700 | Train Loss: 0.000213 | Validation Loss: 0.000536\n",
            "Epoch 2701 | Train Loss: 0.000202 | Validation Loss: 0.000518\n",
            "Epoch 2702 | Train Loss: 0.000207 | Validation Loss: 0.000275\n",
            "Epoch 2703 | Train Loss: 0.000230 | Validation Loss: 0.000278\n",
            "Epoch 2704 | Train Loss: 0.000222 | Validation Loss: 0.000283\n",
            "Epoch 2705 | Train Loss: 0.000227 | Validation Loss: 0.000305\n",
            "Epoch 2706 | Train Loss: 0.000213 | Validation Loss: 0.000750\n",
            "Epoch 2707 | Train Loss: 0.000217 | Validation Loss: 0.000755\n",
            "Epoch 2708 | Train Loss: 0.000217 | Validation Loss: 0.000726\n",
            "Epoch 2709 | Train Loss: 0.000210 | Validation Loss: 0.000549\n",
            "Epoch 2710 | Train Loss: 0.000204 | Validation Loss: 0.000278\n",
            "Epoch 2711 | Train Loss: 0.000216 | Validation Loss: 0.000747\n",
            "Epoch 2712 | Train Loss: 0.000214 | Validation Loss: 0.000762\n",
            "Epoch 2713 | Train Loss: 0.000225 | Validation Loss: 0.000761\n",
            "Epoch 2714 | Train Loss: 0.000215 | Validation Loss: 0.000513\n",
            "Epoch 2715 | Train Loss: 0.000213 | Validation Loss: 0.000509\n",
            "Epoch 2716 | Train Loss: 0.000209 | Validation Loss: 0.000273\n",
            "Epoch 2717 | Train Loss: 0.000209 | Validation Loss: 0.000277\n",
            "Epoch 2718 | Train Loss: 0.000206 | Validation Loss: 0.000505\n",
            "Epoch 2719 | Train Loss: 0.000214 | Validation Loss: 0.000349\n",
            "Epoch 2720 | Train Loss: 0.000229 | Validation Loss: 0.000547\n",
            "Epoch 2721 | Train Loss: 0.000225 | Validation Loss: 0.000639\n",
            "Epoch 2722 | Train Loss: 0.000215 | Validation Loss: 0.000292\n",
            "Epoch 2723 | Train Loss: 0.000216 | Validation Loss: 0.000755\n",
            "Epoch 2724 | Train Loss: 0.000209 | Validation Loss: 0.000516\n",
            "Epoch 2725 | Train Loss: 0.000208 | Validation Loss: 0.000285\n",
            "Epoch 2726 | Train Loss: 0.000220 | Validation Loss: 0.000277\n",
            "Epoch 2727 | Train Loss: 0.000224 | Validation Loss: 0.000274\n",
            "Epoch 2728 | Train Loss: 0.000215 | Validation Loss: 0.000297\n",
            "Epoch 2729 | Train Loss: 0.000214 | Validation Loss: 0.000288\n",
            "Epoch 2730 | Train Loss: 0.000211 | Validation Loss: 0.000281\n",
            "Epoch 2731 | Train Loss: 0.000209 | Validation Loss: 0.000282\n",
            "Epoch 2732 | Train Loss: 0.000204 | Validation Loss: 0.000274\n",
            "Epoch 2733 | Train Loss: 0.000204 | Validation Loss: 0.000299\n",
            "Epoch 2734 | Train Loss: 0.000204 | Validation Loss: 0.000277\n",
            "Epoch 2735 | Train Loss: 0.000211 | Validation Loss: 0.000285\n",
            "Epoch 2736 | Train Loss: 0.000239 | Validation Loss: 0.000276\n",
            "Epoch 2737 | Train Loss: 0.000221 | Validation Loss: 0.000290\n",
            "Epoch 2738 | Train Loss: 0.000213 | Validation Loss: 0.000268\n",
            "Epoch 2739 | Train Loss: 0.000208 | Validation Loss: 0.000271\n",
            "Epoch 2740 | Train Loss: 0.000205 | Validation Loss: 0.000285\n",
            "Epoch 2741 | Train Loss: 0.000220 | Validation Loss: 0.000288\n",
            "Epoch 2742 | Train Loss: 0.000224 | Validation Loss: 0.000291\n",
            "Epoch 2743 | Train Loss: 0.000219 | Validation Loss: 0.000302\n",
            "Epoch 2744 | Train Loss: 0.000210 | Validation Loss: 0.000287\n",
            "Epoch 2745 | Train Loss: 0.000208 | Validation Loss: 0.000293\n",
            "Epoch 2746 | Train Loss: 0.000214 | Validation Loss: 0.000296\n",
            "Epoch 2747 | Train Loss: 0.000237 | Validation Loss: 0.000284\n",
            "Epoch 2748 | Train Loss: 0.000213 | Validation Loss: 0.000288\n",
            "Epoch 2749 | Train Loss: 0.000216 | Validation Loss: 0.000290\n",
            "Epoch 2750 | Train Loss: 0.000209 | Validation Loss: 0.000291\n",
            "Epoch 2751 | Train Loss: 0.000214 | Validation Loss: 0.000281\n",
            "Epoch 2752 | Train Loss: 0.000225 | Validation Loss: 0.000280\n",
            "Epoch 2753 | Train Loss: 0.000216 | Validation Loss: 0.000277\n",
            "Epoch 2754 | Train Loss: 0.000205 | Validation Loss: 0.000284\n",
            "Epoch 2755 | Train Loss: 0.000216 | Validation Loss: 0.000275\n",
            "Epoch 2756 | Train Loss: 0.000200 | Validation Loss: 0.000276\n",
            "Epoch 2757 | Train Loss: 0.000219 | Validation Loss: 0.000286\n",
            "Epoch 2758 | Train Loss: 0.000220 | Validation Loss: 0.000302\n",
            "Epoch 2759 | Train Loss: 0.000207 | Validation Loss: 0.000285\n",
            "Epoch 2760 | Train Loss: 0.000212 | Validation Loss: 0.000285\n",
            "Epoch 2761 | Train Loss: 0.000213 | Validation Loss: 0.000273\n",
            "Epoch 2762 | Train Loss: 0.000214 | Validation Loss: 0.000510\n",
            "Epoch 2763 | Train Loss: 0.000214 | Validation Loss: 0.000749\n",
            "Epoch 2764 | Train Loss: 0.000227 | Validation Loss: 0.000559\n",
            "Epoch 2765 | Train Loss: 0.000218 | Validation Loss: 0.000277\n",
            "Epoch 2766 | Train Loss: 0.000211 | Validation Loss: 0.000281\n",
            "Epoch 2767 | Train Loss: 0.000210 | Validation Loss: 0.000288\n",
            "Epoch 2768 | Train Loss: 0.000208 | Validation Loss: 0.000270\n",
            "Epoch 2769 | Train Loss: 0.000206 | Validation Loss: 0.000525\n",
            "Epoch 2770 | Train Loss: 0.000213 | Validation Loss: 0.000320\n",
            "Epoch 2771 | Train Loss: 0.000220 | Validation Loss: 0.000309\n",
            "Epoch 2772 | Train Loss: 0.000224 | Validation Loss: 0.000288\n",
            "Epoch 2773 | Train Loss: 0.000213 | Validation Loss: 0.000278\n",
            "Epoch 2774 | Train Loss: 0.000208 | Validation Loss: 0.000293\n",
            "Epoch 2775 | Train Loss: 0.000211 | Validation Loss: 0.000286\n",
            "Epoch 2776 | Train Loss: 0.000215 | Validation Loss: 0.000282\n",
            "Epoch 2777 | Train Loss: 0.000202 | Validation Loss: 0.000287\n",
            "Epoch 2778 | Train Loss: 0.000208 | Validation Loss: 0.000277\n",
            "Epoch 2779 | Train Loss: 0.000200 | Validation Loss: 0.000272\n",
            "Epoch 2780 | Train Loss: 0.000196 | Validation Loss: 0.000280\n",
            "Epoch 2781 | Train Loss: 0.000209 | Validation Loss: 0.000283\n",
            "Epoch 2782 | Train Loss: 0.000205 | Validation Loss: 0.000301\n",
            "Epoch 2783 | Train Loss: 0.000212 | Validation Loss: 0.000292\n",
            "Epoch 2784 | Train Loss: 0.000202 | Validation Loss: 0.000290\n",
            "Epoch 2785 | Train Loss: 0.000230 | Validation Loss: 0.000283\n",
            "Epoch 2786 | Train Loss: 0.000223 | Validation Loss: 0.000276\n",
            "Epoch 2787 | Train Loss: 0.000218 | Validation Loss: 0.000278\n",
            "Epoch 2788 | Train Loss: 0.000212 | Validation Loss: 0.000290\n",
            "Epoch 2789 | Train Loss: 0.000199 | Validation Loss: 0.000302\n",
            "Epoch 2790 | Train Loss: 0.000205 | Validation Loss: 0.000292\n",
            "Epoch 2791 | Train Loss: 0.000215 | Validation Loss: 0.000310\n",
            "Epoch 2792 | Train Loss: 0.000229 | Validation Loss: 0.000286\n",
            "Epoch 2793 | Train Loss: 0.000221 | Validation Loss: 0.000282\n",
            "Epoch 2794 | Train Loss: 0.000209 | Validation Loss: 0.000270\n",
            "Epoch 2795 | Train Loss: 0.000208 | Validation Loss: 0.000277\n",
            "Epoch 2796 | Train Loss: 0.000219 | Validation Loss: 0.000278\n",
            "Epoch 2797 | Train Loss: 0.000217 | Validation Loss: 0.000291\n",
            "Epoch 2798 | Train Loss: 0.000212 | Validation Loss: 0.000282\n",
            "Epoch 2799 | Train Loss: 0.000203 | Validation Loss: 0.000265\n",
            "Epoch 2800 | Train Loss: 0.000217 | Validation Loss: 0.000274\n",
            "Epoch 2801 | Train Loss: 0.000218 | Validation Loss: 0.000299\n",
            "Epoch 2802 | Train Loss: 0.000213 | Validation Loss: 0.000278\n",
            "Epoch 2803 | Train Loss: 0.000218 | Validation Loss: 0.000295\n",
            "Epoch 2804 | Train Loss: 0.000212 | Validation Loss: 0.000278\n",
            "Epoch 2805 | Train Loss: 0.000230 | Validation Loss: 0.000299\n",
            "Epoch 2806 | Train Loss: 0.000231 | Validation Loss: 0.000290\n",
            "Epoch 2807 | Train Loss: 0.000212 | Validation Loss: 0.000289\n",
            "Epoch 2808 | Train Loss: 0.000216 | Validation Loss: 0.000277\n",
            "Epoch 2809 | Train Loss: 0.000217 | Validation Loss: 0.000290\n",
            "Epoch 2810 | Train Loss: 0.000200 | Validation Loss: 0.000278\n",
            "Epoch 2811 | Train Loss: 0.000208 | Validation Loss: 0.000282\n",
            "Epoch 2812 | Train Loss: 0.000205 | Validation Loss: 0.000276\n",
            "Epoch 2813 | Train Loss: 0.000196 | Validation Loss: 0.000275\n",
            "Epoch 2814 | Train Loss: 0.000205 | Validation Loss: 0.000292\n",
            "Epoch 2815 | Train Loss: 0.000200 | Validation Loss: 0.000279\n",
            "Epoch 2816 | Train Loss: 0.000212 | Validation Loss: 0.000260\n",
            "Epoch 2817 | Train Loss: 0.000214 | Validation Loss: 0.000287\n",
            "Epoch 2818 | Train Loss: 0.000208 | Validation Loss: 0.000293\n",
            "Epoch 2819 | Train Loss: 0.000204 | Validation Loss: 0.000275\n",
            "Epoch 2820 | Train Loss: 0.000209 | Validation Loss: 0.000291\n",
            "Epoch 2821 | Train Loss: 0.000219 | Validation Loss: 0.000282\n",
            "Epoch 2822 | Train Loss: 0.000208 | Validation Loss: 0.000276\n",
            "Epoch 2823 | Train Loss: 0.000219 | Validation Loss: 0.000280\n",
            "Epoch 2824 | Train Loss: 0.000217 | Validation Loss: 0.000279\n",
            "Epoch 2825 | Train Loss: 0.000208 | Validation Loss: 0.000295\n",
            "Epoch 2826 | Train Loss: 0.000212 | Validation Loss: 0.000293\n",
            "Epoch 2827 | Train Loss: 0.000211 | Validation Loss: 0.000270\n",
            "Epoch 2828 | Train Loss: 0.000216 | Validation Loss: 0.000298\n",
            "Epoch 2829 | Train Loss: 0.000233 | Validation Loss: 0.000322\n",
            "Epoch 2830 | Train Loss: 0.000231 | Validation Loss: 0.000290\n",
            "Epoch 2831 | Train Loss: 0.000217 | Validation Loss: 0.000294\n",
            "Epoch 2832 | Train Loss: 0.000210 | Validation Loss: 0.000294\n",
            "Epoch 2833 | Train Loss: 0.000213 | Validation Loss: 0.000284\n",
            "Epoch 2834 | Train Loss: 0.000216 | Validation Loss: 0.000306\n",
            "Epoch 2835 | Train Loss: 0.000234 | Validation Loss: 0.000291\n",
            "Epoch 2836 | Train Loss: 0.000216 | Validation Loss: 0.000289\n",
            "Epoch 2837 | Train Loss: 0.000212 | Validation Loss: 0.000278\n",
            "Epoch 2838 | Train Loss: 0.000214 | Validation Loss: 0.000278\n",
            "Epoch 2839 | Train Loss: 0.000209 | Validation Loss: 0.000289\n",
            "Epoch 2840 | Train Loss: 0.000216 | Validation Loss: 0.000555\n",
            "Epoch 2841 | Train Loss: 0.000215 | Validation Loss: 0.000304\n",
            "Epoch 2842 | Train Loss: 0.000208 | Validation Loss: 0.000283\n",
            "Epoch 2843 | Train Loss: 0.000208 | Validation Loss: 0.000275\n",
            "Epoch 2844 | Train Loss: 0.000207 | Validation Loss: 0.000282\n",
            "Epoch 2845 | Train Loss: 0.000209 | Validation Loss: 0.000304\n",
            "Epoch 2846 | Train Loss: 0.000206 | Validation Loss: 0.000277\n",
            "Epoch 2847 | Train Loss: 0.000208 | Validation Loss: 0.000276\n",
            "Epoch 2848 | Train Loss: 0.000200 | Validation Loss: 0.000290\n",
            "Epoch 2849 | Train Loss: 0.000216 | Validation Loss: 0.000284\n",
            "Epoch 2850 | Train Loss: 0.000208 | Validation Loss: 0.000280\n",
            "Epoch 2851 | Train Loss: 0.000205 | Validation Loss: 0.000274\n",
            "Epoch 2852 | Train Loss: 0.000220 | Validation Loss: 0.000277\n",
            "Epoch 2853 | Train Loss: 0.000211 | Validation Loss: 0.000286\n",
            "Epoch 2854 | Train Loss: 0.000207 | Validation Loss: 0.000275\n",
            "Epoch 2855 | Train Loss: 0.000203 | Validation Loss: 0.000271\n",
            "Epoch 2856 | Train Loss: 0.000210 | Validation Loss: 0.000283\n",
            "Epoch 2857 | Train Loss: 0.000213 | Validation Loss: 0.000271\n",
            "Epoch 2858 | Train Loss: 0.000219 | Validation Loss: 0.000535\n",
            "Epoch 2859 | Train Loss: 0.000215 | Validation Loss: 0.000279\n",
            "Epoch 2860 | Train Loss: 0.000205 | Validation Loss: 0.000278\n",
            "Epoch 2861 | Train Loss: 0.000211 | Validation Loss: 0.000270\n",
            "Epoch 2862 | Train Loss: 0.000218 | Validation Loss: 0.000510\n",
            "Epoch 2863 | Train Loss: 0.000213 | Validation Loss: 0.000524\n",
            "Epoch 2864 | Train Loss: 0.000223 | Validation Loss: 0.000685\n",
            "Epoch 2865 | Train Loss: 0.000214 | Validation Loss: 0.000534\n",
            "Epoch 2866 | Train Loss: 0.000217 | Validation Loss: 0.000534\n",
            "Epoch 2867 | Train Loss: 0.000214 | Validation Loss: 0.000296\n",
            "Epoch 2868 | Train Loss: 0.000215 | Validation Loss: 0.000293\n",
            "Epoch 2869 | Train Loss: 0.000201 | Validation Loss: 0.000274\n",
            "Epoch 2870 | Train Loss: 0.000209 | Validation Loss: 0.000541\n",
            "Epoch 2871 | Train Loss: 0.000204 | Validation Loss: 0.000274\n",
            "Epoch 2872 | Train Loss: 0.000207 | Validation Loss: 0.000502\n",
            "Epoch 2873 | Train Loss: 0.000210 | Validation Loss: 0.000538\n",
            "Epoch 2874 | Train Loss: 0.000219 | Validation Loss: 0.000302\n",
            "Epoch 2875 | Train Loss: 0.000218 | Validation Loss: 0.000292\n",
            "Epoch 2876 | Train Loss: 0.000224 | Validation Loss: 0.000285\n",
            "Epoch 2877 | Train Loss: 0.000209 | Validation Loss: 0.000535\n",
            "Epoch 2878 | Train Loss: 0.000208 | Validation Loss: 0.000513\n",
            "Epoch 2879 | Train Loss: 0.000209 | Validation Loss: 0.000762\n",
            "Epoch 2880 | Train Loss: 0.000202 | Validation Loss: 0.000541\n",
            "Epoch 2881 | Train Loss: 0.000202 | Validation Loss: 0.000530\n",
            "Epoch 2882 | Train Loss: 0.000213 | Validation Loss: 0.000277\n",
            "Epoch 2883 | Train Loss: 0.000215 | Validation Loss: 0.000504\n",
            "Epoch 2884 | Train Loss: 0.000208 | Validation Loss: 0.000547\n",
            "Epoch 2885 | Train Loss: 0.000221 | Validation Loss: 0.000297\n",
            "Epoch 2886 | Train Loss: 0.000217 | Validation Loss: 0.000293\n",
            "Epoch 2887 | Train Loss: 0.000214 | Validation Loss: 0.000274\n",
            "Epoch 2888 | Train Loss: 0.000204 | Validation Loss: 0.000261\n",
            "Epoch 2889 | Train Loss: 0.000208 | Validation Loss: 0.000291\n",
            "Epoch 2890 | Train Loss: 0.000213 | Validation Loss: 0.000283\n",
            "Epoch 2891 | Train Loss: 0.000209 | Validation Loss: 0.000274\n",
            "Epoch 2892 | Train Loss: 0.000203 | Validation Loss: 0.000268\n",
            "Epoch 2893 | Train Loss: 0.000209 | Validation Loss: 0.000286\n",
            "Epoch 2894 | Train Loss: 0.000205 | Validation Loss: 0.000323\n",
            "Epoch 2895 | Train Loss: 0.000214 | Validation Loss: 0.000284\n",
            "Epoch 2896 | Train Loss: 0.000203 | Validation Loss: 0.000269\n",
            "Epoch 2897 | Train Loss: 0.000206 | Validation Loss: 0.000520\n",
            "Epoch 2898 | Train Loss: 0.000218 | Validation Loss: 0.000504\n",
            "Epoch 2899 | Train Loss: 0.000197 | Validation Loss: 0.000268\n",
            "Epoch 2900 | Train Loss: 0.000210 | Validation Loss: 0.000520\n",
            "Epoch 2901 | Train Loss: 0.000213 | Validation Loss: 0.000529\n",
            "Epoch 2902 | Train Loss: 0.000216 | Validation Loss: 0.000288\n",
            "Epoch 2903 | Train Loss: 0.000210 | Validation Loss: 0.000286\n",
            "Epoch 2904 | Train Loss: 0.000210 | Validation Loss: 0.000286\n",
            "Epoch 2905 | Train Loss: 0.000210 | Validation Loss: 0.000282\n",
            "Epoch 2906 | Train Loss: 0.000201 | Validation Loss: 0.000268\n",
            "Epoch 2907 | Train Loss: 0.000217 | Validation Loss: 0.000287\n",
            "Epoch 2908 | Train Loss: 0.000233 | Validation Loss: 0.000283\n",
            "Epoch 2909 | Train Loss: 0.000221 | Validation Loss: 0.000280\n",
            "Epoch 2910 | Train Loss: 0.000207 | Validation Loss: 0.000271\n",
            "Epoch 2911 | Train Loss: 0.000224 | Validation Loss: 0.000281\n",
            "Epoch 2912 | Train Loss: 0.000209 | Validation Loss: 0.000528\n",
            "Epoch 2913 | Train Loss: 0.000209 | Validation Loss: 0.000516\n",
            "Epoch 2914 | Train Loss: 0.000202 | Validation Loss: 0.000500\n",
            "Epoch 2915 | Train Loss: 0.000203 | Validation Loss: 0.000532\n",
            "Epoch 2916 | Train Loss: 0.000211 | Validation Loss: 0.000282\n",
            "Epoch 2917 | Train Loss: 0.000214 | Validation Loss: 0.000272\n",
            "Epoch 2918 | Train Loss: 0.000211 | Validation Loss: 0.000295\n",
            "Epoch 2919 | Train Loss: 0.000206 | Validation Loss: 0.000293\n",
            "Epoch 2920 | Train Loss: 0.000212 | Validation Loss: 0.000277\n",
            "Epoch 2921 | Train Loss: 0.000203 | Validation Loss: 0.000273\n",
            "Epoch 2922 | Train Loss: 0.000210 | Validation Loss: 0.000298\n",
            "Epoch 2923 | Train Loss: 0.000214 | Validation Loss: 0.000278\n",
            "Epoch 2924 | Train Loss: 0.000214 | Validation Loss: 0.000486\n",
            "Epoch 2925 | Train Loss: 0.000204 | Validation Loss: 0.000285\n",
            "Epoch 2926 | Train Loss: 0.000202 | Validation Loss: 0.000276\n",
            "Epoch 2927 | Train Loss: 0.000206 | Validation Loss: 0.000285\n",
            "Epoch 2928 | Train Loss: 0.000204 | Validation Loss: 0.000266\n",
            "saved!\n",
            "Epoch 2929 | Train Loss: 0.000189 | Validation Loss: 0.000258\n",
            "Epoch 2930 | Train Loss: 0.000220 | Validation Loss: 0.000298\n",
            "Epoch 2931 | Train Loss: 0.000203 | Validation Loss: 0.000290\n",
            "Epoch 2932 | Train Loss: 0.000213 | Validation Loss: 0.000278\n",
            "Epoch 2933 | Train Loss: 0.000211 | Validation Loss: 0.000268\n",
            "Epoch 2934 | Train Loss: 0.000204 | Validation Loss: 0.000286\n",
            "Epoch 2935 | Train Loss: 0.000215 | Validation Loss: 0.000284\n",
            "Epoch 2936 | Train Loss: 0.000200 | Validation Loss: 0.000277\n",
            "Epoch 2937 | Train Loss: 0.000204 | Validation Loss: 0.000279\n",
            "Epoch 2938 | Train Loss: 0.000218 | Validation Loss: 0.000267\n",
            "Epoch 2939 | Train Loss: 0.000208 | Validation Loss: 0.000282\n",
            "Epoch 2940 | Train Loss: 0.000217 | Validation Loss: 0.000285\n",
            "Epoch 2941 | Train Loss: 0.000201 | Validation Loss: 0.000270\n",
            "Epoch 2942 | Train Loss: 0.000206 | Validation Loss: 0.000266\n",
            "Epoch 2943 | Train Loss: 0.000206 | Validation Loss: 0.000283\n",
            "Epoch 2944 | Train Loss: 0.000224 | Validation Loss: 0.000289\n",
            "Epoch 2945 | Train Loss: 0.000210 | Validation Loss: 0.000284\n",
            "Epoch 2946 | Train Loss: 0.000203 | Validation Loss: 0.000282\n",
            "Epoch 2947 | Train Loss: 0.000211 | Validation Loss: 0.000278\n",
            "Epoch 2948 | Train Loss: 0.000206 | Validation Loss: 0.000275\n",
            "Epoch 2949 | Train Loss: 0.000215 | Validation Loss: 0.000273\n",
            "Epoch 2950 | Train Loss: 0.000211 | Validation Loss: 0.000294\n",
            "Epoch 2951 | Train Loss: 0.000214 | Validation Loss: 0.000322\n",
            "Epoch 2952 | Train Loss: 0.000238 | Validation Loss: 0.000300\n",
            "Epoch 2953 | Train Loss: 0.000246 | Validation Loss: 0.000538\n",
            "Epoch 2954 | Train Loss: 0.000228 | Validation Loss: 0.000303\n",
            "Epoch 2955 | Train Loss: 0.000224 | Validation Loss: 0.000364\n",
            "Epoch 2956 | Train Loss: 0.000220 | Validation Loss: 0.000283\n",
            "Epoch 2957 | Train Loss: 0.000207 | Validation Loss: 0.000521\n",
            "Epoch 2958 | Train Loss: 0.000206 | Validation Loss: 0.000264\n",
            "Epoch 2959 | Train Loss: 0.000209 | Validation Loss: 0.000274\n",
            "Epoch 2960 | Train Loss: 0.000212 | Validation Loss: 0.000266\n",
            "Epoch 2961 | Train Loss: 0.000218 | Validation Loss: 0.000276\n",
            "Epoch 2962 | Train Loss: 0.000219 | Validation Loss: 0.000289\n",
            "Epoch 2963 | Train Loss: 0.000216 | Validation Loss: 0.000289\n",
            "Epoch 2964 | Train Loss: 0.000216 | Validation Loss: 0.000278\n",
            "Epoch 2965 | Train Loss: 0.000216 | Validation Loss: 0.000268\n",
            "Epoch 2966 | Train Loss: 0.000212 | Validation Loss: 0.000287\n",
            "Epoch 2967 | Train Loss: 0.000210 | Validation Loss: 0.000298\n",
            "Epoch 2968 | Train Loss: 0.000218 | Validation Loss: 0.000530\n",
            "Epoch 2969 | Train Loss: 0.000210 | Validation Loss: 0.000516\n",
            "Epoch 2970 | Train Loss: 0.000216 | Validation Loss: 0.000294\n",
            "Epoch 2971 | Train Loss: 0.000225 | Validation Loss: 0.000296\n",
            "Epoch 2972 | Train Loss: 0.000243 | Validation Loss: 0.000294\n",
            "Epoch 2973 | Train Loss: 0.000238 | Validation Loss: 0.000275\n",
            "Epoch 2974 | Train Loss: 0.000229 | Validation Loss: 0.000284\n",
            "Epoch 2975 | Train Loss: 0.000219 | Validation Loss: 0.000530\n",
            "Epoch 2976 | Train Loss: 0.000212 | Validation Loss: 0.000268\n",
            "Epoch 2977 | Train Loss: 0.000214 | Validation Loss: 0.000284\n",
            "Epoch 2978 | Train Loss: 0.000226 | Validation Loss: 0.000329\n",
            "Epoch 2979 | Train Loss: 0.000230 | Validation Loss: 0.000282\n",
            "Epoch 2980 | Train Loss: 0.000219 | Validation Loss: 0.000292\n",
            "Epoch 2981 | Train Loss: 0.000213 | Validation Loss: 0.000291\n",
            "Epoch 2982 | Train Loss: 0.000218 | Validation Loss: 0.000286\n",
            "Epoch 2983 | Train Loss: 0.000202 | Validation Loss: 0.000273\n",
            "Epoch 2984 | Train Loss: 0.000208 | Validation Loss: 0.000282\n",
            "Epoch 2985 | Train Loss: 0.000193 | Validation Loss: 0.000271\n",
            "Epoch 2986 | Train Loss: 0.000210 | Validation Loss: 0.000289\n",
            "Epoch 2987 | Train Loss: 0.000210 | Validation Loss: 0.000267\n",
            "Epoch 2988 | Train Loss: 0.000214 | Validation Loss: 0.000288\n",
            "Epoch 2989 | Train Loss: 0.000207 | Validation Loss: 0.000283\n",
            "Epoch 2990 | Train Loss: 0.000222 | Validation Loss: 0.000289\n",
            "Epoch 2991 | Train Loss: 0.000215 | Validation Loss: 0.000536\n",
            "Epoch 2992 | Train Loss: 0.000213 | Validation Loss: 0.000283\n",
            "Epoch 2993 | Train Loss: 0.000209 | Validation Loss: 0.000278\n",
            "Epoch 2994 | Train Loss: 0.000203 | Validation Loss: 0.000292\n",
            "Epoch 2995 | Train Loss: 0.000208 | Validation Loss: 0.000493\n",
            "Epoch 2996 | Train Loss: 0.000203 | Validation Loss: 0.000272\n",
            "Epoch 2997 | Train Loss: 0.000205 | Validation Loss: 0.000545\n",
            "Epoch 2998 | Train Loss: 0.000214 | Validation Loss: 0.000579\n",
            "Epoch 2999 | Train Loss: 0.000203 | Validation Loss: 0.000286\n",
            "Epoch 3000 | Train Loss: 0.000197 | Validation Loss: 0.000291\n",
            "Epoch 3001 | Train Loss: 0.000206 | Validation Loss: 0.000280\n",
            "Epoch 3002 | Train Loss: 0.000202 | Validation Loss: 0.000286\n",
            "Epoch 3003 | Train Loss: 0.000211 | Validation Loss: 0.000285\n",
            "Epoch 3004 | Train Loss: 0.000207 | Validation Loss: 0.000285\n",
            "Epoch 3005 | Train Loss: 0.000218 | Validation Loss: 0.000281\n",
            "Epoch 3006 | Train Loss: 0.000214 | Validation Loss: 0.000313\n",
            "Epoch 3007 | Train Loss: 0.000214 | Validation Loss: 0.000381\n",
            "Epoch 3008 | Train Loss: 0.000210 | Validation Loss: 0.000277\n",
            "Epoch 3009 | Train Loss: 0.000207 | Validation Loss: 0.000742\n",
            "Epoch 3010 | Train Loss: 0.000205 | Validation Loss: 0.000515\n",
            "Epoch 3011 | Train Loss: 0.000202 | Validation Loss: 0.000512\n",
            "Epoch 3012 | Train Loss: 0.000193 | Validation Loss: 0.000508\n",
            "Epoch 3013 | Train Loss: 0.000203 | Validation Loss: 0.000734\n",
            "Epoch 3014 | Train Loss: 0.000215 | Validation Loss: 0.000287\n",
            "Epoch 3015 | Train Loss: 0.000210 | Validation Loss: 0.000743\n",
            "Epoch 3016 | Train Loss: 0.000202 | Validation Loss: 0.000504\n",
            "Epoch 3017 | Train Loss: 0.000215 | Validation Loss: 0.000277\n",
            "Epoch 3018 | Train Loss: 0.000211 | Validation Loss: 0.000280\n",
            "Epoch 3019 | Train Loss: 0.000214 | Validation Loss: 0.000267\n",
            "Epoch 3020 | Train Loss: 0.000217 | Validation Loss: 0.000299\n",
            "Epoch 3021 | Train Loss: 0.000202 | Validation Loss: 0.000272\n",
            "Epoch 3022 | Train Loss: 0.000200 | Validation Loss: 0.000292\n",
            "Epoch 3023 | Train Loss: 0.000213 | Validation Loss: 0.000275\n",
            "Epoch 3024 | Train Loss: 0.000219 | Validation Loss: 0.000269\n",
            "Epoch 3025 | Train Loss: 0.000210 | Validation Loss: 0.000270\n",
            "Epoch 3026 | Train Loss: 0.000225 | Validation Loss: 0.000294\n",
            "Epoch 3027 | Train Loss: 0.000214 | Validation Loss: 0.000287\n",
            "Epoch 3028 | Train Loss: 0.000211 | Validation Loss: 0.000310\n",
            "Epoch 3029 | Train Loss: 0.000232 | Validation Loss: 0.000326\n",
            "Epoch 3030 | Train Loss: 0.000217 | Validation Loss: 0.000307\n",
            "Epoch 3031 | Train Loss: 0.000201 | Validation Loss: 0.000275\n",
            "Epoch 3032 | Train Loss: 0.000214 | Validation Loss: 0.000278\n",
            "Epoch 3033 | Train Loss: 0.000211 | Validation Loss: 0.000292\n",
            "Epoch 3034 | Train Loss: 0.000208 | Validation Loss: 0.000288\n",
            "Epoch 3035 | Train Loss: 0.000204 | Validation Loss: 0.000289\n",
            "Epoch 3036 | Train Loss: 0.000219 | Validation Loss: 0.000308\n",
            "Epoch 3037 | Train Loss: 0.000210 | Validation Loss: 0.000277\n",
            "Epoch 3038 | Train Loss: 0.000218 | Validation Loss: 0.000524\n",
            "Epoch 3039 | Train Loss: 0.000206 | Validation Loss: 0.000273\n",
            "Epoch 3040 | Train Loss: 0.000205 | Validation Loss: 0.000305\n",
            "Epoch 3041 | Train Loss: 0.000208 | Validation Loss: 0.000276\n",
            "Epoch 3042 | Train Loss: 0.000203 | Validation Loss: 0.000526\n",
            "Epoch 3043 | Train Loss: 0.000209 | Validation Loss: 0.000504\n",
            "Epoch 3044 | Train Loss: 0.000205 | Validation Loss: 0.000275\n",
            "Epoch 3045 | Train Loss: 0.000218 | Validation Loss: 0.000511\n",
            "Epoch 3046 | Train Loss: 0.000216 | Validation Loss: 0.000754\n",
            "Epoch 3047 | Train Loss: 0.000214 | Validation Loss: 0.000621\n",
            "Epoch 3048 | Train Loss: 0.000200 | Validation Loss: 0.000518\n",
            "Epoch 3049 | Train Loss: 0.000211 | Validation Loss: 0.000282\n",
            "Epoch 3050 | Train Loss: 0.000220 | Validation Loss: 0.000301\n",
            "Epoch 3051 | Train Loss: 0.000212 | Validation Loss: 0.000764\n",
            "Epoch 3052 | Train Loss: 0.000213 | Validation Loss: 0.000747\n",
            "Epoch 3053 | Train Loss: 0.000209 | Validation Loss: 0.000750\n",
            "Epoch 3054 | Train Loss: 0.000194 | Validation Loss: 0.000329\n",
            "Epoch 3055 | Train Loss: 0.000192 | Validation Loss: 0.000521\n",
            "Epoch 3056 | Train Loss: 0.000195 | Validation Loss: 0.000519\n",
            "Epoch 3057 | Train Loss: 0.000206 | Validation Loss: 0.000503\n",
            "Epoch 3058 | Train Loss: 0.000209 | Validation Loss: 0.000288\n",
            "Epoch 3059 | Train Loss: 0.000209 | Validation Loss: 0.000291\n",
            "Epoch 3060 | Train Loss: 0.000201 | Validation Loss: 0.000276\n",
            "Epoch 3061 | Train Loss: 0.000206 | Validation Loss: 0.000288\n",
            "Epoch 3062 | Train Loss: 0.000214 | Validation Loss: 0.000271\n",
            "Epoch 3063 | Train Loss: 0.000208 | Validation Loss: 0.000292\n",
            "Epoch 3064 | Train Loss: 0.000210 | Validation Loss: 0.000306\n",
            "Epoch 3065 | Train Loss: 0.000213 | Validation Loss: 0.000298\n",
            "Epoch 3066 | Train Loss: 0.000210 | Validation Loss: 0.000281\n",
            "Epoch 3067 | Train Loss: 0.000220 | Validation Loss: 0.000297\n",
            "Epoch 3068 | Train Loss: 0.000208 | Validation Loss: 0.000267\n",
            "Epoch 3069 | Train Loss: 0.000201 | Validation Loss: 0.000269\n",
            "Epoch 3070 | Train Loss: 0.000206 | Validation Loss: 0.000302\n",
            "Epoch 3071 | Train Loss: 0.000209 | Validation Loss: 0.000285\n",
            "Epoch 3072 | Train Loss: 0.000205 | Validation Loss: 0.000295\n",
            "Epoch 3073 | Train Loss: 0.000211 | Validation Loss: 0.000279\n",
            "Epoch 3074 | Train Loss: 0.000213 | Validation Loss: 0.000288\n",
            "Epoch 3075 | Train Loss: 0.000207 | Validation Loss: 0.000272\n",
            "Epoch 3076 | Train Loss: 0.000208 | Validation Loss: 0.000277\n",
            "Epoch 3077 | Train Loss: 0.000206 | Validation Loss: 0.000278\n",
            "Epoch 3078 | Train Loss: 0.000210 | Validation Loss: 0.000283\n",
            "Epoch 3079 | Train Loss: 0.000231 | Validation Loss: 0.000285\n",
            "Epoch 3080 | Train Loss: 0.000222 | Validation Loss: 0.000268\n",
            "Epoch 3081 | Train Loss: 0.000200 | Validation Loss: 0.000284\n",
            "Epoch 3082 | Train Loss: 0.000207 | Validation Loss: 0.000288\n",
            "Epoch 3083 | Train Loss: 0.000202 | Validation Loss: 0.000282\n",
            "Epoch 3084 | Train Loss: 0.000208 | Validation Loss: 0.000283\n",
            "Epoch 3085 | Train Loss: 0.000206 | Validation Loss: 0.000264\n",
            "Epoch 3086 | Train Loss: 0.000208 | Validation Loss: 0.000270\n",
            "Epoch 3087 | Train Loss: 0.000219 | Validation Loss: 0.000280\n",
            "Epoch 3088 | Train Loss: 0.000228 | Validation Loss: 0.000318\n",
            "Epoch 3089 | Train Loss: 0.000234 | Validation Loss: 0.000265\n",
            "Epoch 3090 | Train Loss: 0.000213 | Validation Loss: 0.000271\n",
            "Epoch 3091 | Train Loss: 0.000211 | Validation Loss: 0.000275\n",
            "Epoch 3092 | Train Loss: 0.000209 | Validation Loss: 0.000285\n",
            "Epoch 3093 | Train Loss: 0.000219 | Validation Loss: 0.000282\n",
            "Epoch 3094 | Train Loss: 0.000223 | Validation Loss: 0.000285\n",
            "Epoch 3095 | Train Loss: 0.000218 | Validation Loss: 0.000292\n",
            "Epoch 3096 | Train Loss: 0.000228 | Validation Loss: 0.000277\n",
            "Epoch 3097 | Train Loss: 0.000203 | Validation Loss: 0.000291\n",
            "Epoch 3098 | Train Loss: 0.000215 | Validation Loss: 0.000289\n",
            "Epoch 3099 | Train Loss: 0.000229 | Validation Loss: 0.000301\n",
            "Epoch 3100 | Train Loss: 0.000217 | Validation Loss: 0.000292\n",
            "Epoch 3101 | Train Loss: 0.000214 | Validation Loss: 0.000278\n",
            "Epoch 3102 | Train Loss: 0.000201 | Validation Loss: 0.000282\n",
            "Epoch 3103 | Train Loss: 0.000213 | Validation Loss: 0.000298\n",
            "Epoch 3104 | Train Loss: 0.000226 | Validation Loss: 0.000272\n",
            "Epoch 3105 | Train Loss: 0.000227 | Validation Loss: 0.000277\n",
            "Epoch 3106 | Train Loss: 0.000237 | Validation Loss: 0.000289\n",
            "Epoch 3107 | Train Loss: 0.000214 | Validation Loss: 0.000285\n",
            "Epoch 3108 | Train Loss: 0.000202 | Validation Loss: 0.000306\n",
            "Epoch 3109 | Train Loss: 0.000214 | Validation Loss: 0.000274\n",
            "Epoch 3110 | Train Loss: 0.000204 | Validation Loss: 0.000289\n",
            "saved!\n",
            "Epoch 3111 | Train Loss: 0.000210 | Validation Loss: 0.000251\n",
            "Epoch 3112 | Train Loss: 0.000200 | Validation Loss: 0.000277\n",
            "Epoch 3113 | Train Loss: 0.000207 | Validation Loss: 0.000270\n",
            "Epoch 3114 | Train Loss: 0.000226 | Validation Loss: 0.000279\n",
            "Epoch 3115 | Train Loss: 0.000218 | Validation Loss: 0.000283\n",
            "Epoch 3116 | Train Loss: 0.000210 | Validation Loss: 0.000290\n",
            "Epoch 3117 | Train Loss: 0.000206 | Validation Loss: 0.000280\n",
            "Epoch 3118 | Train Loss: 0.000206 | Validation Loss: 0.000265\n",
            "Epoch 3119 | Train Loss: 0.000228 | Validation Loss: 0.000292\n",
            "Epoch 3120 | Train Loss: 0.000212 | Validation Loss: 0.000262\n",
            "Epoch 3121 | Train Loss: 0.000200 | Validation Loss: 0.000275\n",
            "Epoch 3122 | Train Loss: 0.000200 | Validation Loss: 0.000292\n",
            "Epoch 3123 | Train Loss: 0.000200 | Validation Loss: 0.000285\n",
            "Epoch 3124 | Train Loss: 0.000211 | Validation Loss: 0.000290\n",
            "Epoch 3125 | Train Loss: 0.000227 | Validation Loss: 0.000280\n",
            "Epoch 3126 | Train Loss: 0.000209 | Validation Loss: 0.000271\n",
            "Epoch 3127 | Train Loss: 0.000216 | Validation Loss: 0.000276\n",
            "Epoch 3128 | Train Loss: 0.000218 | Validation Loss: 0.000291\n",
            "Epoch 3129 | Train Loss: 0.000207 | Validation Loss: 0.000288\n",
            "Epoch 3130 | Train Loss: 0.000207 | Validation Loss: 0.000291\n",
            "Epoch 3131 | Train Loss: 0.000214 | Validation Loss: 0.000275\n",
            "Epoch 3132 | Train Loss: 0.000201 | Validation Loss: 0.000279\n",
            "Epoch 3133 | Train Loss: 0.000222 | Validation Loss: 0.000290\n",
            "Epoch 3134 | Train Loss: 0.000208 | Validation Loss: 0.000288\n",
            "Epoch 3135 | Train Loss: 0.000216 | Validation Loss: 0.000288\n",
            "Epoch 3136 | Train Loss: 0.000211 | Validation Loss: 0.000290\n",
            "Epoch 3137 | Train Loss: 0.000209 | Validation Loss: 0.000285\n",
            "Epoch 3138 | Train Loss: 0.000197 | Validation Loss: 0.000269\n",
            "Epoch 3139 | Train Loss: 0.000199 | Validation Loss: 0.000272\n",
            "Epoch 3140 | Train Loss: 0.000203 | Validation Loss: 0.000275\n",
            "Epoch 3141 | Train Loss: 0.000199 | Validation Loss: 0.000277\n",
            "Epoch 3142 | Train Loss: 0.000220 | Validation Loss: 0.000290\n",
            "Epoch 3143 | Train Loss: 0.000212 | Validation Loss: 0.000274\n",
            "Epoch 3144 | Train Loss: 0.000201 | Validation Loss: 0.000286\n",
            "Epoch 3145 | Train Loss: 0.000203 | Validation Loss: 0.000262\n",
            "Epoch 3146 | Train Loss: 0.000206 | Validation Loss: 0.000285\n",
            "Epoch 3147 | Train Loss: 0.000200 | Validation Loss: 0.000279\n",
            "Epoch 3148 | Train Loss: 0.000208 | Validation Loss: 0.000266\n",
            "Epoch 3149 | Train Loss: 0.000210 | Validation Loss: 0.000273\n",
            "Epoch 3150 | Train Loss: 0.000208 | Validation Loss: 0.000279\n",
            "Epoch 3151 | Train Loss: 0.000210 | Validation Loss: 0.000288\n",
            "Epoch 3152 | Train Loss: 0.000214 | Validation Loss: 0.000291\n",
            "Epoch 3153 | Train Loss: 0.000229 | Validation Loss: 0.000288\n",
            "Epoch 3154 | Train Loss: 0.000216 | Validation Loss: 0.000268\n",
            "Epoch 3155 | Train Loss: 0.000219 | Validation Loss: 0.000268\n",
            "Epoch 3156 | Train Loss: 0.000211 | Validation Loss: 0.000284\n",
            "Epoch 3157 | Train Loss: 0.000231 | Validation Loss: 0.000272\n",
            "Epoch 3158 | Train Loss: 0.000216 | Validation Loss: 0.000287\n",
            "Epoch 3159 | Train Loss: 0.000206 | Validation Loss: 0.000289\n",
            "Epoch 3160 | Train Loss: 0.000213 | Validation Loss: 0.000263\n",
            "Epoch 3161 | Train Loss: 0.000209 | Validation Loss: 0.000280\n",
            "Epoch 3162 | Train Loss: 0.000209 | Validation Loss: 0.000284\n",
            "Epoch 3163 | Train Loss: 0.000206 | Validation Loss: 0.000286\n",
            "Epoch 3164 | Train Loss: 0.000202 | Validation Loss: 0.000297\n",
            "Epoch 3165 | Train Loss: 0.000212 | Validation Loss: 0.000285\n",
            "Epoch 3166 | Train Loss: 0.000227 | Validation Loss: 0.000266\n",
            "Epoch 3167 | Train Loss: 0.000204 | Validation Loss: 0.000285\n",
            "Epoch 3168 | Train Loss: 0.000203 | Validation Loss: 0.000276\n",
            "Epoch 3169 | Train Loss: 0.000195 | Validation Loss: 0.000283\n",
            "Epoch 3170 | Train Loss: 0.000208 | Validation Loss: 0.000284\n",
            "Epoch 3171 | Train Loss: 0.000194 | Validation Loss: 0.000390\n",
            "Epoch 3172 | Train Loss: 0.000199 | Validation Loss: 0.000369\n",
            "Epoch 3173 | Train Loss: 0.000202 | Validation Loss: 0.000343\n",
            "Epoch 3174 | Train Loss: 0.000206 | Validation Loss: 0.000284\n",
            "Epoch 3175 | Train Loss: 0.000208 | Validation Loss: 0.000275\n",
            "Epoch 3176 | Train Loss: 0.000204 | Validation Loss: 0.000277\n",
            "Epoch 3177 | Train Loss: 0.000209 | Validation Loss: 0.000262\n",
            "Epoch 3178 | Train Loss: 0.000194 | Validation Loss: 0.000277\n",
            "Epoch 3179 | Train Loss: 0.000211 | Validation Loss: 0.000297\n",
            "Epoch 3180 | Train Loss: 0.000202 | Validation Loss: 0.000266\n",
            "Epoch 3181 | Train Loss: 0.000201 | Validation Loss: 0.000274\n",
            "Epoch 3182 | Train Loss: 0.000217 | Validation Loss: 0.000285\n",
            "Epoch 3183 | Train Loss: 0.000210 | Validation Loss: 0.000275\n",
            "Epoch 3184 | Train Loss: 0.000205 | Validation Loss: 0.000276\n",
            "Epoch 3185 | Train Loss: 0.000203 | Validation Loss: 0.000260\n",
            "Epoch 3186 | Train Loss: 0.000224 | Validation Loss: 0.000298\n",
            "Epoch 3187 | Train Loss: 0.000215 | Validation Loss: 0.000269\n",
            "Epoch 3188 | Train Loss: 0.000211 | Validation Loss: 0.000283\n",
            "Epoch 3189 | Train Loss: 0.000211 | Validation Loss: 0.000297\n",
            "Epoch 3190 | Train Loss: 0.000206 | Validation Loss: 0.000341\n",
            "Epoch 3191 | Train Loss: 0.000211 | Validation Loss: 0.000329\n",
            "Epoch 3192 | Train Loss: 0.000204 | Validation Loss: 0.000336\n",
            "Epoch 3193 | Train Loss: 0.000206 | Validation Loss: 0.000284\n",
            "Epoch 3194 | Train Loss: 0.000209 | Validation Loss: 0.000285\n",
            "Epoch 3195 | Train Loss: 0.000205 | Validation Loss: 0.000295\n",
            "Epoch 3196 | Train Loss: 0.000202 | Validation Loss: 0.000279\n",
            "Epoch 3197 | Train Loss: 0.000211 | Validation Loss: 0.000280\n",
            "Epoch 3198 | Train Loss: 0.000206 | Validation Loss: 0.000271\n",
            "Epoch 3199 | Train Loss: 0.000207 | Validation Loss: 0.000278\n",
            "Epoch 3200 | Train Loss: 0.000213 | Validation Loss: 0.000285\n",
            "Epoch 3201 | Train Loss: 0.000221 | Validation Loss: 0.000288\n",
            "Epoch 3202 | Train Loss: 0.000211 | Validation Loss: 0.000266\n",
            "Epoch 3203 | Train Loss: 0.000212 | Validation Loss: 0.000290\n",
            "Epoch 3204 | Train Loss: 0.000212 | Validation Loss: 0.000289\n",
            "Epoch 3205 | Train Loss: 0.000204 | Validation Loss: 0.000268\n",
            "Epoch 3206 | Train Loss: 0.000198 | Validation Loss: 0.000268\n",
            "Epoch 3207 | Train Loss: 0.000195 | Validation Loss: 0.000271\n",
            "Epoch 3208 | Train Loss: 0.000213 | Validation Loss: 0.000271\n",
            "Epoch 3209 | Train Loss: 0.000210 | Validation Loss: 0.000275\n",
            "Epoch 3210 | Train Loss: 0.000200 | Validation Loss: 0.000277\n",
            "Epoch 3211 | Train Loss: 0.000195 | Validation Loss: 0.000267\n",
            "Epoch 3212 | Train Loss: 0.000199 | Validation Loss: 0.000268\n",
            "Epoch 3213 | Train Loss: 0.000211 | Validation Loss: 0.000283\n",
            "Epoch 3214 | Train Loss: 0.000205 | Validation Loss: 0.000278\n",
            "Epoch 3215 | Train Loss: 0.000193 | Validation Loss: 0.000276\n",
            "Epoch 3216 | Train Loss: 0.000215 | Validation Loss: 0.000281\n",
            "Epoch 3217 | Train Loss: 0.000216 | Validation Loss: 0.000287\n",
            "Epoch 3218 | Train Loss: 0.000224 | Validation Loss: 0.000295\n",
            "Epoch 3219 | Train Loss: 0.000232 | Validation Loss: 0.000297\n",
            "Epoch 3220 | Train Loss: 0.000221 | Validation Loss: 0.000277\n",
            "Epoch 3221 | Train Loss: 0.000211 | Validation Loss: 0.000286\n",
            "Epoch 3222 | Train Loss: 0.000211 | Validation Loss: 0.000270\n",
            "Epoch 3223 | Train Loss: 0.000205 | Validation Loss: 0.000290\n",
            "Epoch 3224 | Train Loss: 0.000202 | Validation Loss: 0.000305\n",
            "Epoch 3225 | Train Loss: 0.000214 | Validation Loss: 0.000303\n",
            "Epoch 3226 | Train Loss: 0.000212 | Validation Loss: 0.000273\n",
            "Epoch 3227 | Train Loss: 0.000207 | Validation Loss: 0.000294\n",
            "Epoch 3228 | Train Loss: 0.000217 | Validation Loss: 0.000303\n",
            "Epoch 3229 | Train Loss: 0.000205 | Validation Loss: 0.000277\n",
            "Epoch 3230 | Train Loss: 0.000219 | Validation Loss: 0.000281\n",
            "Epoch 3231 | Train Loss: 0.000211 | Validation Loss: 0.000295\n",
            "Epoch 3232 | Train Loss: 0.000204 | Validation Loss: 0.000293\n",
            "Epoch 3233 | Train Loss: 0.000215 | Validation Loss: 0.000291\n",
            "Epoch 3234 | Train Loss: 0.000217 | Validation Loss: 0.000275\n",
            "Epoch 3235 | Train Loss: 0.000203 | Validation Loss: 0.000265\n",
            "Epoch 3236 | Train Loss: 0.000201 | Validation Loss: 0.000276\n",
            "Epoch 3237 | Train Loss: 0.000201 | Validation Loss: 0.000264\n",
            "Epoch 3238 | Train Loss: 0.000187 | Validation Loss: 0.000259\n",
            "Epoch 3239 | Train Loss: 0.000196 | Validation Loss: 0.000270\n",
            "Epoch 3240 | Train Loss: 0.000201 | Validation Loss: 0.000281\n",
            "Epoch 3241 | Train Loss: 0.000203 | Validation Loss: 0.000273\n",
            "Epoch 3242 | Train Loss: 0.000194 | Validation Loss: 0.000264\n",
            "Epoch 3243 | Train Loss: 0.000195 | Validation Loss: 0.000271\n",
            "Epoch 3244 | Train Loss: 0.000197 | Validation Loss: 0.000276\n",
            "Epoch 3245 | Train Loss: 0.000205 | Validation Loss: 0.000277\n",
            "Epoch 3246 | Train Loss: 0.000200 | Validation Loss: 0.000252\n",
            "Epoch 3247 | Train Loss: 0.000207 | Validation Loss: 0.000271\n",
            "Epoch 3248 | Train Loss: 0.000199 | Validation Loss: 0.000271\n",
            "Epoch 3249 | Train Loss: 0.000218 | Validation Loss: 0.000270\n",
            "Epoch 3250 | Train Loss: 0.000229 | Validation Loss: 0.000289\n",
            "Epoch 3251 | Train Loss: 0.000209 | Validation Loss: 0.000273\n",
            "Epoch 3252 | Train Loss: 0.000211 | Validation Loss: 0.000272\n",
            "Epoch 3253 | Train Loss: 0.000223 | Validation Loss: 0.000283\n",
            "Epoch 3254 | Train Loss: 0.000217 | Validation Loss: 0.000286\n",
            "Epoch 3255 | Train Loss: 0.000214 | Validation Loss: 0.000271\n",
            "Epoch 3256 | Train Loss: 0.000216 | Validation Loss: 0.000286\n",
            "Epoch 3257 | Train Loss: 0.000221 | Validation Loss: 0.000264\n",
            "Epoch 3258 | Train Loss: 0.000205 | Validation Loss: 0.000273\n",
            "Epoch 3259 | Train Loss: 0.000201 | Validation Loss: 0.000275\n",
            "Epoch 3260 | Train Loss: 0.000209 | Validation Loss: 0.000268\n",
            "Epoch 3261 | Train Loss: 0.000202 | Validation Loss: 0.000268\n",
            "Epoch 3262 | Train Loss: 0.000201 | Validation Loss: 0.000287\n",
            "Epoch 3263 | Train Loss: 0.000201 | Validation Loss: 0.000274\n",
            "Epoch 3264 | Train Loss: 0.000204 | Validation Loss: 0.000280\n",
            "Epoch 3265 | Train Loss: 0.000205 | Validation Loss: 0.000264\n",
            "Epoch 3266 | Train Loss: 0.000201 | Validation Loss: 0.000256\n",
            "Epoch 3267 | Train Loss: 0.000205 | Validation Loss: 0.000281\n",
            "Epoch 3268 | Train Loss: 0.000207 | Validation Loss: 0.000278\n",
            "Epoch 3269 | Train Loss: 0.000207 | Validation Loss: 0.000279\n",
            "Epoch 3270 | Train Loss: 0.000207 | Validation Loss: 0.000274\n",
            "Epoch 3271 | Train Loss: 0.000205 | Validation Loss: 0.000291\n",
            "Epoch 3272 | Train Loss: 0.000197 | Validation Loss: 0.000283\n",
            "Epoch 3273 | Train Loss: 0.000196 | Validation Loss: 0.000273\n",
            "Epoch 3274 | Train Loss: 0.000195 | Validation Loss: 0.000272\n",
            "Epoch 3275 | Train Loss: 0.000186 | Validation Loss: 0.000277\n",
            "Epoch 3276 | Train Loss: 0.000195 | Validation Loss: 0.000282\n",
            "Epoch 3277 | Train Loss: 0.000207 | Validation Loss: 0.000265\n",
            "Epoch 3278 | Train Loss: 0.000213 | Validation Loss: 0.000305\n",
            "Epoch 3279 | Train Loss: 0.000221 | Validation Loss: 0.000295\n",
            "Epoch 3280 | Train Loss: 0.000205 | Validation Loss: 0.000275\n",
            "Epoch 3281 | Train Loss: 0.000201 | Validation Loss: 0.000274\n",
            "Epoch 3282 | Train Loss: 0.000207 | Validation Loss: 0.000280\n",
            "Epoch 3283 | Train Loss: 0.000203 | Validation Loss: 0.000274\n",
            "Epoch 3284 | Train Loss: 0.000199 | Validation Loss: 0.000264\n",
            "Epoch 3285 | Train Loss: 0.000203 | Validation Loss: 0.000277\n",
            "Epoch 3286 | Train Loss: 0.000208 | Validation Loss: 0.000290\n",
            "Epoch 3287 | Train Loss: 0.000213 | Validation Loss: 0.000274\n",
            "Epoch 3288 | Train Loss: 0.000209 | Validation Loss: 0.000275\n",
            "Epoch 3289 | Train Loss: 0.000190 | Validation Loss: 0.000274\n",
            "Epoch 3290 | Train Loss: 0.000189 | Validation Loss: 0.000265\n",
            "Epoch 3291 | Train Loss: 0.000197 | Validation Loss: 0.000280\n",
            "Epoch 3292 | Train Loss: 0.000210 | Validation Loss: 0.000298\n",
            "Epoch 3293 | Train Loss: 0.000207 | Validation Loss: 0.000273\n",
            "Epoch 3294 | Train Loss: 0.000199 | Validation Loss: 0.000281\n",
            "Epoch 3295 | Train Loss: 0.000199 | Validation Loss: 0.000310\n",
            "Epoch 3296 | Train Loss: 0.000218 | Validation Loss: 0.000292\n",
            "Epoch 3297 | Train Loss: 0.000218 | Validation Loss: 0.000271\n",
            "Epoch 3298 | Train Loss: 0.000214 | Validation Loss: 0.000283\n",
            "Epoch 3299 | Train Loss: 0.000198 | Validation Loss: 0.000272\n",
            "Epoch 3300 | Train Loss: 0.000202 | Validation Loss: 0.000260\n",
            "Epoch 3301 | Train Loss: 0.000199 | Validation Loss: 0.000281\n",
            "Epoch 3302 | Train Loss: 0.000203 | Validation Loss: 0.000279\n",
            "Epoch 3303 | Train Loss: 0.000202 | Validation Loss: 0.000268\n",
            "Epoch 3304 | Train Loss: 0.000195 | Validation Loss: 0.000260\n",
            "Epoch 3305 | Train Loss: 0.000201 | Validation Loss: 0.000268\n",
            "Epoch 3306 | Train Loss: 0.000203 | Validation Loss: 0.000271\n",
            "Epoch 3307 | Train Loss: 0.000202 | Validation Loss: 0.000275\n",
            "Epoch 3308 | Train Loss: 0.000198 | Validation Loss: 0.000280\n",
            "Epoch 3309 | Train Loss: 0.000209 | Validation Loss: 0.000291\n",
            "Epoch 3310 | Train Loss: 0.000203 | Validation Loss: 0.000281\n",
            "Epoch 3311 | Train Loss: 0.000210 | Validation Loss: 0.000268\n",
            "Epoch 3312 | Train Loss: 0.000207 | Validation Loss: 0.000264\n",
            "Epoch 3313 | Train Loss: 0.000206 | Validation Loss: 0.000273\n",
            "Epoch 3314 | Train Loss: 0.000197 | Validation Loss: 0.000272\n",
            "Epoch 3315 | Train Loss: 0.000197 | Validation Loss: 0.000266\n",
            "Epoch 3316 | Train Loss: 0.000198 | Validation Loss: 0.000268\n",
            "Epoch 3317 | Train Loss: 0.000217 | Validation Loss: 0.000275\n",
            "Epoch 3318 | Train Loss: 0.000212 | Validation Loss: 0.000283\n",
            "Epoch 3319 | Train Loss: 0.000218 | Validation Loss: 0.000280\n",
            "Epoch 3320 | Train Loss: 0.000218 | Validation Loss: 0.000279\n",
            "Epoch 3321 | Train Loss: 0.000212 | Validation Loss: 0.000290\n",
            "Epoch 3322 | Train Loss: 0.000204 | Validation Loss: 0.000272\n",
            "Epoch 3323 | Train Loss: 0.000195 | Validation Loss: 0.000272\n",
            "Epoch 3324 | Train Loss: 0.000194 | Validation Loss: 0.000272\n",
            "Epoch 3325 | Train Loss: 0.000203 | Validation Loss: 0.000279\n",
            "Epoch 3326 | Train Loss: 0.000212 | Validation Loss: 0.000278\n",
            "Epoch 3327 | Train Loss: 0.000207 | Validation Loss: 0.000290\n",
            "Epoch 3328 | Train Loss: 0.000211 | Validation Loss: 0.000302\n",
            "Epoch 3329 | Train Loss: 0.000215 | Validation Loss: 0.000292\n",
            "Epoch 3330 | Train Loss: 0.000198 | Validation Loss: 0.000255\n",
            "Epoch 3331 | Train Loss: 0.000200 | Validation Loss: 0.000274\n",
            "Epoch 3332 | Train Loss: 0.000196 | Validation Loss: 0.000297\n",
            "Epoch 3333 | Train Loss: 0.000202 | Validation Loss: 0.000279\n",
            "Epoch 3334 | Train Loss: 0.000207 | Validation Loss: 0.000305\n",
            "Epoch 3335 | Train Loss: 0.000210 | Validation Loss: 0.000297\n",
            "Epoch 3336 | Train Loss: 0.000202 | Validation Loss: 0.000272\n",
            "Epoch 3337 | Train Loss: 0.000210 | Validation Loss: 0.000288\n",
            "Epoch 3338 | Train Loss: 0.000200 | Validation Loss: 0.000286\n",
            "Epoch 3339 | Train Loss: 0.000208 | Validation Loss: 0.000283\n",
            "Epoch 3340 | Train Loss: 0.000212 | Validation Loss: 0.000292\n",
            "Epoch 3341 | Train Loss: 0.000201 | Validation Loss: 0.000286\n",
            "Epoch 3342 | Train Loss: 0.000202 | Validation Loss: 0.000253\n",
            "saved!\n",
            "Epoch 3343 | Train Loss: 0.000199 | Validation Loss: 0.000250\n",
            "Epoch 3344 | Train Loss: 0.000198 | Validation Loss: 0.000279\n",
            "Epoch 3345 | Train Loss: 0.000201 | Validation Loss: 0.000257\n",
            "Epoch 3346 | Train Loss: 0.000198 | Validation Loss: 0.000268\n",
            "Epoch 3347 | Train Loss: 0.000198 | Validation Loss: 0.000282\n",
            "Epoch 3348 | Train Loss: 0.000209 | Validation Loss: 0.000291\n",
            "Epoch 3349 | Train Loss: 0.000203 | Validation Loss: 0.000270\n",
            "Epoch 3350 | Train Loss: 0.000209 | Validation Loss: 0.000287\n",
            "Epoch 3351 | Train Loss: 0.000205 | Validation Loss: 0.000277\n",
            "Epoch 3352 | Train Loss: 0.000210 | Validation Loss: 0.000281\n",
            "Epoch 3353 | Train Loss: 0.000216 | Validation Loss: 0.000278\n",
            "Epoch 3354 | Train Loss: 0.000219 | Validation Loss: 0.000298\n",
            "Epoch 3355 | Train Loss: 0.000240 | Validation Loss: 0.000293\n",
            "Epoch 3356 | Train Loss: 0.000207 | Validation Loss: 0.000290\n",
            "Epoch 3357 | Train Loss: 0.000219 | Validation Loss: 0.000279\n",
            "Epoch 3358 | Train Loss: 0.000199 | Validation Loss: 0.000268\n",
            "Epoch 3359 | Train Loss: 0.000210 | Validation Loss: 0.000279\n",
            "Epoch 3360 | Train Loss: 0.000206 | Validation Loss: 0.000264\n",
            "Epoch 3361 | Train Loss: 0.000214 | Validation Loss: 0.000278\n",
            "Epoch 3362 | Train Loss: 0.000197 | Validation Loss: 0.000276\n",
            "Epoch 3363 | Train Loss: 0.000194 | Validation Loss: 0.000250\n",
            "Epoch 3364 | Train Loss: 0.000192 | Validation Loss: 0.000265\n",
            "saved!\n",
            "Epoch 3365 | Train Loss: 0.000201 | Validation Loss: 0.000244\n",
            "Epoch 3366 | Train Loss: 0.000194 | Validation Loss: 0.000274\n",
            "Epoch 3367 | Train Loss: 0.000209 | Validation Loss: 0.000274\n",
            "Epoch 3368 | Train Loss: 0.000200 | Validation Loss: 0.000268\n",
            "Epoch 3369 | Train Loss: 0.000199 | Validation Loss: 0.000265\n",
            "Epoch 3370 | Train Loss: 0.000199 | Validation Loss: 0.000285\n",
            "Epoch 3371 | Train Loss: 0.000201 | Validation Loss: 0.000279\n",
            "Epoch 3372 | Train Loss: 0.000212 | Validation Loss: 0.000282\n",
            "Epoch 3373 | Train Loss: 0.000212 | Validation Loss: 0.000307\n",
            "Epoch 3374 | Train Loss: 0.000202 | Validation Loss: 0.000284\n",
            "Epoch 3375 | Train Loss: 0.000203 | Validation Loss: 0.000275\n",
            "Epoch 3376 | Train Loss: 0.000209 | Validation Loss: 0.000274\n",
            "Epoch 3377 | Train Loss: 0.000194 | Validation Loss: 0.000264\n",
            "Epoch 3378 | Train Loss: 0.000198 | Validation Loss: 0.000275\n",
            "Epoch 3379 | Train Loss: 0.000198 | Validation Loss: 0.000279\n",
            "Epoch 3380 | Train Loss: 0.000210 | Validation Loss: 0.000290\n",
            "Epoch 3381 | Train Loss: 0.000202 | Validation Loss: 0.000268\n",
            "Epoch 3382 | Train Loss: 0.000204 | Validation Loss: 0.000263\n",
            "Epoch 3383 | Train Loss: 0.000201 | Validation Loss: 0.000264\n",
            "Epoch 3384 | Train Loss: 0.000208 | Validation Loss: 0.000275\n",
            "Epoch 3385 | Train Loss: 0.000202 | Validation Loss: 0.000285\n",
            "Epoch 3386 | Train Loss: 0.000200 | Validation Loss: 0.000267\n",
            "Epoch 3387 | Train Loss: 0.000196 | Validation Loss: 0.000279\n",
            "Epoch 3388 | Train Loss: 0.000197 | Validation Loss: 0.000263\n",
            "Epoch 3389 | Train Loss: 0.000201 | Validation Loss: 0.000267\n",
            "Epoch 3390 | Train Loss: 0.000200 | Validation Loss: 0.000262\n",
            "Epoch 3391 | Train Loss: 0.000199 | Validation Loss: 0.000265\n",
            "Epoch 3392 | Train Loss: 0.000206 | Validation Loss: 0.000278\n",
            "Epoch 3393 | Train Loss: 0.000204 | Validation Loss: 0.000285\n",
            "Epoch 3394 | Train Loss: 0.000205 | Validation Loss: 0.000282\n",
            "Epoch 3395 | Train Loss: 0.000208 | Validation Loss: 0.000295\n",
            "Epoch 3396 | Train Loss: 0.000211 | Validation Loss: 0.000285\n",
            "Epoch 3397 | Train Loss: 0.000196 | Validation Loss: 0.000273\n",
            "Epoch 3398 | Train Loss: 0.000197 | Validation Loss: 0.000291\n",
            "Epoch 3399 | Train Loss: 0.000205 | Validation Loss: 0.000276\n",
            "Epoch 3400 | Train Loss: 0.000197 | Validation Loss: 0.000295\n",
            "Epoch 3401 | Train Loss: 0.000216 | Validation Loss: 0.000282\n",
            "Epoch 3402 | Train Loss: 0.000195 | Validation Loss: 0.000280\n",
            "Epoch 3403 | Train Loss: 0.000198 | Validation Loss: 0.000273\n",
            "Epoch 3404 | Train Loss: 0.000198 | Validation Loss: 0.000272\n",
            "Epoch 3405 | Train Loss: 0.000207 | Validation Loss: 0.000284\n",
            "Epoch 3406 | Train Loss: 0.000207 | Validation Loss: 0.000294\n",
            "Epoch 3407 | Train Loss: 0.000209 | Validation Loss: 0.000284\n",
            "Epoch 3408 | Train Loss: 0.000208 | Validation Loss: 0.000276\n",
            "Epoch 3409 | Train Loss: 0.000202 | Validation Loss: 0.000281\n",
            "Epoch 3410 | Train Loss: 0.000201 | Validation Loss: 0.000274\n",
            "Epoch 3411 | Train Loss: 0.000208 | Validation Loss: 0.000272\n",
            "Epoch 3412 | Train Loss: 0.000211 | Validation Loss: 0.000290\n",
            "Epoch 3413 | Train Loss: 0.000226 | Validation Loss: 0.000267\n",
            "Epoch 3414 | Train Loss: 0.000220 | Validation Loss: 0.000278\n",
            "Epoch 3415 | Train Loss: 0.000225 | Validation Loss: 0.000311\n",
            "Epoch 3416 | Train Loss: 0.000234 | Validation Loss: 0.000310\n",
            "Epoch 3417 | Train Loss: 0.000226 | Validation Loss: 0.000279\n",
            "Epoch 3418 | Train Loss: 0.000224 | Validation Loss: 0.000279\n",
            "Epoch 3419 | Train Loss: 0.000216 | Validation Loss: 0.000290\n",
            "Epoch 3420 | Train Loss: 0.000215 | Validation Loss: 0.000283\n",
            "Epoch 3421 | Train Loss: 0.000199 | Validation Loss: 0.000281\n",
            "Epoch 3422 | Train Loss: 0.000204 | Validation Loss: 0.000274\n",
            "Epoch 3423 | Train Loss: 0.000213 | Validation Loss: 0.000283\n",
            "Epoch 3424 | Train Loss: 0.000220 | Validation Loss: 0.000303\n",
            "Epoch 3425 | Train Loss: 0.000198 | Validation Loss: 0.000279\n",
            "Epoch 3426 | Train Loss: 0.000214 | Validation Loss: 0.000291\n",
            "Epoch 3427 | Train Loss: 0.000206 | Validation Loss: 0.000285\n",
            "Epoch 3428 | Train Loss: 0.000210 | Validation Loss: 0.000285\n",
            "Epoch 3429 | Train Loss: 0.000203 | Validation Loss: 0.000273\n",
            "Epoch 3430 | Train Loss: 0.000202 | Validation Loss: 0.000276\n",
            "Epoch 3431 | Train Loss: 0.000214 | Validation Loss: 0.000272\n",
            "Epoch 3432 | Train Loss: 0.000192 | Validation Loss: 0.000276\n",
            "Epoch 3433 | Train Loss: 0.000208 | Validation Loss: 0.000281\n",
            "Epoch 3434 | Train Loss: 0.000207 | Validation Loss: 0.000410\n",
            "Epoch 3435 | Train Loss: 0.000210 | Validation Loss: 0.000273\n",
            "Epoch 3436 | Train Loss: 0.000214 | Validation Loss: 0.000287\n",
            "Epoch 3437 | Train Loss: 0.000207 | Validation Loss: 0.000284\n",
            "Epoch 3438 | Train Loss: 0.000201 | Validation Loss: 0.000270\n",
            "Epoch 3439 | Train Loss: 0.000199 | Validation Loss: 0.000275\n",
            "Epoch 3440 | Train Loss: 0.000200 | Validation Loss: 0.000269\n",
            "Epoch 3441 | Train Loss: 0.000213 | Validation Loss: 0.000286\n",
            "Epoch 3442 | Train Loss: 0.000215 | Validation Loss: 0.000280\n",
            "Epoch 3443 | Train Loss: 0.000195 | Validation Loss: 0.000286\n",
            "Epoch 3444 | Train Loss: 0.000197 | Validation Loss: 0.000285\n",
            "Epoch 3445 | Train Loss: 0.000206 | Validation Loss: 0.000284\n",
            "Epoch 3446 | Train Loss: 0.000202 | Validation Loss: 0.000276\n",
            "Epoch 3447 | Train Loss: 0.000202 | Validation Loss: 0.000282\n",
            "Epoch 3448 | Train Loss: 0.000209 | Validation Loss: 0.000268\n",
            "Epoch 3449 | Train Loss: 0.000229 | Validation Loss: 0.000298\n",
            "Epoch 3450 | Train Loss: 0.000227 | Validation Loss: 0.000291\n",
            "Epoch 3451 | Train Loss: 0.000200 | Validation Loss: 0.000255\n",
            "Epoch 3452 | Train Loss: 0.000201 | Validation Loss: 0.000263\n",
            "Epoch 3453 | Train Loss: 0.000207 | Validation Loss: 0.000258\n",
            "Epoch 3454 | Train Loss: 0.000195 | Validation Loss: 0.000280\n",
            "Epoch 3455 | Train Loss: 0.000198 | Validation Loss: 0.000290\n",
            "Epoch 3456 | Train Loss: 0.000203 | Validation Loss: 0.000379\n",
            "Epoch 3457 | Train Loss: 0.000206 | Validation Loss: 0.000290\n",
            "Epoch 3458 | Train Loss: 0.000205 | Validation Loss: 0.000272\n",
            "Epoch 3459 | Train Loss: 0.000209 | Validation Loss: 0.000265\n",
            "Epoch 3460 | Train Loss: 0.000212 | Validation Loss: 0.000379\n",
            "Epoch 3461 | Train Loss: 0.000215 | Validation Loss: 0.000279\n",
            "Epoch 3462 | Train Loss: 0.000201 | Validation Loss: 0.000288\n",
            "Epoch 3463 | Train Loss: 0.000216 | Validation Loss: 0.000319\n",
            "Epoch 3464 | Train Loss: 0.000201 | Validation Loss: 0.000260\n",
            "Epoch 3465 | Train Loss: 0.000216 | Validation Loss: 0.000299\n",
            "Epoch 3466 | Train Loss: 0.000207 | Validation Loss: 0.000277\n",
            "Epoch 3467 | Train Loss: 0.000217 | Validation Loss: 0.000253\n",
            "Epoch 3468 | Train Loss: 0.000200 | Validation Loss: 0.000265\n",
            "Epoch 3469 | Train Loss: 0.000202 | Validation Loss: 0.000264\n",
            "Epoch 3470 | Train Loss: 0.000219 | Validation Loss: 0.000287\n",
            "Epoch 3471 | Train Loss: 0.000204 | Validation Loss: 0.000274\n",
            "Epoch 3472 | Train Loss: 0.000195 | Validation Loss: 0.000267\n",
            "Epoch 3473 | Train Loss: 0.000198 | Validation Loss: 0.000290\n",
            "Epoch 3474 | Train Loss: 0.000207 | Validation Loss: 0.000284\n",
            "Epoch 3475 | Train Loss: 0.000203 | Validation Loss: 0.000273\n",
            "Epoch 3476 | Train Loss: 0.000215 | Validation Loss: 0.000296\n",
            "Epoch 3477 | Train Loss: 0.000214 | Validation Loss: 0.000285\n",
            "Epoch 3478 | Train Loss: 0.000216 | Validation Loss: 0.000267\n",
            "Epoch 3479 | Train Loss: 0.000211 | Validation Loss: 0.000295\n",
            "Epoch 3480 | Train Loss: 0.000218 | Validation Loss: 0.000270\n",
            "Epoch 3481 | Train Loss: 0.000214 | Validation Loss: 0.000278\n",
            "Epoch 3482 | Train Loss: 0.000205 | Validation Loss: 0.000270\n",
            "Epoch 3483 | Train Loss: 0.000208 | Validation Loss: 0.000297\n",
            "Epoch 3484 | Train Loss: 0.000197 | Validation Loss: 0.000283\n",
            "Epoch 3485 | Train Loss: 0.000202 | Validation Loss: 0.000286\n",
            "Epoch 3486 | Train Loss: 0.000208 | Validation Loss: 0.000278\n",
            "Epoch 3487 | Train Loss: 0.000207 | Validation Loss: 0.000266\n",
            "Epoch 3488 | Train Loss: 0.000201 | Validation Loss: 0.000267\n",
            "Epoch 3489 | Train Loss: 0.000203 | Validation Loss: 0.000280\n",
            "Epoch 3490 | Train Loss: 0.000205 | Validation Loss: 0.000362\n",
            "Epoch 3491 | Train Loss: 0.000189 | Validation Loss: 0.000276\n",
            "Epoch 3492 | Train Loss: 0.000189 | Validation Loss: 0.000285\n",
            "Epoch 3493 | Train Loss: 0.000194 | Validation Loss: 0.000270\n",
            "Epoch 3494 | Train Loss: 0.000194 | Validation Loss: 0.000272\n",
            "Epoch 3495 | Train Loss: 0.000195 | Validation Loss: 0.000266\n",
            "Epoch 3496 | Train Loss: 0.000197 | Validation Loss: 0.000267\n",
            "Epoch 3497 | Train Loss: 0.000207 | Validation Loss: 0.000274\n",
            "Epoch 3498 | Train Loss: 0.000213 | Validation Loss: 0.000274\n",
            "Epoch 3499 | Train Loss: 0.000209 | Validation Loss: 0.000263\n",
            "Epoch 3500 | Train Loss: 0.000197 | Validation Loss: 0.000253\n",
            "Epoch 3501 | Train Loss: 0.000203 | Validation Loss: 0.000277\n",
            "Epoch 3502 | Train Loss: 0.000203 | Validation Loss: 0.000286\n",
            "Epoch 3503 | Train Loss: 0.000211 | Validation Loss: 0.000281\n",
            "Epoch 3504 | Train Loss: 0.000204 | Validation Loss: 0.000286\n",
            "Epoch 3505 | Train Loss: 0.000214 | Validation Loss: 0.000268\n",
            "Epoch 3506 | Train Loss: 0.000209 | Validation Loss: 0.000283\n",
            "Epoch 3507 | Train Loss: 0.000201 | Validation Loss: 0.000271\n",
            "Epoch 3508 | Train Loss: 0.000234 | Validation Loss: 0.000278\n",
            "Epoch 3509 | Train Loss: 0.000198 | Validation Loss: 0.000266\n",
            "Epoch 3510 | Train Loss: 0.000207 | Validation Loss: 0.000267\n",
            "Epoch 3511 | Train Loss: 0.000196 | Validation Loss: 0.000270\n",
            "Epoch 3512 | Train Loss: 0.000206 | Validation Loss: 0.000285\n",
            "Epoch 3513 | Train Loss: 0.000225 | Validation Loss: 0.000297\n",
            "Epoch 3514 | Train Loss: 0.000210 | Validation Loss: 0.000280\n",
            "Epoch 3515 | Train Loss: 0.000209 | Validation Loss: 0.000272\n",
            "Epoch 3516 | Train Loss: 0.000200 | Validation Loss: 0.000274\n",
            "Epoch 3517 | Train Loss: 0.000200 | Validation Loss: 0.000262\n",
            "Epoch 3518 | Train Loss: 0.000199 | Validation Loss: 0.000272\n",
            "Epoch 3519 | Train Loss: 0.000226 | Validation Loss: 0.000263\n",
            "Epoch 3520 | Train Loss: 0.000206 | Validation Loss: 0.000274\n",
            "Epoch 3521 | Train Loss: 0.000198 | Validation Loss: 0.000272\n",
            "Epoch 3522 | Train Loss: 0.000207 | Validation Loss: 0.000390\n",
            "Epoch 3523 | Train Loss: 0.000218 | Validation Loss: 0.000273\n",
            "Epoch 3524 | Train Loss: 0.000207 | Validation Loss: 0.000275\n",
            "Epoch 3525 | Train Loss: 0.000204 | Validation Loss: 0.000389\n",
            "Epoch 3526 | Train Loss: 0.000201 | Validation Loss: 0.000390\n",
            "Epoch 3527 | Train Loss: 0.000193 | Validation Loss: 0.000257\n",
            "Epoch 3528 | Train Loss: 0.000201 | Validation Loss: 0.000280\n",
            "Epoch 3529 | Train Loss: 0.000198 | Validation Loss: 0.000278\n",
            "Epoch 3530 | Train Loss: 0.000204 | Validation Loss: 0.000350\n",
            "Epoch 3531 | Train Loss: 0.000203 | Validation Loss: 0.000388\n",
            "Epoch 3532 | Train Loss: 0.000211 | Validation Loss: 0.000393\n",
            "Epoch 3533 | Train Loss: 0.000210 | Validation Loss: 0.000390\n",
            "Epoch 3534 | Train Loss: 0.000215 | Validation Loss: 0.000385\n",
            "Epoch 3535 | Train Loss: 0.000205 | Validation Loss: 0.000377\n",
            "Epoch 3536 | Train Loss: 0.000205 | Validation Loss: 0.000385\n",
            "Epoch 3537 | Train Loss: 0.000206 | Validation Loss: 0.000392\n",
            "Epoch 3538 | Train Loss: 0.000196 | Validation Loss: 0.000378\n",
            "Epoch 3539 | Train Loss: 0.000207 | Validation Loss: 0.000346\n",
            "Epoch 3540 | Train Loss: 0.000207 | Validation Loss: 0.000403\n",
            "Epoch 3541 | Train Loss: 0.000222 | Validation Loss: 0.000407\n",
            "Epoch 3542 | Train Loss: 0.000208 | Validation Loss: 0.000367\n",
            "Epoch 3543 | Train Loss: 0.000204 | Validation Loss: 0.000260\n",
            "Epoch 3544 | Train Loss: 0.000217 | Validation Loss: 0.000396\n",
            "Epoch 3545 | Train Loss: 0.000215 | Validation Loss: 0.000389\n",
            "Epoch 3546 | Train Loss: 0.000202 | Validation Loss: 0.000373\n",
            "Epoch 3547 | Train Loss: 0.000202 | Validation Loss: 0.000390\n",
            "Epoch 3548 | Train Loss: 0.000225 | Validation Loss: 0.000376\n",
            "Epoch 3549 | Train Loss: 0.000205 | Validation Loss: 0.000254\n",
            "Epoch 3550 | Train Loss: 0.000197 | Validation Loss: 0.000372\n",
            "Epoch 3551 | Train Loss: 0.000216 | Validation Loss: 0.000420\n",
            "Epoch 3552 | Train Loss: 0.000212 | Validation Loss: 0.000400\n",
            "Epoch 3553 | Train Loss: 0.000210 | Validation Loss: 0.000289\n",
            "Epoch 3554 | Train Loss: 0.000215 | Validation Loss: 0.000279\n",
            "Epoch 3555 | Train Loss: 0.000201 | Validation Loss: 0.000291\n",
            "Epoch 3556 | Train Loss: 0.000217 | Validation Loss: 0.000284\n",
            "Epoch 3557 | Train Loss: 0.000210 | Validation Loss: 0.000279\n",
            "Epoch 3558 | Train Loss: 0.000202 | Validation Loss: 0.000248\n",
            "Epoch 3559 | Train Loss: 0.000197 | Validation Loss: 0.000266\n",
            "Epoch 3560 | Train Loss: 0.000210 | Validation Loss: 0.000292\n",
            "Epoch 3561 | Train Loss: 0.000202 | Validation Loss: 0.000270\n",
            "Epoch 3562 | Train Loss: 0.000203 | Validation Loss: 0.000265\n",
            "Epoch 3563 | Train Loss: 0.000207 | Validation Loss: 0.000268\n",
            "Epoch 3564 | Train Loss: 0.000195 | Validation Loss: 0.000275\n",
            "Epoch 3565 | Train Loss: 0.000208 | Validation Loss: 0.000280\n",
            "Epoch 3566 | Train Loss: 0.000218 | Validation Loss: 0.000425\n",
            "Epoch 3567 | Train Loss: 0.000208 | Validation Loss: 0.000369\n",
            "Epoch 3568 | Train Loss: 0.000205 | Validation Loss: 0.000366\n",
            "Epoch 3569 | Train Loss: 0.000194 | Validation Loss: 0.000274\n",
            "Epoch 3570 | Train Loss: 0.000192 | Validation Loss: 0.000400\n",
            "Epoch 3571 | Train Loss: 0.000227 | Validation Loss: 0.000287\n",
            "Epoch 3572 | Train Loss: 0.000224 | Validation Loss: 0.000275\n",
            "Epoch 3573 | Train Loss: 0.000207 | Validation Loss: 0.000287\n",
            "Epoch 3574 | Train Loss: 0.000213 | Validation Loss: 0.000280\n",
            "Epoch 3575 | Train Loss: 0.000193 | Validation Loss: 0.000378\n",
            "Epoch 3576 | Train Loss: 0.000202 | Validation Loss: 0.000289\n",
            "Epoch 3577 | Train Loss: 0.000197 | Validation Loss: 0.000291\n",
            "Epoch 3578 | Train Loss: 0.000194 | Validation Loss: 0.000271\n",
            "Epoch 3579 | Train Loss: 0.000199 | Validation Loss: 0.000266\n",
            "Epoch 3580 | Train Loss: 0.000210 | Validation Loss: 0.000267\n",
            "Epoch 3581 | Train Loss: 0.000204 | Validation Loss: 0.000371\n",
            "Epoch 3582 | Train Loss: 0.000205 | Validation Loss: 0.000272\n",
            "Epoch 3583 | Train Loss: 0.000196 | Validation Loss: 0.000281\n",
            "Epoch 3584 | Train Loss: 0.000211 | Validation Loss: 0.000289\n",
            "Epoch 3585 | Train Loss: 0.000199 | Validation Loss: 0.000263\n",
            "Epoch 3586 | Train Loss: 0.000195 | Validation Loss: 0.000269\n",
            "Epoch 3587 | Train Loss: 0.000203 | Validation Loss: 0.000386\n",
            "Epoch 3588 | Train Loss: 0.000201 | Validation Loss: 0.000362\n",
            "Epoch 3589 | Train Loss: 0.000191 | Validation Loss: 0.000263\n",
            "Epoch 3590 | Train Loss: 0.000195 | Validation Loss: 0.000262\n",
            "Epoch 3591 | Train Loss: 0.000194 | Validation Loss: 0.000255\n",
            "Epoch 3592 | Train Loss: 0.000199 | Validation Loss: 0.000257\n",
            "Epoch 3593 | Train Loss: 0.000194 | Validation Loss: 0.000258\n",
            "Epoch 3594 | Train Loss: 0.000203 | Validation Loss: 0.000267\n",
            "Epoch 3595 | Train Loss: 0.000208 | Validation Loss: 0.000268\n",
            "Epoch 3596 | Train Loss: 0.000201 | Validation Loss: 0.000269\n",
            "Epoch 3597 | Train Loss: 0.000200 | Validation Loss: 0.000264\n",
            "Epoch 3598 | Train Loss: 0.000201 | Validation Loss: 0.000284\n",
            "Epoch 3599 | Train Loss: 0.000209 | Validation Loss: 0.000372\n",
            "Epoch 3600 | Train Loss: 0.000202 | Validation Loss: 0.000287\n",
            "Epoch 3601 | Train Loss: 0.000209 | Validation Loss: 0.000286\n",
            "Epoch 3602 | Train Loss: 0.000220 | Validation Loss: 0.000285\n",
            "Epoch 3603 | Train Loss: 0.000201 | Validation Loss: 0.000276\n",
            "Epoch 3604 | Train Loss: 0.000194 | Validation Loss: 0.000265\n",
            "Epoch 3605 | Train Loss: 0.000195 | Validation Loss: 0.000283\n",
            "Epoch 3606 | Train Loss: 0.000191 | Validation Loss: 0.000285\n",
            "Epoch 3607 | Train Loss: 0.000201 | Validation Loss: 0.000277\n",
            "Epoch 3608 | Train Loss: 0.000206 | Validation Loss: 0.000254\n",
            "Epoch 3609 | Train Loss: 0.000196 | Validation Loss: 0.000289\n",
            "Epoch 3610 | Train Loss: 0.000197 | Validation Loss: 0.000278\n",
            "Epoch 3611 | Train Loss: 0.000203 | Validation Loss: 0.000286\n",
            "Epoch 3612 | Train Loss: 0.000206 | Validation Loss: 0.000282\n",
            "Epoch 3613 | Train Loss: 0.000203 | Validation Loss: 0.000285\n",
            "Epoch 3614 | Train Loss: 0.000197 | Validation Loss: 0.000271\n",
            "Epoch 3615 | Train Loss: 0.000215 | Validation Loss: 0.000298\n",
            "Epoch 3616 | Train Loss: 0.000207 | Validation Loss: 0.000398\n",
            "Epoch 3617 | Train Loss: 0.000203 | Validation Loss: 0.000260\n",
            "Epoch 3618 | Train Loss: 0.000207 | Validation Loss: 0.000277\n",
            "Epoch 3619 | Train Loss: 0.000202 | Validation Loss: 0.000267\n",
            "Epoch 3620 | Train Loss: 0.000197 | Validation Loss: 0.000278\n",
            "Epoch 3621 | Train Loss: 0.000200 | Validation Loss: 0.000271\n",
            "Epoch 3622 | Train Loss: 0.000202 | Validation Loss: 0.000369\n",
            "Epoch 3623 | Train Loss: 0.000201 | Validation Loss: 0.000317\n",
            "Epoch 3624 | Train Loss: 0.000198 | Validation Loss: 0.000267\n",
            "Epoch 3625 | Train Loss: 0.000201 | Validation Loss: 0.000279\n",
            "Epoch 3626 | Train Loss: 0.000202 | Validation Loss: 0.000373\n",
            "Epoch 3627 | Train Loss: 0.000195 | Validation Loss: 0.000325\n",
            "Epoch 3628 | Train Loss: 0.000216 | Validation Loss: 0.000279\n",
            "Epoch 3629 | Train Loss: 0.000216 | Validation Loss: 0.000289\n",
            "Epoch 3630 | Train Loss: 0.000212 | Validation Loss: 0.000398\n",
            "Epoch 3631 | Train Loss: 0.000201 | Validation Loss: 0.000269\n",
            "Epoch 3632 | Train Loss: 0.000190 | Validation Loss: 0.000280\n",
            "Epoch 3633 | Train Loss: 0.000196 | Validation Loss: 0.000264\n",
            "Epoch 3634 | Train Loss: 0.000221 | Validation Loss: 0.000285\n",
            "Epoch 3635 | Train Loss: 0.000207 | Validation Loss: 0.000253\n",
            "Epoch 3636 | Train Loss: 0.000208 | Validation Loss: 0.000274\n",
            "Epoch 3637 | Train Loss: 0.000187 | Validation Loss: 0.000277\n",
            "Epoch 3638 | Train Loss: 0.000193 | Validation Loss: 0.000267\n",
            "Epoch 3639 | Train Loss: 0.000193 | Validation Loss: 0.000270\n",
            "Epoch 3640 | Train Loss: 0.000198 | Validation Loss: 0.000279\n",
            "Epoch 3641 | Train Loss: 0.000202 | Validation Loss: 0.000262\n",
            "Epoch 3642 | Train Loss: 0.000204 | Validation Loss: 0.000279\n",
            "Epoch 3643 | Train Loss: 0.000209 | Validation Loss: 0.000276\n",
            "Epoch 3644 | Train Loss: 0.000191 | Validation Loss: 0.000282\n",
            "Epoch 3645 | Train Loss: 0.000200 | Validation Loss: 0.000273\n",
            "Epoch 3646 | Train Loss: 0.000200 | Validation Loss: 0.000282\n",
            "Epoch 3647 | Train Loss: 0.000209 | Validation Loss: 0.000277\n",
            "Epoch 3648 | Train Loss: 0.000203 | Validation Loss: 0.000379\n",
            "Epoch 3649 | Train Loss: 0.000209 | Validation Loss: 0.000394\n",
            "Epoch 3650 | Train Loss: 0.000204 | Validation Loss: 0.000276\n",
            "Epoch 3651 | Train Loss: 0.000197 | Validation Loss: 0.000381\n",
            "Epoch 3652 | Train Loss: 0.000213 | Validation Loss: 0.000279\n",
            "Epoch 3653 | Train Loss: 0.000195 | Validation Loss: 0.000292\n",
            "Epoch 3654 | Train Loss: 0.000201 | Validation Loss: 0.000413\n",
            "Epoch 3655 | Train Loss: 0.000207 | Validation Loss: 0.000272\n",
            "Epoch 3656 | Train Loss: 0.000203 | Validation Loss: 0.000279\n",
            "Epoch 3657 | Train Loss: 0.000207 | Validation Loss: 0.000255\n",
            "Epoch 3658 | Train Loss: 0.000217 | Validation Loss: 0.000268\n",
            "Epoch 3659 | Train Loss: 0.000219 | Validation Loss: 0.000278\n",
            "Epoch 3660 | Train Loss: 0.000212 | Validation Loss: 0.000270\n",
            "Epoch 3661 | Train Loss: 0.000211 | Validation Loss: 0.000268\n",
            "Epoch 3662 | Train Loss: 0.000200 | Validation Loss: 0.000276\n",
            "Epoch 3663 | Train Loss: 0.000190 | Validation Loss: 0.000263\n",
            "Epoch 3664 | Train Loss: 0.000189 | Validation Loss: 0.000274\n",
            "Epoch 3665 | Train Loss: 0.000195 | Validation Loss: 0.000255\n",
            "Epoch 3666 | Train Loss: 0.000199 | Validation Loss: 0.000274\n",
            "Epoch 3667 | Train Loss: 0.000201 | Validation Loss: 0.000279\n",
            "Epoch 3668 | Train Loss: 0.000214 | Validation Loss: 0.000287\n",
            "Epoch 3669 | Train Loss: 0.000207 | Validation Loss: 0.000265\n",
            "Epoch 3670 | Train Loss: 0.000201 | Validation Loss: 0.000275\n",
            "Epoch 3671 | Train Loss: 0.000207 | Validation Loss: 0.000277\n",
            "Epoch 3672 | Train Loss: 0.000196 | Validation Loss: 0.000292\n",
            "Epoch 3673 | Train Loss: 0.000187 | Validation Loss: 0.000275\n",
            "Epoch 3674 | Train Loss: 0.000194 | Validation Loss: 0.000265\n",
            "Epoch 3675 | Train Loss: 0.000213 | Validation Loss: 0.000275\n",
            "Epoch 3676 | Train Loss: 0.000203 | Validation Loss: 0.000275\n",
            "Epoch 3677 | Train Loss: 0.000201 | Validation Loss: 0.000277\n",
            "Epoch 3678 | Train Loss: 0.000188 | Validation Loss: 0.000269\n",
            "Epoch 3679 | Train Loss: 0.000198 | Validation Loss: 0.000282\n",
            "Epoch 3680 | Train Loss: 0.000194 | Validation Loss: 0.000266\n",
            "Epoch 3681 | Train Loss: 0.000192 | Validation Loss: 0.000258\n",
            "Epoch 3682 | Train Loss: 0.000192 | Validation Loss: 0.000290\n",
            "Epoch 3683 | Train Loss: 0.000197 | Validation Loss: 0.000283\n",
            "Epoch 3684 | Train Loss: 0.000205 | Validation Loss: 0.000284\n",
            "Epoch 3685 | Train Loss: 0.000212 | Validation Loss: 0.000284\n",
            "Epoch 3686 | Train Loss: 0.000221 | Validation Loss: 0.000276\n",
            "Epoch 3687 | Train Loss: 0.000192 | Validation Loss: 0.000281\n",
            "Epoch 3688 | Train Loss: 0.000190 | Validation Loss: 0.000285\n",
            "Epoch 3689 | Train Loss: 0.000195 | Validation Loss: 0.000281\n",
            "Epoch 3690 | Train Loss: 0.000207 | Validation Loss: 0.000387\n",
            "Epoch 3691 | Train Loss: 0.000207 | Validation Loss: 0.000389\n",
            "Epoch 3692 | Train Loss: 0.000205 | Validation Loss: 0.000403\n",
            "Epoch 3693 | Train Loss: 0.000202 | Validation Loss: 0.000404\n",
            "Epoch 3694 | Train Loss: 0.000227 | Validation Loss: 0.000380\n",
            "Epoch 3695 | Train Loss: 0.000214 | Validation Loss: 0.000287\n",
            "Epoch 3696 | Train Loss: 0.000226 | Validation Loss: 0.000290\n",
            "Epoch 3697 | Train Loss: 0.000208 | Validation Loss: 0.000396\n",
            "Epoch 3698 | Train Loss: 0.000218 | Validation Loss: 0.000389\n",
            "Epoch 3699 | Train Loss: 0.000210 | Validation Loss: 0.000392\n",
            "Epoch 3700 | Train Loss: 0.000202 | Validation Loss: 0.000393\n",
            "Epoch 3701 | Train Loss: 0.000200 | Validation Loss: 0.000282\n",
            "Epoch 3702 | Train Loss: 0.000192 | Validation Loss: 0.000381\n",
            "Epoch 3703 | Train Loss: 0.000200 | Validation Loss: 0.000396\n",
            "Epoch 3704 | Train Loss: 0.000201 | Validation Loss: 0.000396\n",
            "Epoch 3705 | Train Loss: 0.000192 | Validation Loss: 0.000393\n",
            "Epoch 3706 | Train Loss: 0.000201 | Validation Loss: 0.000397\n",
            "Epoch 3707 | Train Loss: 0.000207 | Validation Loss: 0.000407\n",
            "Epoch 3708 | Train Loss: 0.000207 | Validation Loss: 0.000392\n",
            "Epoch 3709 | Train Loss: 0.000208 | Validation Loss: 0.000409\n",
            "Epoch 3710 | Train Loss: 0.000206 | Validation Loss: 0.000270\n",
            "Epoch 3711 | Train Loss: 0.000204 | Validation Loss: 0.000378\n",
            "Epoch 3712 | Train Loss: 0.000194 | Validation Loss: 0.000392\n",
            "Epoch 3713 | Train Loss: 0.000196 | Validation Loss: 0.000403\n",
            "Epoch 3714 | Train Loss: 0.000193 | Validation Loss: 0.000280\n",
            "Epoch 3715 | Train Loss: 0.000203 | Validation Loss: 0.000274\n",
            "Epoch 3716 | Train Loss: 0.000205 | Validation Loss: 0.000264\n",
            "Epoch 3717 | Train Loss: 0.000193 | Validation Loss: 0.000258\n",
            "Epoch 3718 | Train Loss: 0.000198 | Validation Loss: 0.000278\n",
            "Epoch 3719 | Train Loss: 0.000205 | Validation Loss: 0.000271\n",
            "Epoch 3720 | Train Loss: 0.000197 | Validation Loss: 0.000272\n",
            "saved!\n",
            "Epoch 3721 | Train Loss: 0.000198 | Validation Loss: 0.000241\n",
            "Epoch 3722 | Train Loss: 0.000209 | Validation Loss: 0.000264\n",
            "Epoch 3723 | Train Loss: 0.000197 | Validation Loss: 0.000270\n",
            "Epoch 3724 | Train Loss: 0.000203 | Validation Loss: 0.000281\n",
            "Epoch 3725 | Train Loss: 0.000194 | Validation Loss: 0.000274\n",
            "Epoch 3726 | Train Loss: 0.000207 | Validation Loss: 0.000281\n",
            "Epoch 3727 | Train Loss: 0.000201 | Validation Loss: 0.000315\n",
            "Epoch 3728 | Train Loss: 0.000186 | Validation Loss: 0.000267\n",
            "Epoch 3729 | Train Loss: 0.000197 | Validation Loss: 0.000271\n",
            "Epoch 3730 | Train Loss: 0.000209 | Validation Loss: 0.000273\n",
            "Epoch 3731 | Train Loss: 0.000200 | Validation Loss: 0.000267\n",
            "Epoch 3732 | Train Loss: 0.000216 | Validation Loss: 0.000282\n",
            "Epoch 3733 | Train Loss: 0.000204 | Validation Loss: 0.000280\n",
            "Epoch 3734 | Train Loss: 0.000199 | Validation Loss: 0.000275\n",
            "Epoch 3735 | Train Loss: 0.000195 | Validation Loss: 0.000254\n",
            "Epoch 3736 | Train Loss: 0.000189 | Validation Loss: 0.000258\n",
            "Epoch 3737 | Train Loss: 0.000203 | Validation Loss: 0.000263\n",
            "Epoch 3738 | Train Loss: 0.000187 | Validation Loss: 0.000273\n",
            "Epoch 3739 | Train Loss: 0.000207 | Validation Loss: 0.000266\n",
            "Epoch 3740 | Train Loss: 0.000199 | Validation Loss: 0.000274\n",
            "Epoch 3741 | Train Loss: 0.000195 | Validation Loss: 0.000262\n",
            "Epoch 3742 | Train Loss: 0.000200 | Validation Loss: 0.000269\n",
            "Epoch 3743 | Train Loss: 0.000196 | Validation Loss: 0.000275\n",
            "Epoch 3744 | Train Loss: 0.000193 | Validation Loss: 0.000263\n",
            "Epoch 3745 | Train Loss: 0.000198 | Validation Loss: 0.000269\n",
            "Epoch 3746 | Train Loss: 0.000199 | Validation Loss: 0.000274\n",
            "Epoch 3747 | Train Loss: 0.000197 | Validation Loss: 0.000264\n",
            "Epoch 3748 | Train Loss: 0.000201 | Validation Loss: 0.000278\n",
            "Epoch 3749 | Train Loss: 0.000201 | Validation Loss: 0.000263\n",
            "Epoch 3750 | Train Loss: 0.000207 | Validation Loss: 0.000284\n",
            "Epoch 3751 | Train Loss: 0.000214 | Validation Loss: 0.000283\n",
            "Epoch 3752 | Train Loss: 0.000213 | Validation Loss: 0.000277\n",
            "Epoch 3753 | Train Loss: 0.000209 | Validation Loss: 0.000282\n",
            "Epoch 3754 | Train Loss: 0.000194 | Validation Loss: 0.000261\n",
            "Epoch 3755 | Train Loss: 0.000211 | Validation Loss: 0.000257\n",
            "Epoch 3756 | Train Loss: 0.000203 | Validation Loss: 0.000281\n",
            "Epoch 3757 | Train Loss: 0.000214 | Validation Loss: 0.000279\n",
            "Epoch 3758 | Train Loss: 0.000228 | Validation Loss: 0.000299\n",
            "Epoch 3759 | Train Loss: 0.000227 | Validation Loss: 0.000280\n",
            "Epoch 3760 | Train Loss: 0.000210 | Validation Loss: 0.000268\n",
            "Epoch 3761 | Train Loss: 0.000201 | Validation Loss: 0.000265\n",
            "Epoch 3762 | Train Loss: 0.000203 | Validation Loss: 0.000270\n",
            "Epoch 3763 | Train Loss: 0.000205 | Validation Loss: 0.000287\n",
            "Epoch 3764 | Train Loss: 0.000222 | Validation Loss: 0.000283\n",
            "Epoch 3765 | Train Loss: 0.000203 | Validation Loss: 0.000282\n",
            "Epoch 3766 | Train Loss: 0.000195 | Validation Loss: 0.000278\n",
            "Epoch 3767 | Train Loss: 0.000196 | Validation Loss: 0.000254\n",
            "Epoch 3768 | Train Loss: 0.000204 | Validation Loss: 0.000278\n",
            "Epoch 3769 | Train Loss: 0.000199 | Validation Loss: 0.000283\n",
            "Epoch 3770 | Train Loss: 0.000205 | Validation Loss: 0.000287\n",
            "Epoch 3771 | Train Loss: 0.000194 | Validation Loss: 0.000257\n",
            "Epoch 3772 | Train Loss: 0.000199 | Validation Loss: 0.000265\n",
            "Epoch 3773 | Train Loss: 0.000195 | Validation Loss: 0.000266\n",
            "Epoch 3774 | Train Loss: 0.000193 | Validation Loss: 0.000272\n",
            "Epoch 3775 | Train Loss: 0.000195 | Validation Loss: 0.000273\n",
            "Epoch 3776 | Train Loss: 0.000192 | Validation Loss: 0.000276\n",
            "Epoch 3777 | Train Loss: 0.000205 | Validation Loss: 0.000266\n",
            "Epoch 3778 | Train Loss: 0.000197 | Validation Loss: 0.000264\n",
            "Epoch 3779 | Train Loss: 0.000188 | Validation Loss: 0.000258\n",
            "Epoch 3780 | Train Loss: 0.000193 | Validation Loss: 0.000267\n",
            "Epoch 3781 | Train Loss: 0.000195 | Validation Loss: 0.000262\n",
            "Epoch 3782 | Train Loss: 0.000205 | Validation Loss: 0.000285\n",
            "Epoch 3783 | Train Loss: 0.000221 | Validation Loss: 0.000267\n",
            "Epoch 3784 | Train Loss: 0.000199 | Validation Loss: 0.000270\n",
            "Epoch 3785 | Train Loss: 0.000201 | Validation Loss: 0.000268\n",
            "Epoch 3786 | Train Loss: 0.000192 | Validation Loss: 0.000271\n",
            "Epoch 3787 | Train Loss: 0.000192 | Validation Loss: 0.000275\n",
            "Epoch 3788 | Train Loss: 0.000203 | Validation Loss: 0.000261\n",
            "Epoch 3789 | Train Loss: 0.000206 | Validation Loss: 0.000256\n",
            "Epoch 3790 | Train Loss: 0.000200 | Validation Loss: 0.000262\n",
            "Epoch 3791 | Train Loss: 0.000194 | Validation Loss: 0.000268\n",
            "Epoch 3792 | Train Loss: 0.000192 | Validation Loss: 0.000257\n",
            "Epoch 3793 | Train Loss: 0.000198 | Validation Loss: 0.000271\n",
            "Epoch 3794 | Train Loss: 0.000198 | Validation Loss: 0.000266\n",
            "Epoch 3795 | Train Loss: 0.000183 | Validation Loss: 0.000274\n",
            "Epoch 3796 | Train Loss: 0.000190 | Validation Loss: 0.000266\n",
            "Epoch 3797 | Train Loss: 0.000195 | Validation Loss: 0.000260\n",
            "Epoch 3798 | Train Loss: 0.000191 | Validation Loss: 0.000281\n",
            "Epoch 3799 | Train Loss: 0.000205 | Validation Loss: 0.000278\n",
            "Epoch 3800 | Train Loss: 0.000206 | Validation Loss: 0.000268\n",
            "Epoch 3801 | Train Loss: 0.000196 | Validation Loss: 0.000277\n",
            "Epoch 3802 | Train Loss: 0.000194 | Validation Loss: 0.000272\n",
            "Epoch 3803 | Train Loss: 0.000195 | Validation Loss: 0.000280\n",
            "Epoch 3804 | Train Loss: 0.000200 | Validation Loss: 0.000269\n",
            "Epoch 3805 | Train Loss: 0.000199 | Validation Loss: 0.000269\n",
            "Epoch 3806 | Train Loss: 0.000187 | Validation Loss: 0.000282\n",
            "Epoch 3807 | Train Loss: 0.000203 | Validation Loss: 0.000279\n",
            "Epoch 3808 | Train Loss: 0.000211 | Validation Loss: 0.000265\n",
            "Epoch 3809 | Train Loss: 0.000202 | Validation Loss: 0.000275\n",
            "Epoch 3810 | Train Loss: 0.000202 | Validation Loss: 0.000271\n",
            "Epoch 3811 | Train Loss: 0.000211 | Validation Loss: 0.000275\n",
            "Epoch 3812 | Train Loss: 0.000210 | Validation Loss: 0.000275\n",
            "Epoch 3813 | Train Loss: 0.000207 | Validation Loss: 0.000262\n",
            "Epoch 3814 | Train Loss: 0.000196 | Validation Loss: 0.000255\n",
            "Epoch 3815 | Train Loss: 0.000195 | Validation Loss: 0.000273\n",
            "Epoch 3816 | Train Loss: 0.000210 | Validation Loss: 0.000263\n",
            "Epoch 3817 | Train Loss: 0.000188 | Validation Loss: 0.000261\n",
            "Epoch 3818 | Train Loss: 0.000199 | Validation Loss: 0.000263\n",
            "Epoch 3819 | Train Loss: 0.000191 | Validation Loss: 0.000297\n",
            "Epoch 3820 | Train Loss: 0.000221 | Validation Loss: 0.000264\n",
            "Epoch 3821 | Train Loss: 0.000204 | Validation Loss: 0.000273\n",
            "Epoch 3822 | Train Loss: 0.000202 | Validation Loss: 0.000275\n",
            "Epoch 3823 | Train Loss: 0.000219 | Validation Loss: 0.000290\n",
            "Epoch 3824 | Train Loss: 0.000212 | Validation Loss: 0.000285\n",
            "Epoch 3825 | Train Loss: 0.000197 | Validation Loss: 0.000273\n",
            "Epoch 3826 | Train Loss: 0.000193 | Validation Loss: 0.000275\n",
            "Epoch 3827 | Train Loss: 0.000210 | Validation Loss: 0.000273\n",
            "Epoch 3828 | Train Loss: 0.000203 | Validation Loss: 0.000286\n",
            "Epoch 3829 | Train Loss: 0.000208 | Validation Loss: 0.000287\n",
            "Epoch 3830 | Train Loss: 0.000211 | Validation Loss: 0.000280\n",
            "Epoch 3831 | Train Loss: 0.000215 | Validation Loss: 0.000287\n",
            "Epoch 3832 | Train Loss: 0.000213 | Validation Loss: 0.000284\n",
            "Epoch 3833 | Train Loss: 0.000206 | Validation Loss: 0.000279\n",
            "Epoch 3834 | Train Loss: 0.000199 | Validation Loss: 0.000271\n",
            "Epoch 3835 | Train Loss: 0.000191 | Validation Loss: 0.000259\n",
            "Epoch 3836 | Train Loss: 0.000194 | Validation Loss: 0.000266\n",
            "Epoch 3837 | Train Loss: 0.000202 | Validation Loss: 0.000288\n",
            "Epoch 3838 | Train Loss: 0.000214 | Validation Loss: 0.000279\n",
            "Epoch 3839 | Train Loss: 0.000211 | Validation Loss: 0.000283\n",
            "Epoch 3840 | Train Loss: 0.000250 | Validation Loss: 0.000305\n",
            "Epoch 3841 | Train Loss: 0.000230 | Validation Loss: 0.000282\n",
            "Epoch 3842 | Train Loss: 0.000221 | Validation Loss: 0.000275\n",
            "Epoch 3843 | Train Loss: 0.000209 | Validation Loss: 0.000272\n",
            "Epoch 3844 | Train Loss: 0.000203 | Validation Loss: 0.000285\n",
            "Epoch 3845 | Train Loss: 0.000196 | Validation Loss: 0.000285\n",
            "Epoch 3846 | Train Loss: 0.000213 | Validation Loss: 0.000292\n",
            "Epoch 3847 | Train Loss: 0.000202 | Validation Loss: 0.000283\n",
            "Epoch 3848 | Train Loss: 0.000194 | Validation Loss: 0.000281\n",
            "Epoch 3849 | Train Loss: 0.000201 | Validation Loss: 0.000276\n",
            "Epoch 3850 | Train Loss: 0.000199 | Validation Loss: 0.000251\n",
            "Epoch 3851 | Train Loss: 0.000197 | Validation Loss: 0.000276\n",
            "Epoch 3852 | Train Loss: 0.000193 | Validation Loss: 0.000283\n",
            "Epoch 3853 | Train Loss: 0.000201 | Validation Loss: 0.000262\n",
            "Epoch 3854 | Train Loss: 0.000185 | Validation Loss: 0.000259\n",
            "Epoch 3855 | Train Loss: 0.000188 | Validation Loss: 0.000265\n",
            "Epoch 3856 | Train Loss: 0.000191 | Validation Loss: 0.000268\n",
            "Epoch 3857 | Train Loss: 0.000192 | Validation Loss: 0.000260\n",
            "Epoch 3858 | Train Loss: 0.000198 | Validation Loss: 0.000274\n",
            "Epoch 3859 | Train Loss: 0.000197 | Validation Loss: 0.000255\n",
            "Epoch 3860 | Train Loss: 0.000200 | Validation Loss: 0.000283\n",
            "Epoch 3861 | Train Loss: 0.000208 | Validation Loss: 0.000279\n",
            "Epoch 3862 | Train Loss: 0.000189 | Validation Loss: 0.000271\n",
            "Epoch 3863 | Train Loss: 0.000195 | Validation Loss: 0.000281\n",
            "Epoch 3864 | Train Loss: 0.000205 | Validation Loss: 0.000271\n",
            "Epoch 3865 | Train Loss: 0.000201 | Validation Loss: 0.000271\n",
            "Epoch 3866 | Train Loss: 0.000194 | Validation Loss: 0.000278\n",
            "Epoch 3867 | Train Loss: 0.000210 | Validation Loss: 0.000294\n",
            "Epoch 3868 | Train Loss: 0.000197 | Validation Loss: 0.000271\n",
            "Epoch 3869 | Train Loss: 0.000204 | Validation Loss: 0.000282\n",
            "Epoch 3870 | Train Loss: 0.000212 | Validation Loss: 0.000286\n",
            "Epoch 3871 | Train Loss: 0.000206 | Validation Loss: 0.000272\n",
            "Epoch 3872 | Train Loss: 0.000207 | Validation Loss: 0.000276\n",
            "Epoch 3873 | Train Loss: 0.000196 | Validation Loss: 0.000278\n",
            "Epoch 3874 | Train Loss: 0.000208 | Validation Loss: 0.000270\n",
            "Epoch 3875 | Train Loss: 0.000202 | Validation Loss: 0.000285\n",
            "Epoch 3876 | Train Loss: 0.000192 | Validation Loss: 0.000282\n",
            "Epoch 3877 | Train Loss: 0.000197 | Validation Loss: 0.000280\n",
            "Epoch 3878 | Train Loss: 0.000198 | Validation Loss: 0.000276\n",
            "Epoch 3879 | Train Loss: 0.000202 | Validation Loss: 0.000268\n",
            "Epoch 3880 | Train Loss: 0.000198 | Validation Loss: 0.000279\n",
            "Epoch 3881 | Train Loss: 0.000194 | Validation Loss: 0.000274\n",
            "Epoch 3882 | Train Loss: 0.000198 | Validation Loss: 0.000264\n",
            "Epoch 3883 | Train Loss: 0.000198 | Validation Loss: 0.000276\n",
            "Epoch 3884 | Train Loss: 0.000194 | Validation Loss: 0.000270\n",
            "Epoch 3885 | Train Loss: 0.000205 | Validation Loss: 0.000268\n",
            "Epoch 3886 | Train Loss: 0.000208 | Validation Loss: 0.000273\n",
            "Epoch 3887 | Train Loss: 0.000212 | Validation Loss: 0.000283\n",
            "Epoch 3888 | Train Loss: 0.000203 | Validation Loss: 0.000298\n",
            "Epoch 3889 | Train Loss: 0.000197 | Validation Loss: 0.000287\n",
            "Epoch 3890 | Train Loss: 0.000207 | Validation Loss: 0.000303\n",
            "Epoch 3891 | Train Loss: 0.000215 | Validation Loss: 0.000292\n",
            "Epoch 3892 | Train Loss: 0.000201 | Validation Loss: 0.000274\n",
            "Epoch 3893 | Train Loss: 0.000202 | Validation Loss: 0.000279\n",
            "Epoch 3894 | Train Loss: 0.000199 | Validation Loss: 0.000272\n",
            "Epoch 3895 | Train Loss: 0.000205 | Validation Loss: 0.000291\n",
            "Epoch 3896 | Train Loss: 0.000194 | Validation Loss: 0.000291\n",
            "Epoch 3897 | Train Loss: 0.000196 | Validation Loss: 0.000265\n",
            "Epoch 3898 | Train Loss: 0.000198 | Validation Loss: 0.000296\n",
            "Epoch 3899 | Train Loss: 0.000210 | Validation Loss: 0.000248\n",
            "Epoch 3900 | Train Loss: 0.000220 | Validation Loss: 0.000277\n",
            "Epoch 3901 | Train Loss: 0.000205 | Validation Loss: 0.000259\n",
            "Epoch 3902 | Train Loss: 0.000193 | Validation Loss: 0.000266\n",
            "Epoch 3903 | Train Loss: 0.000196 | Validation Loss: 0.000267\n",
            "Epoch 3904 | Train Loss: 0.000200 | Validation Loss: 0.000266\n",
            "Epoch 3905 | Train Loss: 0.000217 | Validation Loss: 0.000254\n",
            "Epoch 3906 | Train Loss: 0.000208 | Validation Loss: 0.000391\n",
            "Epoch 3907 | Train Loss: 0.000212 | Validation Loss: 0.000389\n",
            "Epoch 3908 | Train Loss: 0.000205 | Validation Loss: 0.000385\n",
            "Epoch 3909 | Train Loss: 0.000205 | Validation Loss: 0.000382\n",
            "Epoch 3910 | Train Loss: 0.000195 | Validation Loss: 0.000376\n",
            "Epoch 3911 | Train Loss: 0.000199 | Validation Loss: 0.000379\n",
            "Epoch 3912 | Train Loss: 0.000212 | Validation Loss: 0.000386\n",
            "Epoch 3913 | Train Loss: 0.000208 | Validation Loss: 0.000390\n",
            "Epoch 3914 | Train Loss: 0.000204 | Validation Loss: 0.000382\n",
            "Epoch 3915 | Train Loss: 0.000210 | Validation Loss: 0.000375\n",
            "Epoch 3916 | Train Loss: 0.000202 | Validation Loss: 0.000283\n",
            "Epoch 3917 | Train Loss: 0.000217 | Validation Loss: 0.000381\n",
            "Epoch 3918 | Train Loss: 0.000203 | Validation Loss: 0.000284\n",
            "Epoch 3919 | Train Loss: 0.000194 | Validation Loss: 0.000385\n",
            "Epoch 3920 | Train Loss: 0.000188 | Validation Loss: 0.000381\n",
            "Epoch 3921 | Train Loss: 0.000204 | Validation Loss: 0.000383\n",
            "Epoch 3922 | Train Loss: 0.000200 | Validation Loss: 0.000381\n",
            "Epoch 3923 | Train Loss: 0.000198 | Validation Loss: 0.000269\n",
            "Epoch 3924 | Train Loss: 0.000203 | Validation Loss: 0.000408\n",
            "Epoch 3925 | Train Loss: 0.000208 | Validation Loss: 0.000378\n",
            "Epoch 3926 | Train Loss: 0.000193 | Validation Loss: 0.000374\n",
            "Epoch 3927 | Train Loss: 0.000195 | Validation Loss: 0.000383\n",
            "Epoch 3928 | Train Loss: 0.000206 | Validation Loss: 0.000271\n",
            "Epoch 3929 | Train Loss: 0.000214 | Validation Loss: 0.000361\n",
            "Epoch 3930 | Train Loss: 0.000207 | Validation Loss: 0.000368\n",
            "Epoch 3931 | Train Loss: 0.000204 | Validation Loss: 0.000400\n",
            "Epoch 3932 | Train Loss: 0.000211 | Validation Loss: 0.000384\n",
            "Epoch 3933 | Train Loss: 0.000208 | Validation Loss: 0.000399\n",
            "Epoch 3934 | Train Loss: 0.000207 | Validation Loss: 0.000872\n",
            "Epoch 3935 | Train Loss: 0.000197 | Validation Loss: 0.000395\n",
            "Epoch 3936 | Train Loss: 0.000203 | Validation Loss: 0.000406\n",
            "Epoch 3937 | Train Loss: 0.000202 | Validation Loss: 0.000382\n",
            "Epoch 3938 | Train Loss: 0.000199 | Validation Loss: 0.000381\n",
            "Epoch 3939 | Train Loss: 0.000192 | Validation Loss: 0.000389\n",
            "Epoch 3940 | Train Loss: 0.000185 | Validation Loss: 0.000387\n",
            "Epoch 3941 | Train Loss: 0.000208 | Validation Loss: 0.000386\n",
            "Epoch 3942 | Train Loss: 0.000196 | Validation Loss: 0.000380\n",
            "Epoch 3943 | Train Loss: 0.000194 | Validation Loss: 0.000385\n",
            "Epoch 3944 | Train Loss: 0.000203 | Validation Loss: 0.000424\n",
            "Epoch 3945 | Train Loss: 0.000226 | Validation Loss: 0.000414\n",
            "Epoch 3946 | Train Loss: 0.000228 | Validation Loss: 0.000397\n",
            "Epoch 3947 | Train Loss: 0.000215 | Validation Loss: 0.000403\n",
            "Epoch 3948 | Train Loss: 0.000215 | Validation Loss: 0.000278\n",
            "Epoch 3949 | Train Loss: 0.000207 | Validation Loss: 0.000287\n",
            "Epoch 3950 | Train Loss: 0.000203 | Validation Loss: 0.000426\n",
            "Epoch 3951 | Train Loss: 0.000216 | Validation Loss: 0.000420\n",
            "Epoch 3952 | Train Loss: 0.000200 | Validation Loss: 0.000377\n",
            "Epoch 3953 | Train Loss: 0.000207 | Validation Loss: 0.000297\n",
            "Epoch 3954 | Train Loss: 0.000207 | Validation Loss: 0.000343\n",
            "Epoch 3955 | Train Loss: 0.000199 | Validation Loss: 0.000283\n",
            "Epoch 3956 | Train Loss: 0.000194 | Validation Loss: 0.000328\n",
            "Epoch 3957 | Train Loss: 0.000191 | Validation Loss: 0.000282\n",
            "Epoch 3958 | Train Loss: 0.000201 | Validation Loss: 0.000410\n",
            "Epoch 3959 | Train Loss: 0.000223 | Validation Loss: 0.000408\n",
            "Epoch 3960 | Train Loss: 0.000208 | Validation Loss: 0.000401\n",
            "Epoch 3961 | Train Loss: 0.000206 | Validation Loss: 0.000385\n",
            "Epoch 3962 | Train Loss: 0.000192 | Validation Loss: 0.000374\n",
            "Epoch 3963 | Train Loss: 0.000203 | Validation Loss: 0.000381\n",
            "Epoch 3964 | Train Loss: 0.000225 | Validation Loss: 0.000394\n",
            "Epoch 3965 | Train Loss: 0.000207 | Validation Loss: 0.000386\n",
            "Epoch 3966 | Train Loss: 0.000196 | Validation Loss: 0.000395\n",
            "Epoch 3967 | Train Loss: 0.000197 | Validation Loss: 0.000390\n",
            "Epoch 3968 | Train Loss: 0.000197 | Validation Loss: 0.000395\n",
            "Epoch 3969 | Train Loss: 0.000192 | Validation Loss: 0.000371\n",
            "Epoch 3970 | Train Loss: 0.000202 | Validation Loss: 0.000378\n",
            "Epoch 3971 | Train Loss: 0.000210 | Validation Loss: 0.000320\n",
            "Epoch 3972 | Train Loss: 0.000210 | Validation Loss: 0.000288\n",
            "Epoch 3973 | Train Loss: 0.000205 | Validation Loss: 0.000280\n",
            "Epoch 3974 | Train Loss: 0.000201 | Validation Loss: 0.000284\n",
            "Epoch 3975 | Train Loss: 0.000217 | Validation Loss: 0.000386\n",
            "Epoch 3976 | Train Loss: 0.000210 | Validation Loss: 0.000283\n",
            "Epoch 3977 | Train Loss: 0.000200 | Validation Loss: 0.000371\n",
            "Epoch 3978 | Train Loss: 0.000193 | Validation Loss: 0.000384\n",
            "Epoch 3979 | Train Loss: 0.000197 | Validation Loss: 0.000371\n",
            "Epoch 3980 | Train Loss: 0.000194 | Validation Loss: 0.000269\n",
            "Epoch 3981 | Train Loss: 0.000193 | Validation Loss: 0.000386\n",
            "Epoch 3982 | Train Loss: 0.000195 | Validation Loss: 0.000381\n",
            "Epoch 3983 | Train Loss: 0.000200 | Validation Loss: 0.000390\n",
            "Epoch 3984 | Train Loss: 0.000182 | Validation Loss: 0.000374\n",
            "Epoch 3985 | Train Loss: 0.000191 | Validation Loss: 0.000363\n",
            "Epoch 3986 | Train Loss: 0.000189 | Validation Loss: 0.000374\n",
            "Epoch 3987 | Train Loss: 0.000195 | Validation Loss: 0.000363\n",
            "Epoch 3988 | Train Loss: 0.000200 | Validation Loss: 0.000392\n",
            "Epoch 3989 | Train Loss: 0.000186 | Validation Loss: 0.000281\n",
            "Epoch 3990 | Train Loss: 0.000191 | Validation Loss: 0.000269\n",
            "Epoch 3991 | Train Loss: 0.000199 | Validation Loss: 0.000383\n",
            "Epoch 3992 | Train Loss: 0.000204 | Validation Loss: 0.000395\n",
            "Epoch 3993 | Train Loss: 0.000196 | Validation Loss: 0.000380\n",
            "Epoch 3994 | Train Loss: 0.000195 | Validation Loss: 0.000382\n",
            "Epoch 3995 | Train Loss: 0.000209 | Validation Loss: 0.000393\n",
            "Epoch 3996 | Train Loss: 0.000207 | Validation Loss: 0.000384\n",
            "Epoch 3997 | Train Loss: 0.000195 | Validation Loss: 0.000388\n",
            "Epoch 3998 | Train Loss: 0.000200 | Validation Loss: 0.000376\n",
            "Epoch 3999 | Train Loss: 0.000195 | Validation Loss: 0.000355\n",
            "Epoch 4000 | Train Loss: 0.000190 | Validation Loss: 0.000377\n",
            "Epoch 4001 | Train Loss: 0.000188 | Validation Loss: 0.000367\n",
            "Epoch 4002 | Train Loss: 0.000209 | Validation Loss: 0.000368\n",
            "Epoch 4003 | Train Loss: 0.000198 | Validation Loss: 0.000363\n",
            "Epoch 4004 | Train Loss: 0.000185 | Validation Loss: 0.000342\n",
            "Epoch 4005 | Train Loss: 0.000188 | Validation Loss: 0.000374\n",
            "Epoch 4006 | Train Loss: 0.000193 | Validation Loss: 0.000374\n",
            "Epoch 4007 | Train Loss: 0.000198 | Validation Loss: 0.000374\n",
            "Epoch 4008 | Train Loss: 0.000193 | Validation Loss: 0.000362\n",
            "Epoch 4009 | Train Loss: 0.000215 | Validation Loss: 0.000384\n",
            "Epoch 4010 | Train Loss: 0.000215 | Validation Loss: 0.000365\n",
            "Epoch 4011 | Train Loss: 0.000194 | Validation Loss: 0.000381\n",
            "Epoch 4012 | Train Loss: 0.000204 | Validation Loss: 0.000395\n",
            "Epoch 4013 | Train Loss: 0.000201 | Validation Loss: 0.000395\n",
            "Epoch 4014 | Train Loss: 0.000194 | Validation Loss: 0.000394\n",
            "Epoch 4015 | Train Loss: 0.000194 | Validation Loss: 0.000365\n",
            "Epoch 4016 | Train Loss: 0.000195 | Validation Loss: 0.000377\n",
            "Epoch 4017 | Train Loss: 0.000191 | Validation Loss: 0.000400\n",
            "Epoch 4018 | Train Loss: 0.000197 | Validation Loss: 0.000325\n",
            "Epoch 4019 | Train Loss: 0.000208 | Validation Loss: 0.000373\n",
            "Epoch 4020 | Train Loss: 0.000195 | Validation Loss: 0.000273\n",
            "Epoch 4021 | Train Loss: 0.000186 | Validation Loss: 0.000385\n",
            "Epoch 4022 | Train Loss: 0.000191 | Validation Loss: 0.000386\n",
            "Epoch 4023 | Train Loss: 0.000194 | Validation Loss: 0.000392\n",
            "Epoch 4024 | Train Loss: 0.000195 | Validation Loss: 0.000384\n",
            "Epoch 4025 | Train Loss: 0.000205 | Validation Loss: 0.000391\n",
            "Epoch 4026 | Train Loss: 0.000204 | Validation Loss: 0.000372\n",
            "Epoch 4027 | Train Loss: 0.000190 | Validation Loss: 0.000378\n",
            "Epoch 4028 | Train Loss: 0.000210 | Validation Loss: 0.000377\n",
            "Epoch 4029 | Train Loss: 0.000206 | Validation Loss: 0.000411\n",
            "Epoch 4030 | Train Loss: 0.000197 | Validation Loss: 0.000371\n",
            "Epoch 4031 | Train Loss: 0.000190 | Validation Loss: 0.000364\n",
            "Epoch 4032 | Train Loss: 0.000189 | Validation Loss: 0.000387\n",
            "Epoch 4033 | Train Loss: 0.000194 | Validation Loss: 0.000386\n",
            "Epoch 4034 | Train Loss: 0.000205 | Validation Loss: 0.000393\n",
            "Epoch 4035 | Train Loss: 0.000197 | Validation Loss: 0.000249\n",
            "Epoch 4036 | Train Loss: 0.000199 | Validation Loss: 0.000384\n",
            "Epoch 4037 | Train Loss: 0.000194 | Validation Loss: 0.000385\n",
            "Epoch 4038 | Train Loss: 0.000211 | Validation Loss: 0.000273\n",
            "Epoch 4039 | Train Loss: 0.000195 | Validation Loss: 0.000361\n",
            "Epoch 4040 | Train Loss: 0.000200 | Validation Loss: 0.000388\n",
            "Epoch 4041 | Train Loss: 0.000201 | Validation Loss: 0.000387\n",
            "Epoch 4042 | Train Loss: 0.000206 | Validation Loss: 0.000274\n",
            "Epoch 4043 | Train Loss: 0.000208 | Validation Loss: 0.000375\n",
            "Epoch 4044 | Train Loss: 0.000193 | Validation Loss: 0.000267\n",
            "Epoch 4045 | Train Loss: 0.000207 | Validation Loss: 0.000261\n",
            "Epoch 4046 | Train Loss: 0.000200 | Validation Loss: 0.000378\n",
            "Epoch 4047 | Train Loss: 0.000203 | Validation Loss: 0.000262\n",
            "Epoch 4048 | Train Loss: 0.000197 | Validation Loss: 0.000375\n",
            "Epoch 4049 | Train Loss: 0.000198 | Validation Loss: 0.000263\n",
            "Epoch 4050 | Train Loss: 0.000197 | Validation Loss: 0.000391\n",
            "Epoch 4051 | Train Loss: 0.000210 | Validation Loss: 0.000388\n",
            "Epoch 4052 | Train Loss: 0.000187 | Validation Loss: 0.000382\n",
            "Epoch 4053 | Train Loss: 0.000186 | Validation Loss: 0.000415\n",
            "Epoch 4054 | Train Loss: 0.000208 | Validation Loss: 0.000390\n",
            "Epoch 4055 | Train Loss: 0.000199 | Validation Loss: 0.000361\n",
            "Epoch 4056 | Train Loss: 0.000205 | Validation Loss: 0.000392\n",
            "Epoch 4057 | Train Loss: 0.000219 | Validation Loss: 0.000365\n",
            "Epoch 4058 | Train Loss: 0.000206 | Validation Loss: 0.000363\n",
            "Epoch 4059 | Train Loss: 0.000202 | Validation Loss: 0.000397\n",
            "Epoch 4060 | Train Loss: 0.000196 | Validation Loss: 0.000358\n",
            "Epoch 4061 | Train Loss: 0.000197 | Validation Loss: 0.000384\n",
            "Epoch 4062 | Train Loss: 0.000207 | Validation Loss: 0.000403\n",
            "Epoch 4063 | Train Loss: 0.000198 | Validation Loss: 0.000392\n",
            "Epoch 4064 | Train Loss: 0.000223 | Validation Loss: 0.000349\n",
            "Epoch 4065 | Train Loss: 0.000192 | Validation Loss: 0.000389\n",
            "Epoch 4066 | Train Loss: 0.000199 | Validation Loss: 0.000377\n",
            "Epoch 4067 | Train Loss: 0.000191 | Validation Loss: 0.000378\n",
            "Epoch 4068 | Train Loss: 0.000199 | Validation Loss: 0.000362\n",
            "Epoch 4069 | Train Loss: 0.000197 | Validation Loss: 0.000391\n",
            "Epoch 4070 | Train Loss: 0.000209 | Validation Loss: 0.000381\n",
            "Epoch 4071 | Train Loss: 0.000222 | Validation Loss: 0.000380\n",
            "Epoch 4072 | Train Loss: 0.000204 | Validation Loss: 0.000339\n",
            "Epoch 4073 | Train Loss: 0.000207 | Validation Loss: 0.000364\n",
            "Epoch 4074 | Train Loss: 0.000190 | Validation Loss: 0.000368\n",
            "Epoch 4075 | Train Loss: 0.000190 | Validation Loss: 0.000362\n",
            "Epoch 4076 | Train Loss: 0.000194 | Validation Loss: 0.000374\n",
            "Epoch 4077 | Train Loss: 0.000197 | Validation Loss: 0.000382\n",
            "Epoch 4078 | Train Loss: 0.000196 | Validation Loss: 0.000344\n",
            "Epoch 4079 | Train Loss: 0.000196 | Validation Loss: 0.000376\n",
            "Epoch 4080 | Train Loss: 0.000191 | Validation Loss: 0.000397\n",
            "Epoch 4081 | Train Loss: 0.000200 | Validation Loss: 0.000390\n",
            "Epoch 4082 | Train Loss: 0.000201 | Validation Loss: 0.000360\n",
            "Epoch 4083 | Train Loss: 0.000195 | Validation Loss: 0.000373\n",
            "Epoch 4084 | Train Loss: 0.000191 | Validation Loss: 0.000372\n",
            "Epoch 4085 | Train Loss: 0.000187 | Validation Loss: 0.000371\n",
            "Epoch 4086 | Train Loss: 0.000192 | Validation Loss: 0.000374\n",
            "Epoch 4087 | Train Loss: 0.000197 | Validation Loss: 0.000363\n",
            "Epoch 4088 | Train Loss: 0.000202 | Validation Loss: 0.000375\n",
            "Epoch 4089 | Train Loss: 0.000189 | Validation Loss: 0.000371\n",
            "Epoch 4090 | Train Loss: 0.000194 | Validation Loss: 0.000395\n",
            "Epoch 4091 | Train Loss: 0.000191 | Validation Loss: 0.000397\n",
            "Epoch 4092 | Train Loss: 0.000201 | Validation Loss: 0.000251\n",
            "Epoch 4093 | Train Loss: 0.000192 | Validation Loss: 0.000275\n",
            "Epoch 4094 | Train Loss: 0.000191 | Validation Loss: 0.000379\n",
            "Epoch 4095 | Train Loss: 0.000214 | Validation Loss: 0.000392\n",
            "Epoch 4096 | Train Loss: 0.000189 | Validation Loss: 0.000400\n",
            "Epoch 4097 | Train Loss: 0.000203 | Validation Loss: 0.000378\n",
            "Epoch 4098 | Train Loss: 0.000205 | Validation Loss: 0.000374\n",
            "Epoch 4099 | Train Loss: 0.000199 | Validation Loss: 0.000385\n",
            "Epoch 4100 | Train Loss: 0.000196 | Validation Loss: 0.000406\n",
            "Epoch 4101 | Train Loss: 0.000188 | Validation Loss: 0.000373\n",
            "Epoch 4102 | Train Loss: 0.000195 | Validation Loss: 0.000381\n",
            "Epoch 4103 | Train Loss: 0.000198 | Validation Loss: 0.000384\n",
            "Epoch 4104 | Train Loss: 0.000200 | Validation Loss: 0.000377\n",
            "Epoch 4105 | Train Loss: 0.000197 | Validation Loss: 0.000383\n",
            "Epoch 4106 | Train Loss: 0.000196 | Validation Loss: 0.000392\n",
            "Epoch 4107 | Train Loss: 0.000215 | Validation Loss: 0.000395\n",
            "Epoch 4108 | Train Loss: 0.000213 | Validation Loss: 0.000396\n",
            "Epoch 4109 | Train Loss: 0.000188 | Validation Loss: 0.000384\n",
            "Epoch 4110 | Train Loss: 0.000193 | Validation Loss: 0.000384\n",
            "Epoch 4111 | Train Loss: 0.000193 | Validation Loss: 0.000367\n",
            "Epoch 4112 | Train Loss: 0.000193 | Validation Loss: 0.000361\n",
            "Epoch 4113 | Train Loss: 0.000206 | Validation Loss: 0.000389\n",
            "Epoch 4114 | Train Loss: 0.000191 | Validation Loss: 0.000361\n",
            "Epoch 4115 | Train Loss: 0.000191 | Validation Loss: 0.000368\n",
            "Epoch 4116 | Train Loss: 0.000190 | Validation Loss: 0.000363\n",
            "Epoch 4117 | Train Loss: 0.000192 | Validation Loss: 0.000379\n",
            "Epoch 4118 | Train Loss: 0.000190 | Validation Loss: 0.000384\n",
            "Epoch 4119 | Train Loss: 0.000189 | Validation Loss: 0.000388\n",
            "Epoch 4120 | Train Loss: 0.000192 | Validation Loss: 0.000381\n",
            "Epoch 4121 | Train Loss: 0.000188 | Validation Loss: 0.000374\n",
            "Epoch 4122 | Train Loss: 0.000199 | Validation Loss: 0.000267\n",
            "Epoch 4123 | Train Loss: 0.000230 | Validation Loss: 0.000392\n",
            "Epoch 4124 | Train Loss: 0.000200 | Validation Loss: 0.000399\n",
            "Epoch 4125 | Train Loss: 0.000204 | Validation Loss: 0.000407\n",
            "Epoch 4126 | Train Loss: 0.000196 | Validation Loss: 0.000395\n",
            "Epoch 4127 | Train Loss: 0.000191 | Validation Loss: 0.000396\n",
            "Epoch 4128 | Train Loss: 0.000190 | Validation Loss: 0.000374\n",
            "Epoch 4129 | Train Loss: 0.000197 | Validation Loss: 0.000388\n",
            "Epoch 4130 | Train Loss: 0.000200 | Validation Loss: 0.000382\n",
            "Epoch 4131 | Train Loss: 0.000197 | Validation Loss: 0.000387\n",
            "Epoch 4132 | Train Loss: 0.000202 | Validation Loss: 0.000386\n",
            "Epoch 4133 | Train Loss: 0.000207 | Validation Loss: 0.000396\n",
            "Epoch 4134 | Train Loss: 0.000200 | Validation Loss: 0.000385\n",
            "Epoch 4135 | Train Loss: 0.000201 | Validation Loss: 0.000373\n",
            "Epoch 4136 | Train Loss: 0.000194 | Validation Loss: 0.000378\n",
            "Epoch 4137 | Train Loss: 0.000196 | Validation Loss: 0.000399\n",
            "Epoch 4138 | Train Loss: 0.000207 | Validation Loss: 0.000410\n",
            "Epoch 4139 | Train Loss: 0.000208 | Validation Loss: 0.000385\n",
            "Epoch 4140 | Train Loss: 0.000200 | Validation Loss: 0.000374\n",
            "Epoch 4141 | Train Loss: 0.000191 | Validation Loss: 0.000383\n",
            "Epoch 4142 | Train Loss: 0.000216 | Validation Loss: 0.000402\n",
            "Epoch 4143 | Train Loss: 0.000204 | Validation Loss: 0.000388\n",
            "Epoch 4144 | Train Loss: 0.000198 | Validation Loss: 0.000360\n",
            "Epoch 4145 | Train Loss: 0.000194 | Validation Loss: 0.000368\n",
            "Epoch 4146 | Train Loss: 0.000198 | Validation Loss: 0.000364\n",
            "Epoch 4147 | Train Loss: 0.000204 | Validation Loss: 0.000382\n",
            "Epoch 4148 | Train Loss: 0.000201 | Validation Loss: 0.000388\n",
            "Epoch 4149 | Train Loss: 0.000207 | Validation Loss: 0.000374\n",
            "Epoch 4150 | Train Loss: 0.000199 | Validation Loss: 0.000351\n",
            "Epoch 4151 | Train Loss: 0.000196 | Validation Loss: 0.000374\n",
            "Epoch 4152 | Train Loss: 0.000200 | Validation Loss: 0.000376\n",
            "Epoch 4153 | Train Loss: 0.000195 | Validation Loss: 0.000379\n",
            "Epoch 4154 | Train Loss: 0.000208 | Validation Loss: 0.000414\n",
            "Epoch 4155 | Train Loss: 0.000212 | Validation Loss: 0.000274\n",
            "Epoch 4156 | Train Loss: 0.000193 | Validation Loss: 0.000378\n",
            "Epoch 4157 | Train Loss: 0.000202 | Validation Loss: 0.000372\n",
            "Epoch 4158 | Train Loss: 0.000197 | Validation Loss: 0.000381\n",
            "Epoch 4159 | Train Loss: 0.000199 | Validation Loss: 0.000379\n",
            "Epoch 4160 | Train Loss: 0.000201 | Validation Loss: 0.000384\n",
            "Epoch 4161 | Train Loss: 0.000190 | Validation Loss: 0.000376\n",
            "Epoch 4162 | Train Loss: 0.000188 | Validation Loss: 0.000385\n",
            "Epoch 4163 | Train Loss: 0.000199 | Validation Loss: 0.000384\n",
            "Epoch 4164 | Train Loss: 0.000192 | Validation Loss: 0.000379\n",
            "Epoch 4165 | Train Loss: 0.000185 | Validation Loss: 0.000384\n",
            "Epoch 4166 | Train Loss: 0.000188 | Validation Loss: 0.000384\n",
            "Epoch 4167 | Train Loss: 0.000203 | Validation Loss: 0.000361\n",
            "Epoch 4168 | Train Loss: 0.000207 | Validation Loss: 0.000391\n",
            "Epoch 4169 | Train Loss: 0.000209 | Validation Loss: 0.000375\n",
            "Epoch 4170 | Train Loss: 0.000211 | Validation Loss: 0.000373\n",
            "Epoch 4171 | Train Loss: 0.000213 | Validation Loss: 0.000386\n",
            "Epoch 4172 | Train Loss: 0.000205 | Validation Loss: 0.000361\n",
            "Epoch 4173 | Train Loss: 0.000196 | Validation Loss: 0.000384\n",
            "Epoch 4174 | Train Loss: 0.000224 | Validation Loss: 0.000392\n",
            "Epoch 4175 | Train Loss: 0.000202 | Validation Loss: 0.000386\n",
            "Epoch 4176 | Train Loss: 0.000204 | Validation Loss: 0.000382\n",
            "Epoch 4177 | Train Loss: 0.000201 | Validation Loss: 0.000363\n",
            "Epoch 4178 | Train Loss: 0.000199 | Validation Loss: 0.000376\n",
            "Epoch 4179 | Train Loss: 0.000200 | Validation Loss: 0.000380\n",
            "Epoch 4180 | Train Loss: 0.000196 | Validation Loss: 0.000386\n",
            "Epoch 4181 | Train Loss: 0.000198 | Validation Loss: 0.000399\n",
            "Epoch 4182 | Train Loss: 0.000190 | Validation Loss: 0.000389\n",
            "Epoch 4183 | Train Loss: 0.000195 | Validation Loss: 0.000394\n",
            "Epoch 4184 | Train Loss: 0.000185 | Validation Loss: 0.000376\n",
            "Epoch 4185 | Train Loss: 0.000191 | Validation Loss: 0.000388\n",
            "Epoch 4186 | Train Loss: 0.000190 | Validation Loss: 0.000382\n",
            "Epoch 4187 | Train Loss: 0.000214 | Validation Loss: 0.000372\n",
            "Epoch 4188 | Train Loss: 0.000203 | Validation Loss: 0.000378\n",
            "Epoch 4189 | Train Loss: 0.000206 | Validation Loss: 0.000382\n",
            "Epoch 4190 | Train Loss: 0.000211 | Validation Loss: 0.000355\n",
            "Epoch 4191 | Train Loss: 0.000193 | Validation Loss: 0.000386\n",
            "Epoch 4192 | Train Loss: 0.000211 | Validation Loss: 0.000384\n",
            "Epoch 4193 | Train Loss: 0.000200 | Validation Loss: 0.000378\n",
            "Epoch 4194 | Train Loss: 0.000199 | Validation Loss: 0.000382\n",
            "Epoch 4195 | Train Loss: 0.000201 | Validation Loss: 0.000378\n",
            "Epoch 4196 | Train Loss: 0.000194 | Validation Loss: 0.000369\n",
            "Epoch 4197 | Train Loss: 0.000196 | Validation Loss: 0.000366\n",
            "Epoch 4198 | Train Loss: 0.000184 | Validation Loss: 0.000371\n",
            "Epoch 4199 | Train Loss: 0.000195 | Validation Loss: 0.000378\n",
            "Epoch 4200 | Train Loss: 0.000195 | Validation Loss: 0.000352\n",
            "Epoch 4201 | Train Loss: 0.000195 | Validation Loss: 0.000385\n",
            "Epoch 4202 | Train Loss: 0.000197 | Validation Loss: 0.000389\n",
            "Epoch 4203 | Train Loss: 0.000194 | Validation Loss: 0.000366\n",
            "Epoch 4204 | Train Loss: 0.000197 | Validation Loss: 0.000363\n",
            "Epoch 4205 | Train Loss: 0.000197 | Validation Loss: 0.000372\n",
            "Epoch 4206 | Train Loss: 0.000192 | Validation Loss: 0.000382\n",
            "Epoch 4207 | Train Loss: 0.000190 | Validation Loss: 0.000385\n",
            "Epoch 4208 | Train Loss: 0.000190 | Validation Loss: 0.000362\n",
            "Epoch 4209 | Train Loss: 0.000196 | Validation Loss: 0.000373\n",
            "Epoch 4210 | Train Loss: 0.000186 | Validation Loss: 0.000370\n",
            "Epoch 4211 | Train Loss: 0.000188 | Validation Loss: 0.000368\n",
            "Epoch 4212 | Train Loss: 0.000195 | Validation Loss: 0.000372\n",
            "Epoch 4213 | Train Loss: 0.000197 | Validation Loss: 0.000375\n",
            "Epoch 4214 | Train Loss: 0.000186 | Validation Loss: 0.000388\n",
            "Epoch 4215 | Train Loss: 0.000182 | Validation Loss: 0.000369\n",
            "Epoch 4216 | Train Loss: 0.000213 | Validation Loss: 0.000368\n",
            "Epoch 4217 | Train Loss: 0.000196 | Validation Loss: 0.000370\n",
            "Epoch 4218 | Train Loss: 0.000203 | Validation Loss: 0.000374\n",
            "Epoch 4219 | Train Loss: 0.000202 | Validation Loss: 0.000373\n",
            "Epoch 4220 | Train Loss: 0.000211 | Validation Loss: 0.000381\n",
            "Epoch 4221 | Train Loss: 0.000198 | Validation Loss: 0.000384\n",
            "Epoch 4222 | Train Loss: 0.000205 | Validation Loss: 0.000396\n",
            "Epoch 4223 | Train Loss: 0.000189 | Validation Loss: 0.000392\n",
            "Epoch 4224 | Train Loss: 0.000188 | Validation Loss: 0.000396\n",
            "Epoch 4225 | Train Loss: 0.000199 | Validation Loss: 0.000390\n",
            "Epoch 4226 | Train Loss: 0.000189 | Validation Loss: 0.000374\n",
            "Epoch 4227 | Train Loss: 0.000190 | Validation Loss: 0.000364\n",
            "Epoch 4228 | Train Loss: 0.000199 | Validation Loss: 0.000264\n",
            "Epoch 4229 | Train Loss: 0.000200 | Validation Loss: 0.000382\n",
            "Epoch 4230 | Train Loss: 0.000195 | Validation Loss: 0.000371\n",
            "Epoch 4231 | Train Loss: 0.000199 | Validation Loss: 0.000385\n",
            "Epoch 4232 | Train Loss: 0.000194 | Validation Loss: 0.000391\n",
            "Epoch 4233 | Train Loss: 0.000193 | Validation Loss: 0.000258\n",
            "Epoch 4234 | Train Loss: 0.000205 | Validation Loss: 0.000258\n",
            "Epoch 4235 | Train Loss: 0.000189 | Validation Loss: 0.000260\n",
            "Epoch 4236 | Train Loss: 0.000184 | Validation Loss: 0.000265\n",
            "Epoch 4237 | Train Loss: 0.000196 | Validation Loss: 0.000392\n",
            "Epoch 4238 | Train Loss: 0.000199 | Validation Loss: 0.000416\n",
            "Epoch 4239 | Train Loss: 0.000201 | Validation Loss: 0.000287\n",
            "Epoch 4240 | Train Loss: 0.000214 | Validation Loss: 0.000395\n",
            "Epoch 4241 | Train Loss: 0.000206 | Validation Loss: 0.000374\n",
            "Epoch 4242 | Train Loss: 0.000193 | Validation Loss: 0.000373\n",
            "Epoch 4243 | Train Loss: 0.000191 | Validation Loss: 0.000389\n",
            "Epoch 4244 | Train Loss: 0.000214 | Validation Loss: 0.000383\n",
            "Epoch 4245 | Train Loss: 0.000204 | Validation Loss: 0.000382\n",
            "Epoch 4246 | Train Loss: 0.000190 | Validation Loss: 0.000378\n",
            "Epoch 4247 | Train Loss: 0.000201 | Validation Loss: 0.000369\n",
            "Epoch 4248 | Train Loss: 0.000203 | Validation Loss: 0.000378\n",
            "Epoch 4249 | Train Loss: 0.000198 | Validation Loss: 0.000247\n",
            "Epoch 4250 | Train Loss: 0.000194 | Validation Loss: 0.000250\n",
            "Epoch 4251 | Train Loss: 0.000189 | Validation Loss: 0.000267\n",
            "Epoch 4252 | Train Loss: 0.000197 | Validation Loss: 0.000251\n",
            "Epoch 4253 | Train Loss: 0.000183 | Validation Loss: 0.000389\n",
            "Epoch 4254 | Train Loss: 0.000186 | Validation Loss: 0.000371\n",
            "Epoch 4255 | Train Loss: 0.000192 | Validation Loss: 0.000367\n",
            "Epoch 4256 | Train Loss: 0.000186 | Validation Loss: 0.000365\n",
            "Epoch 4257 | Train Loss: 0.000197 | Validation Loss: 0.000377\n",
            "Epoch 4258 | Train Loss: 0.000199 | Validation Loss: 0.000369\n",
            "Epoch 4259 | Train Loss: 0.000213 | Validation Loss: 0.000382\n",
            "Epoch 4260 | Train Loss: 0.000203 | Validation Loss: 0.000388\n",
            "Epoch 4261 | Train Loss: 0.000198 | Validation Loss: 0.000376\n",
            "Epoch 4262 | Train Loss: 0.000199 | Validation Loss: 0.000376\n",
            "Epoch 4263 | Train Loss: 0.000209 | Validation Loss: 0.000376\n",
            "Epoch 4264 | Train Loss: 0.000193 | Validation Loss: 0.000371\n",
            "Epoch 4265 | Train Loss: 0.000194 | Validation Loss: 0.000358\n",
            "Epoch 4266 | Train Loss: 0.000191 | Validation Loss: 0.000375\n",
            "Epoch 4267 | Train Loss: 0.000187 | Validation Loss: 0.000372\n",
            "Epoch 4268 | Train Loss: 0.000199 | Validation Loss: 0.000377\n",
            "Epoch 4269 | Train Loss: 0.000206 | Validation Loss: 0.000385\n",
            "Epoch 4270 | Train Loss: 0.000185 | Validation Loss: 0.000365\n",
            "Epoch 4271 | Train Loss: 0.000200 | Validation Loss: 0.000375\n",
            "Epoch 4272 | Train Loss: 0.000203 | Validation Loss: 0.000366\n",
            "Epoch 4273 | Train Loss: 0.000202 | Validation Loss: 0.000271\n",
            "Epoch 4274 | Train Loss: 0.000192 | Validation Loss: 0.000247\n",
            "Epoch 4275 | Train Loss: 0.000191 | Validation Loss: 0.000369\n",
            "Epoch 4276 | Train Loss: 0.000198 | Validation Loss: 0.000385\n",
            "Epoch 4277 | Train Loss: 0.000213 | Validation Loss: 0.000408\n",
            "Epoch 4278 | Train Loss: 0.000210 | Validation Loss: 0.000380\n",
            "Epoch 4279 | Train Loss: 0.000209 | Validation Loss: 0.000382\n",
            "Epoch 4280 | Train Loss: 0.000201 | Validation Loss: 0.000373\n",
            "Epoch 4281 | Train Loss: 0.000187 | Validation Loss: 0.000387\n",
            "Epoch 4282 | Train Loss: 0.000193 | Validation Loss: 0.000387\n",
            "Epoch 4283 | Train Loss: 0.000198 | Validation Loss: 0.000400\n",
            "Epoch 4284 | Train Loss: 0.000200 | Validation Loss: 0.000393\n",
            "Epoch 4285 | Train Loss: 0.000196 | Validation Loss: 0.000397\n",
            "Epoch 4286 | Train Loss: 0.000200 | Validation Loss: 0.000399\n",
            "Epoch 4287 | Train Loss: 0.000204 | Validation Loss: 0.000290\n",
            "Epoch 4288 | Train Loss: 0.000193 | Validation Loss: 0.000360\n",
            "Epoch 4289 | Train Loss: 0.000201 | Validation Loss: 0.000377\n",
            "Epoch 4290 | Train Loss: 0.000200 | Validation Loss: 0.000356\n",
            "Epoch 4291 | Train Loss: 0.000187 | Validation Loss: 0.000386\n",
            "Epoch 4292 | Train Loss: 0.000195 | Validation Loss: 0.000367\n",
            "Epoch 4293 | Train Loss: 0.000199 | Validation Loss: 0.000374\n",
            "Epoch 4294 | Train Loss: 0.000201 | Validation Loss: 0.000288\n",
            "Epoch 4295 | Train Loss: 0.000182 | Validation Loss: 0.000285\n",
            "Epoch 4296 | Train Loss: 0.000208 | Validation Loss: 0.000399\n",
            "Epoch 4297 | Train Loss: 0.000202 | Validation Loss: 0.000346\n",
            "Epoch 4298 | Train Loss: 0.000201 | Validation Loss: 0.000274\n",
            "Epoch 4299 | Train Loss: 0.000200 | Validation Loss: 0.000373\n",
            "Epoch 4300 | Train Loss: 0.000199 | Validation Loss: 0.000268\n",
            "Epoch 4301 | Train Loss: 0.000195 | Validation Loss: 0.000357\n",
            "Epoch 4302 | Train Loss: 0.000196 | Validation Loss: 0.000348\n",
            "Epoch 4303 | Train Loss: 0.000202 | Validation Loss: 0.000383\n",
            "Epoch 4304 | Train Loss: 0.000193 | Validation Loss: 0.000357\n",
            "Epoch 4305 | Train Loss: 0.000184 | Validation Loss: 0.000250\n",
            "Epoch 4306 | Train Loss: 0.000202 | Validation Loss: 0.000263\n",
            "Epoch 4307 | Train Loss: 0.000193 | Validation Loss: 0.000262\n",
            "Epoch 4308 | Train Loss: 0.000200 | Validation Loss: 0.000265\n",
            "Epoch 4309 | Train Loss: 0.000202 | Validation Loss: 0.000408\n",
            "Epoch 4310 | Train Loss: 0.000214 | Validation Loss: 0.000387\n",
            "Epoch 4311 | Train Loss: 0.000187 | Validation Loss: 0.000365\n",
            "Epoch 4312 | Train Loss: 0.000200 | Validation Loss: 0.000372\n",
            "Epoch 4313 | Train Loss: 0.000215 | Validation Loss: 0.000396\n",
            "Epoch 4314 | Train Loss: 0.000208 | Validation Loss: 0.000375\n",
            "Epoch 4315 | Train Loss: 0.000197 | Validation Loss: 0.000382\n",
            "Epoch 4316 | Train Loss: 0.000202 | Validation Loss: 0.000255\n",
            "Epoch 4317 | Train Loss: 0.000200 | Validation Loss: 0.000387\n",
            "Epoch 4318 | Train Loss: 0.000192 | Validation Loss: 0.000343\n",
            "Epoch 4319 | Train Loss: 0.000188 | Validation Loss: 0.000260\n",
            "Epoch 4320 | Train Loss: 0.000192 | Validation Loss: 0.000259\n",
            "Epoch 4321 | Train Loss: 0.000191 | Validation Loss: 0.000250\n",
            "Epoch 4322 | Train Loss: 0.000193 | Validation Loss: 0.000257\n",
            "Epoch 4323 | Train Loss: 0.000201 | Validation Loss: 0.000266\n",
            "Epoch 4324 | Train Loss: 0.000210 | Validation Loss: 0.000391\n",
            "Epoch 4325 | Train Loss: 0.000196 | Validation Loss: 0.000385\n",
            "Epoch 4326 | Train Loss: 0.000197 | Validation Loss: 0.000383\n",
            "Epoch 4327 | Train Loss: 0.000200 | Validation Loss: 0.000386\n",
            "Epoch 4328 | Train Loss: 0.000197 | Validation Loss: 0.000352\n",
            "Epoch 4329 | Train Loss: 0.000187 | Validation Loss: 0.000253\n",
            "Epoch 4330 | Train Loss: 0.000185 | Validation Loss: 0.000313\n",
            "Epoch 4331 | Train Loss: 0.000184 | Validation Loss: 0.000352\n",
            "Epoch 4332 | Train Loss: 0.000198 | Validation Loss: 0.000255\n",
            "Epoch 4333 | Train Loss: 0.000186 | Validation Loss: 0.000368\n",
            "Epoch 4334 | Train Loss: 0.000188 | Validation Loss: 0.000409\n",
            "Epoch 4335 | Train Loss: 0.000197 | Validation Loss: 0.000394\n",
            "Epoch 4336 | Train Loss: 0.000198 | Validation Loss: 0.000381\n",
            "Epoch 4337 | Train Loss: 0.000205 | Validation Loss: 0.000385\n",
            "Epoch 4338 | Train Loss: 0.000193 | Validation Loss: 0.000377\n",
            "Epoch 4339 | Train Loss: 0.000181 | Validation Loss: 0.000357\n",
            "Epoch 4340 | Train Loss: 0.000187 | Validation Loss: 0.000362\n",
            "Epoch 4341 | Train Loss: 0.000198 | Validation Loss: 0.000405\n",
            "Epoch 4342 | Train Loss: 0.000195 | Validation Loss: 0.000387\n",
            "Epoch 4343 | Train Loss: 0.000200 | Validation Loss: 0.000407\n",
            "Epoch 4344 | Train Loss: 0.000205 | Validation Loss: 0.000393\n",
            "Epoch 4345 | Train Loss: 0.000199 | Validation Loss: 0.000381\n",
            "Epoch 4346 | Train Loss: 0.000199 | Validation Loss: 0.000361\n",
            "Epoch 4347 | Train Loss: 0.000194 | Validation Loss: 0.000391\n",
            "Epoch 4348 | Train Loss: 0.000201 | Validation Loss: 0.000383\n",
            "Epoch 4349 | Train Loss: 0.000191 | Validation Loss: 0.000379\n",
            "Epoch 4350 | Train Loss: 0.000194 | Validation Loss: 0.000378\n",
            "Epoch 4351 | Train Loss: 0.000196 | Validation Loss: 0.000387\n",
            "Epoch 4352 | Train Loss: 0.000201 | Validation Loss: 0.000391\n",
            "Epoch 4353 | Train Loss: 0.000185 | Validation Loss: 0.000371\n",
            "Epoch 4354 | Train Loss: 0.000192 | Validation Loss: 0.000367\n",
            "Epoch 4355 | Train Loss: 0.000191 | Validation Loss: 0.000392\n",
            "Epoch 4356 | Train Loss: 0.000190 | Validation Loss: 0.000391\n",
            "Epoch 4357 | Train Loss: 0.000206 | Validation Loss: 0.000378\n",
            "Epoch 4358 | Train Loss: 0.000192 | Validation Loss: 0.000279\n",
            "Epoch 4359 | Train Loss: 0.000193 | Validation Loss: 0.000285\n",
            "Epoch 4360 | Train Loss: 0.000193 | Validation Loss: 0.000262\n",
            "Epoch 4361 | Train Loss: 0.000190 | Validation Loss: 0.000254\n",
            "Epoch 4362 | Train Loss: 0.000195 | Validation Loss: 0.000268\n",
            "Epoch 4363 | Train Loss: 0.000197 | Validation Loss: 0.000279\n",
            "Epoch 4364 | Train Loss: 0.000203 | Validation Loss: 0.000259\n",
            "Epoch 4365 | Train Loss: 0.000202 | Validation Loss: 0.000269\n",
            "Epoch 4366 | Train Loss: 0.000195 | Validation Loss: 0.000268\n",
            "Epoch 4367 | Train Loss: 0.000185 | Validation Loss: 0.000270\n",
            "Epoch 4368 | Train Loss: 0.000193 | Validation Loss: 0.000262\n",
            "Epoch 4369 | Train Loss: 0.000197 | Validation Loss: 0.000281\n",
            "Epoch 4370 | Train Loss: 0.000193 | Validation Loss: 0.000269\n",
            "Epoch 4371 | Train Loss: 0.000199 | Validation Loss: 0.000250\n",
            "Epoch 4372 | Train Loss: 0.000199 | Validation Loss: 0.000256\n",
            "Epoch 4373 | Train Loss: 0.000199 | Validation Loss: 0.000279\n",
            "Epoch 4374 | Train Loss: 0.000192 | Validation Loss: 0.000266\n",
            "Epoch 4375 | Train Loss: 0.000193 | Validation Loss: 0.000264\n",
            "Epoch 4376 | Train Loss: 0.000191 | Validation Loss: 0.000267\n",
            "Epoch 4377 | Train Loss: 0.000186 | Validation Loss: 0.000271\n",
            "Epoch 4378 | Train Loss: 0.000204 | Validation Loss: 0.000270\n",
            "Epoch 4379 | Train Loss: 0.000193 | Validation Loss: 0.000350\n",
            "Epoch 4380 | Train Loss: 0.000216 | Validation Loss: 0.000285\n",
            "Epoch 4381 | Train Loss: 0.000231 | Validation Loss: 0.000283\n",
            "Epoch 4382 | Train Loss: 0.000217 | Validation Loss: 0.000269\n",
            "Epoch 4383 | Train Loss: 0.000208 | Validation Loss: 0.000267\n",
            "Epoch 4384 | Train Loss: 0.000195 | Validation Loss: 0.000275\n",
            "Epoch 4385 | Train Loss: 0.000201 | Validation Loss: 0.000253\n",
            "Epoch 4386 | Train Loss: 0.000199 | Validation Loss: 0.000269\n",
            "Epoch 4387 | Train Loss: 0.000187 | Validation Loss: 0.000260\n",
            "Epoch 4388 | Train Loss: 0.000189 | Validation Loss: 0.000253\n",
            "Epoch 4389 | Train Loss: 0.000201 | Validation Loss: 0.000252\n",
            "Epoch 4390 | Train Loss: 0.000194 | Validation Loss: 0.000263\n",
            "Epoch 4391 | Train Loss: 0.000188 | Validation Loss: 0.000267\n",
            "Epoch 4392 | Train Loss: 0.000188 | Validation Loss: 0.000266\n",
            "Epoch 4393 | Train Loss: 0.000189 | Validation Loss: 0.000264\n",
            "Epoch 4394 | Train Loss: 0.000187 | Validation Loss: 0.000271\n",
            "Epoch 4395 | Train Loss: 0.000201 | Validation Loss: 0.000282\n",
            "Epoch 4396 | Train Loss: 0.000192 | Validation Loss: 0.000268\n",
            "Epoch 4397 | Train Loss: 0.000206 | Validation Loss: 0.000271\n",
            "Epoch 4398 | Train Loss: 0.000193 | Validation Loss: 0.000253\n",
            "Epoch 4399 | Train Loss: 0.000186 | Validation Loss: 0.000267\n",
            "Epoch 4400 | Train Loss: 0.000199 | Validation Loss: 0.000274\n",
            "Epoch 4401 | Train Loss: 0.000197 | Validation Loss: 0.000268\n",
            "Epoch 4402 | Train Loss: 0.000191 | Validation Loss: 0.000254\n",
            "Epoch 4403 | Train Loss: 0.000182 | Validation Loss: 0.000252\n",
            "Epoch 4404 | Train Loss: 0.000187 | Validation Loss: 0.000259\n",
            "Epoch 4405 | Train Loss: 0.000194 | Validation Loss: 0.000254\n",
            "Epoch 4406 | Train Loss: 0.000198 | Validation Loss: 0.000270\n",
            "Epoch 4407 | Train Loss: 0.000189 | Validation Loss: 0.000262\n",
            "Epoch 4408 | Train Loss: 0.000193 | Validation Loss: 0.000256\n",
            "Epoch 4409 | Train Loss: 0.000193 | Validation Loss: 0.000300\n",
            "Epoch 4410 | Train Loss: 0.000188 | Validation Loss: 0.000265\n",
            "Epoch 4411 | Train Loss: 0.000187 | Validation Loss: 0.000249\n",
            "Epoch 4412 | Train Loss: 0.000183 | Validation Loss: 0.000281\n",
            "Epoch 4413 | Train Loss: 0.000191 | Validation Loss: 0.000272\n",
            "Epoch 4414 | Train Loss: 0.000183 | Validation Loss: 0.000320\n",
            "Epoch 4415 | Train Loss: 0.000186 | Validation Loss: 0.000266\n",
            "Epoch 4416 | Train Loss: 0.000184 | Validation Loss: 0.000319\n",
            "Epoch 4417 | Train Loss: 0.000192 | Validation Loss: 0.000354\n",
            "Epoch 4418 | Train Loss: 0.000186 | Validation Loss: 0.000356\n",
            "Epoch 4419 | Train Loss: 0.000197 | Validation Loss: 0.000267\n",
            "Epoch 4420 | Train Loss: 0.000194 | Validation Loss: 0.000334\n",
            "Epoch 4421 | Train Loss: 0.000189 | Validation Loss: 0.000265\n",
            "Epoch 4422 | Train Loss: 0.000187 | Validation Loss: 0.000361\n",
            "Epoch 4423 | Train Loss: 0.000204 | Validation Loss: 0.000395\n",
            "Epoch 4424 | Train Loss: 0.000201 | Validation Loss: 0.000348\n",
            "Epoch 4425 | Train Loss: 0.000192 | Validation Loss: 0.000339\n",
            "Epoch 4426 | Train Loss: 0.000191 | Validation Loss: 0.000450\n",
            "Epoch 4427 | Train Loss: 0.000185 | Validation Loss: 0.000394\n",
            "Epoch 4428 | Train Loss: 0.000190 | Validation Loss: 0.000462\n",
            "Epoch 4429 | Train Loss: 0.000203 | Validation Loss: 0.000475\n",
            "Epoch 4430 | Train Loss: 0.000196 | Validation Loss: 0.000497\n",
            "Epoch 4431 | Train Loss: 0.000196 | Validation Loss: 0.000458\n",
            "Epoch 4432 | Train Loss: 0.000187 | Validation Loss: 0.000388\n",
            "Epoch 4433 | Train Loss: 0.000187 | Validation Loss: 0.000509\n",
            "Epoch 4434 | Train Loss: 0.000204 | Validation Loss: 0.000574\n",
            "Epoch 4435 | Train Loss: 0.000194 | Validation Loss: 0.000485\n",
            "Epoch 4436 | Train Loss: 0.000202 | Validation Loss: 0.000469\n",
            "Epoch 4437 | Train Loss: 0.000208 | Validation Loss: 0.000356\n",
            "Epoch 4438 | Train Loss: 0.000193 | Validation Loss: 0.000298\n",
            "Epoch 4439 | Train Loss: 0.000187 | Validation Loss: 0.000341\n",
            "Epoch 4440 | Train Loss: 0.000191 | Validation Loss: 0.000329\n",
            "Epoch 4441 | Train Loss: 0.000199 | Validation Loss: 0.000281\n",
            "Epoch 4442 | Train Loss: 0.000214 | Validation Loss: 0.000285\n",
            "Epoch 4443 | Train Loss: 0.000197 | Validation Loss: 0.000281\n",
            "Epoch 4444 | Train Loss: 0.000196 | Validation Loss: 0.000254\n",
            "Epoch 4445 | Train Loss: 0.000194 | Validation Loss: 0.000277\n",
            "Epoch 4446 | Train Loss: 0.000202 | Validation Loss: 0.000280\n",
            "Epoch 4447 | Train Loss: 0.000203 | Validation Loss: 0.000295\n",
            "Epoch 4448 | Train Loss: 0.000188 | Validation Loss: 0.000323\n",
            "Epoch 4449 | Train Loss: 0.000191 | Validation Loss: 0.000273\n",
            "Epoch 4450 | Train Loss: 0.000192 | Validation Loss: 0.000271\n",
            "Epoch 4451 | Train Loss: 0.000196 | Validation Loss: 0.000274\n",
            "Epoch 4452 | Train Loss: 0.000199 | Validation Loss: 0.000268\n",
            "Epoch 4453 | Train Loss: 0.000199 | Validation Loss: 0.000257\n",
            "Epoch 4454 | Train Loss: 0.000196 | Validation Loss: 0.000291\n",
            "Epoch 4455 | Train Loss: 0.000198 | Validation Loss: 0.000268\n",
            "Epoch 4456 | Train Loss: 0.000200 | Validation Loss: 0.000256\n",
            "Epoch 4457 | Train Loss: 0.000199 | Validation Loss: 0.000263\n",
            "Epoch 4458 | Train Loss: 0.000193 | Validation Loss: 0.000268\n",
            "Epoch 4459 | Train Loss: 0.000199 | Validation Loss: 0.000290\n",
            "Epoch 4460 | Train Loss: 0.000193 | Validation Loss: 0.000289\n",
            "Epoch 4461 | Train Loss: 0.000195 | Validation Loss: 0.000290\n",
            "Epoch 4462 | Train Loss: 0.000195 | Validation Loss: 0.000252\n",
            "Epoch 4463 | Train Loss: 0.000192 | Validation Loss: 0.000273\n",
            "Epoch 4464 | Train Loss: 0.000203 | Validation Loss: 0.000279\n",
            "Epoch 4465 | Train Loss: 0.000204 | Validation Loss: 0.000277\n",
            "Epoch 4466 | Train Loss: 0.000215 | Validation Loss: 0.000275\n",
            "Epoch 4467 | Train Loss: 0.000202 | Validation Loss: 0.000264\n",
            "Epoch 4468 | Train Loss: 0.000206 | Validation Loss: 0.000279\n",
            "Epoch 4469 | Train Loss: 0.000201 | Validation Loss: 0.000278\n",
            "Epoch 4470 | Train Loss: 0.000205 | Validation Loss: 0.000296\n",
            "Epoch 4471 | Train Loss: 0.000211 | Validation Loss: 0.000267\n",
            "Epoch 4472 | Train Loss: 0.000194 | Validation Loss: 0.000260\n",
            "Epoch 4473 | Train Loss: 0.000217 | Validation Loss: 0.000287\n",
            "Epoch 4474 | Train Loss: 0.000204 | Validation Loss: 0.000269\n",
            "Epoch 4475 | Train Loss: 0.000200 | Validation Loss: 0.000275\n",
            "Epoch 4476 | Train Loss: 0.000205 | Validation Loss: 0.000405\n",
            "Epoch 4477 | Train Loss: 0.000213 | Validation Loss: 0.000386\n",
            "Epoch 4478 | Train Loss: 0.000204 | Validation Loss: 0.000377\n",
            "Epoch 4479 | Train Loss: 0.000195 | Validation Loss: 0.000274\n",
            "Epoch 4480 | Train Loss: 0.000192 | Validation Loss: 0.000270\n",
            "Epoch 4481 | Train Loss: 0.000203 | Validation Loss: 0.000265\n",
            "Epoch 4482 | Train Loss: 0.000182 | Validation Loss: 0.000254\n",
            "Epoch 4483 | Train Loss: 0.000199 | Validation Loss: 0.000257\n",
            "Epoch 4484 | Train Loss: 0.000212 | Validation Loss: 0.000276\n",
            "Epoch 4485 | Train Loss: 0.000197 | Validation Loss: 0.000274\n",
            "Epoch 4486 | Train Loss: 0.000206 | Validation Loss: 0.000268\n",
            "Epoch 4487 | Train Loss: 0.000218 | Validation Loss: 0.000284\n",
            "Epoch 4488 | Train Loss: 0.000206 | Validation Loss: 0.000256\n",
            "Epoch 4489 | Train Loss: 0.000198 | Validation Loss: 0.000258\n",
            "Epoch 4490 | Train Loss: 0.000190 | Validation Loss: 0.000274\n",
            "Epoch 4491 | Train Loss: 0.000193 | Validation Loss: 0.000253\n",
            "Epoch 4492 | Train Loss: 0.000202 | Validation Loss: 0.000258\n",
            "Epoch 4493 | Train Loss: 0.000197 | Validation Loss: 0.000262\n",
            "Epoch 4494 | Train Loss: 0.000200 | Validation Loss: 0.000281\n",
            "Epoch 4495 | Train Loss: 0.000192 | Validation Loss: 0.000320\n",
            "Epoch 4496 | Train Loss: 0.000192 | Validation Loss: 0.000276\n",
            "Epoch 4497 | Train Loss: 0.000194 | Validation Loss: 0.000289\n",
            "Epoch 4498 | Train Loss: 0.000198 | Validation Loss: 0.000264\n",
            "Epoch 4499 | Train Loss: 0.000207 | Validation Loss: 0.000286\n",
            "Epoch 4500 | Train Loss: 0.000212 | Validation Loss: 0.000271\n",
            "Epoch 4501 | Train Loss: 0.000195 | Validation Loss: 0.000274\n",
            "Epoch 4502 | Train Loss: 0.000193 | Validation Loss: 0.000294\n",
            "Epoch 4503 | Train Loss: 0.000191 | Validation Loss: 0.000261\n",
            "Epoch 4504 | Train Loss: 0.000190 | Validation Loss: 0.000268\n",
            "Epoch 4505 | Train Loss: 0.000185 | Validation Loss: 0.000259\n",
            "Epoch 4506 | Train Loss: 0.000193 | Validation Loss: 0.000267\n",
            "Epoch 4507 | Train Loss: 0.000200 | Validation Loss: 0.000256\n",
            "Epoch 4508 | Train Loss: 0.000211 | Validation Loss: 0.000286\n",
            "Epoch 4509 | Train Loss: 0.000205 | Validation Loss: 0.000261\n",
            "Epoch 4510 | Train Loss: 0.000186 | Validation Loss: 0.000249\n",
            "Epoch 4511 | Train Loss: 0.000196 | Validation Loss: 0.000262\n",
            "Epoch 4512 | Train Loss: 0.000195 | Validation Loss: 0.000267\n",
            "Epoch 4513 | Train Loss: 0.000186 | Validation Loss: 0.000274\n",
            "Epoch 4514 | Train Loss: 0.000184 | Validation Loss: 0.000273\n",
            "Epoch 4515 | Train Loss: 0.000194 | Validation Loss: 0.000327\n",
            "Epoch 4516 | Train Loss: 0.000191 | Validation Loss: 0.000260\n",
            "Epoch 4517 | Train Loss: 0.000193 | Validation Loss: 0.000354\n",
            "Epoch 4518 | Train Loss: 0.000200 | Validation Loss: 0.000351\n",
            "Epoch 4519 | Train Loss: 0.000196 | Validation Loss: 0.000277\n",
            "Epoch 4520 | Train Loss: 0.000194 | Validation Loss: 0.000273\n",
            "Epoch 4521 | Train Loss: 0.000195 | Validation Loss: 0.000270\n",
            "Epoch 4522 | Train Loss: 0.000199 | Validation Loss: 0.000274\n",
            "Epoch 4523 | Train Loss: 0.000216 | Validation Loss: 0.000264\n",
            "Epoch 4524 | Train Loss: 0.000211 | Validation Loss: 0.000263\n",
            "Epoch 4525 | Train Loss: 0.000215 | Validation Loss: 0.000263\n",
            "Epoch 4526 | Train Loss: 0.000201 | Validation Loss: 0.000273\n",
            "Epoch 4527 | Train Loss: 0.000206 | Validation Loss: 0.000251\n",
            "Epoch 4528 | Train Loss: 0.000197 | Validation Loss: 0.000257\n",
            "Epoch 4529 | Train Loss: 0.000193 | Validation Loss: 0.000273\n",
            "Epoch 4530 | Train Loss: 0.000188 | Validation Loss: 0.000266\n",
            "Epoch 4531 | Train Loss: 0.000185 | Validation Loss: 0.000264\n",
            "Epoch 4532 | Train Loss: 0.000186 | Validation Loss: 0.000245\n",
            "Epoch 4533 | Train Loss: 0.000182 | Validation Loss: 0.000260\n",
            "Epoch 4534 | Train Loss: 0.000192 | Validation Loss: 0.000267\n",
            "Epoch 4535 | Train Loss: 0.000201 | Validation Loss: 0.000261\n",
            "Epoch 4536 | Train Loss: 0.000192 | Validation Loss: 0.000505\n",
            "Epoch 4537 | Train Loss: 0.000190 | Validation Loss: 0.000360\n",
            "Epoch 4538 | Train Loss: 0.000183 | Validation Loss: 0.000429\n",
            "Epoch 4539 | Train Loss: 0.000187 | Validation Loss: 0.000510\n",
            "Epoch 4540 | Train Loss: 0.000196 | Validation Loss: 0.000492\n",
            "Epoch 4541 | Train Loss: 0.000195 | Validation Loss: 0.000258\n",
            "Epoch 4542 | Train Loss: 0.000202 | Validation Loss: 0.000276\n",
            "Epoch 4543 | Train Loss: 0.000218 | Validation Loss: 0.000286\n",
            "Epoch 4544 | Train Loss: 0.000226 | Validation Loss: 0.000290\n",
            "Epoch 4545 | Train Loss: 0.000222 | Validation Loss: 0.000278\n",
            "Epoch 4546 | Train Loss: 0.000201 | Validation Loss: 0.000263\n",
            "Epoch 4547 | Train Loss: 0.000191 | Validation Loss: 0.000274\n",
            "Epoch 4548 | Train Loss: 0.000180 | Validation Loss: 0.000256\n",
            "Epoch 4549 | Train Loss: 0.000185 | Validation Loss: 0.000257\n",
            "Epoch 4550 | Train Loss: 0.000195 | Validation Loss: 0.000268\n",
            "Epoch 4551 | Train Loss: 0.000182 | Validation Loss: 0.000250\n",
            "Epoch 4552 | Train Loss: 0.000199 | Validation Loss: 0.000246\n",
            "Epoch 4553 | Train Loss: 0.000197 | Validation Loss: 0.000254\n",
            "Epoch 4554 | Train Loss: 0.000185 | Validation Loss: 0.000272\n",
            "Epoch 4555 | Train Loss: 0.000184 | Validation Loss: 0.000265\n",
            "Epoch 4556 | Train Loss: 0.000185 | Validation Loss: 0.000258\n",
            "Epoch 4557 | Train Loss: 0.000215 | Validation Loss: 0.000257\n",
            "Epoch 4558 | Train Loss: 0.000197 | Validation Loss: 0.000280\n",
            "Epoch 4559 | Train Loss: 0.000199 | Validation Loss: 0.000265\n",
            "Epoch 4560 | Train Loss: 0.000191 | Validation Loss: 0.000264\n",
            "Epoch 4561 | Train Loss: 0.000190 | Validation Loss: 0.000276\n",
            "Epoch 4562 | Train Loss: 0.000196 | Validation Loss: 0.000257\n",
            "Epoch 4563 | Train Loss: 0.000210 | Validation Loss: 0.000259\n",
            "Epoch 4564 | Train Loss: 0.000192 | Validation Loss: 0.000289\n",
            "Epoch 4565 | Train Loss: 0.000190 | Validation Loss: 0.000378\n",
            "Epoch 4566 | Train Loss: 0.000188 | Validation Loss: 0.000255\n",
            "Epoch 4567 | Train Loss: 0.000193 | Validation Loss: 0.000257\n",
            "Epoch 4568 | Train Loss: 0.000189 | Validation Loss: 0.000255\n",
            "Epoch 4569 | Train Loss: 0.000187 | Validation Loss: 0.000261\n",
            "Epoch 4570 | Train Loss: 0.000194 | Validation Loss: 0.000266\n",
            "Epoch 4571 | Train Loss: 0.000200 | Validation Loss: 0.000280\n",
            "Epoch 4572 | Train Loss: 0.000207 | Validation Loss: 0.000257\n",
            "Epoch 4573 | Train Loss: 0.000203 | Validation Loss: 0.000263\n",
            "Epoch 4574 | Train Loss: 0.000202 | Validation Loss: 0.000260\n",
            "Epoch 4575 | Train Loss: 0.000206 | Validation Loss: 0.000279\n",
            "Epoch 4576 | Train Loss: 0.000193 | Validation Loss: 0.000274\n",
            "Epoch 4577 | Train Loss: 0.000184 | Validation Loss: 0.000253\n",
            "Epoch 4578 | Train Loss: 0.000187 | Validation Loss: 0.000256\n",
            "Epoch 4579 | Train Loss: 0.000186 | Validation Loss: 0.000268\n",
            "Epoch 4580 | Train Loss: 0.000190 | Validation Loss: 0.000274\n",
            "Epoch 4581 | Train Loss: 0.000199 | Validation Loss: 0.000266\n",
            "Epoch 4582 | Train Loss: 0.000205 | Validation Loss: 0.000253\n",
            "Epoch 4583 | Train Loss: 0.000193 | Validation Loss: 0.000251\n",
            "saved!\n",
            "Epoch 4584 | Train Loss: 0.000189 | Validation Loss: 0.000236\n",
            "Epoch 4585 | Train Loss: 0.000187 | Validation Loss: 0.000368\n",
            "Epoch 4586 | Train Loss: 0.000187 | Validation Loss: 0.000273\n",
            "Epoch 4587 | Train Loss: 0.000184 | Validation Loss: 0.000261\n",
            "Epoch 4588 | Train Loss: 0.000187 | Validation Loss: 0.000245\n",
            "Epoch 4589 | Train Loss: 0.000188 | Validation Loss: 0.000265\n",
            "Epoch 4590 | Train Loss: 0.000207 | Validation Loss: 0.000255\n",
            "Epoch 4591 | Train Loss: 0.000193 | Validation Loss: 0.000265\n",
            "Epoch 4592 | Train Loss: 0.000186 | Validation Loss: 0.000268\n",
            "Epoch 4593 | Train Loss: 0.000186 | Validation Loss: 0.000275\n",
            "Epoch 4594 | Train Loss: 0.000195 | Validation Loss: 0.000264\n",
            "Epoch 4595 | Train Loss: 0.000207 | Validation Loss: 0.000276\n",
            "Epoch 4596 | Train Loss: 0.000196 | Validation Loss: 0.000387\n",
            "Epoch 4597 | Train Loss: 0.000200 | Validation Loss: 0.000374\n",
            "Epoch 4598 | Train Loss: 0.000205 | Validation Loss: 0.000402\n",
            "Epoch 4599 | Train Loss: 0.000202 | Validation Loss: 0.000367\n",
            "Epoch 4600 | Train Loss: 0.000197 | Validation Loss: 0.000286\n",
            "Epoch 4601 | Train Loss: 0.000195 | Validation Loss: 0.000257\n",
            "Epoch 4602 | Train Loss: 0.000184 | Validation Loss: 0.000258\n",
            "Epoch 4603 | Train Loss: 0.000214 | Validation Loss: 0.000402\n",
            "Epoch 4604 | Train Loss: 0.000209 | Validation Loss: 0.000289\n",
            "Epoch 4605 | Train Loss: 0.000202 | Validation Loss: 0.000371\n",
            "Epoch 4606 | Train Loss: 0.000185 | Validation Loss: 0.000276\n",
            "Epoch 4607 | Train Loss: 0.000187 | Validation Loss: 0.000271\n",
            "Epoch 4608 | Train Loss: 0.000193 | Validation Loss: 0.000248\n",
            "Epoch 4609 | Train Loss: 0.000183 | Validation Loss: 0.000256\n",
            "Epoch 4610 | Train Loss: 0.000188 | Validation Loss: 0.000245\n",
            "Epoch 4611 | Train Loss: 0.000196 | Validation Loss: 0.000252\n",
            "Epoch 4612 | Train Loss: 0.000195 | Validation Loss: 0.000244\n",
            "Epoch 4613 | Train Loss: 0.000192 | Validation Loss: 0.000279\n",
            "Epoch 4614 | Train Loss: 0.000190 | Validation Loss: 0.000256\n",
            "Epoch 4615 | Train Loss: 0.000201 | Validation Loss: 0.000265\n",
            "Epoch 4616 | Train Loss: 0.000205 | Validation Loss: 0.000267\n",
            "Epoch 4617 | Train Loss: 0.000194 | Validation Loss: 0.000262\n",
            "Epoch 4618 | Train Loss: 0.000204 | Validation Loss: 0.000273\n",
            "Epoch 4619 | Train Loss: 0.000205 | Validation Loss: 0.000270\n",
            "Epoch 4620 | Train Loss: 0.000196 | Validation Loss: 0.000253\n",
            "Epoch 4621 | Train Loss: 0.000203 | Validation Loss: 0.000268\n",
            "Epoch 4622 | Train Loss: 0.000197 | Validation Loss: 0.000268\n",
            "Epoch 4623 | Train Loss: 0.000196 | Validation Loss: 0.000259\n",
            "Epoch 4624 | Train Loss: 0.000198 | Validation Loss: 0.000257\n",
            "Epoch 4625 | Train Loss: 0.000197 | Validation Loss: 0.000349\n",
            "Epoch 4626 | Train Loss: 0.000187 | Validation Loss: 0.000381\n",
            "Epoch 4627 | Train Loss: 0.000186 | Validation Loss: 0.000359\n",
            "Epoch 4628 | Train Loss: 0.000188 | Validation Loss: 0.000265\n",
            "Epoch 4629 | Train Loss: 0.000200 | Validation Loss: 0.000258\n",
            "Epoch 4630 | Train Loss: 0.000191 | Validation Loss: 0.000265\n",
            "Epoch 4631 | Train Loss: 0.000189 | Validation Loss: 0.000257\n",
            "Epoch 4632 | Train Loss: 0.000198 | Validation Loss: 0.000254\n",
            "Epoch 4633 | Train Loss: 0.000204 | Validation Loss: 0.000276\n",
            "Epoch 4634 | Train Loss: 0.000196 | Validation Loss: 0.000250\n",
            "Epoch 4635 | Train Loss: 0.000201 | Validation Loss: 0.000250\n",
            "Epoch 4636 | Train Loss: 0.000187 | Validation Loss: 0.000252\n",
            "Epoch 4637 | Train Loss: 0.000184 | Validation Loss: 0.000270\n",
            "Epoch 4638 | Train Loss: 0.000189 | Validation Loss: 0.000263\n",
            "Epoch 4639 | Train Loss: 0.000197 | Validation Loss: 0.000283\n",
            "Epoch 4640 | Train Loss: 0.000189 | Validation Loss: 0.000282\n",
            "Epoch 4641 | Train Loss: 0.000201 | Validation Loss: 0.000261\n",
            "Epoch 4642 | Train Loss: 0.000192 | Validation Loss: 0.000273\n",
            "Epoch 4643 | Train Loss: 0.000201 | Validation Loss: 0.000256\n",
            "Epoch 4644 | Train Loss: 0.000212 | Validation Loss: 0.000262\n",
            "Epoch 4645 | Train Loss: 0.000194 | Validation Loss: 0.000256\n",
            "Epoch 4646 | Train Loss: 0.000187 | Validation Loss: 0.000255\n",
            "Epoch 4647 | Train Loss: 0.000196 | Validation Loss: 0.000370\n",
            "Epoch 4648 | Train Loss: 0.000191 | Validation Loss: 0.000272\n",
            "Epoch 4649 | Train Loss: 0.000188 | Validation Loss: 0.000252\n",
            "Epoch 4650 | Train Loss: 0.000192 | Validation Loss: 0.000265\n",
            "Epoch 4651 | Train Loss: 0.000185 | Validation Loss: 0.000377\n",
            "Epoch 4652 | Train Loss: 0.000194 | Validation Loss: 0.000272\n",
            "Epoch 4653 | Train Loss: 0.000187 | Validation Loss: 0.000294\n",
            "Epoch 4654 | Train Loss: 0.000193 | Validation Loss: 0.000256\n",
            "Epoch 4655 | Train Loss: 0.000192 | Validation Loss: 0.000267\n",
            "Epoch 4656 | Train Loss: 0.000193 | Validation Loss: 0.000385\n",
            "Epoch 4657 | Train Loss: 0.000196 | Validation Loss: 0.000259\n",
            "Epoch 4658 | Train Loss: 0.000193 | Validation Loss: 0.000257\n",
            "Epoch 4659 | Train Loss: 0.000196 | Validation Loss: 0.000271\n",
            "Epoch 4660 | Train Loss: 0.000199 | Validation Loss: 0.000277\n",
            "Epoch 4661 | Train Loss: 0.000207 | Validation Loss: 0.000247\n",
            "Epoch 4662 | Train Loss: 0.000196 | Validation Loss: 0.000260\n",
            "Epoch 4663 | Train Loss: 0.000189 | Validation Loss: 0.000268\n",
            "Epoch 4664 | Train Loss: 0.000199 | Validation Loss: 0.000260\n",
            "Epoch 4665 | Train Loss: 0.000208 | Validation Loss: 0.000272\n",
            "Epoch 4666 | Train Loss: 0.000212 | Validation Loss: 0.000282\n",
            "Epoch 4667 | Train Loss: 0.000205 | Validation Loss: 0.000265\n",
            "Epoch 4668 | Train Loss: 0.000200 | Validation Loss: 0.000264\n",
            "Epoch 4669 | Train Loss: 0.000232 | Validation Loss: 0.000286\n",
            "Epoch 4670 | Train Loss: 0.000214 | Validation Loss: 0.000489\n",
            "Epoch 4671 | Train Loss: 0.000214 | Validation Loss: 0.000277\n",
            "Epoch 4672 | Train Loss: 0.000201 | Validation Loss: 0.000258\n",
            "Epoch 4673 | Train Loss: 0.000220 | Validation Loss: 0.000275\n",
            "Epoch 4674 | Train Loss: 0.000200 | Validation Loss: 0.000290\n",
            "Epoch 4675 | Train Loss: 0.000185 | Validation Loss: 0.000252\n",
            "Epoch 4676 | Train Loss: 0.000197 | Validation Loss: 0.000285\n",
            "Epoch 4677 | Train Loss: 0.000205 | Validation Loss: 0.000277\n",
            "Epoch 4678 | Train Loss: 0.000194 | Validation Loss: 0.000267\n",
            "Epoch 4679 | Train Loss: 0.000204 | Validation Loss: 0.000275\n",
            "Epoch 4680 | Train Loss: 0.000198 | Validation Loss: 0.000369\n",
            "Epoch 4681 | Train Loss: 0.000199 | Validation Loss: 0.000259\n",
            "Epoch 4682 | Train Loss: 0.000192 | Validation Loss: 0.000266\n",
            "Epoch 4683 | Train Loss: 0.000182 | Validation Loss: 0.000252\n",
            "Epoch 4684 | Train Loss: 0.000202 | Validation Loss: 0.000287\n",
            "Epoch 4685 | Train Loss: 0.000200 | Validation Loss: 0.000278\n",
            "Epoch 4686 | Train Loss: 0.000181 | Validation Loss: 0.000250\n",
            "Epoch 4687 | Train Loss: 0.000199 | Validation Loss: 0.000278\n",
            "Epoch 4688 | Train Loss: 0.000194 | Validation Loss: 0.000274\n",
            "Epoch 4689 | Train Loss: 0.000193 | Validation Loss: 0.000242\n",
            "Epoch 4690 | Train Loss: 0.000185 | Validation Loss: 0.000255\n",
            "Epoch 4691 | Train Loss: 0.000188 | Validation Loss: 0.000264\n",
            "Epoch 4692 | Train Loss: 0.000201 | Validation Loss: 0.000259\n",
            "Epoch 4693 | Train Loss: 0.000192 | Validation Loss: 0.000265\n",
            "Epoch 4694 | Train Loss: 0.000202 | Validation Loss: 0.000293\n",
            "Epoch 4695 | Train Loss: 0.000205 | Validation Loss: 0.000291\n",
            "Epoch 4696 | Train Loss: 0.000207 | Validation Loss: 0.000284\n",
            "Epoch 4697 | Train Loss: 0.000199 | Validation Loss: 0.000259\n",
            "Epoch 4698 | Train Loss: 0.000193 | Validation Loss: 0.000245\n",
            "Epoch 4699 | Train Loss: 0.000199 | Validation Loss: 0.000265\n",
            "Epoch 4700 | Train Loss: 0.000193 | Validation Loss: 0.000272\n",
            "Epoch 4701 | Train Loss: 0.000196 | Validation Loss: 0.000261\n",
            "Epoch 4702 | Train Loss: 0.000203 | Validation Loss: 0.000289\n",
            "Epoch 4703 | Train Loss: 0.000198 | Validation Loss: 0.000269\n",
            "Epoch 4704 | Train Loss: 0.000187 | Validation Loss: 0.000277\n",
            "Epoch 4705 | Train Loss: 0.000196 | Validation Loss: 0.000271\n",
            "Epoch 4706 | Train Loss: 0.000188 | Validation Loss: 0.000273\n",
            "Epoch 4707 | Train Loss: 0.000188 | Validation Loss: 0.000253\n",
            "Epoch 4708 | Train Loss: 0.000179 | Validation Loss: 0.000256\n",
            "Epoch 4709 | Train Loss: 0.000194 | Validation Loss: 0.000299\n",
            "Epoch 4710 | Train Loss: 0.000205 | Validation Loss: 0.000368\n",
            "Epoch 4711 | Train Loss: 0.000182 | Validation Loss: 0.000330\n",
            "Epoch 4712 | Train Loss: 0.000196 | Validation Loss: 0.000270\n",
            "Epoch 4713 | Train Loss: 0.000197 | Validation Loss: 0.000392\n",
            "Epoch 4714 | Train Loss: 0.000206 | Validation Loss: 0.000371\n",
            "Epoch 4715 | Train Loss: 0.000222 | Validation Loss: 0.000286\n",
            "Epoch 4716 | Train Loss: 0.000208 | Validation Loss: 0.000306\n",
            "Epoch 4717 | Train Loss: 0.000219 | Validation Loss: 0.000273\n",
            "Epoch 4718 | Train Loss: 0.000205 | Validation Loss: 0.000302\n",
            "Epoch 4719 | Train Loss: 0.000215 | Validation Loss: 0.000297\n",
            "Epoch 4720 | Train Loss: 0.000197 | Validation Loss: 0.000263\n",
            "Epoch 4721 | Train Loss: 0.000202 | Validation Loss: 0.000282\n",
            "Epoch 4722 | Train Loss: 0.000198 | Validation Loss: 0.000291\n",
            "Epoch 4723 | Train Loss: 0.000199 | Validation Loss: 0.000280\n",
            "Epoch 4724 | Train Loss: 0.000198 | Validation Loss: 0.000294\n",
            "Epoch 4725 | Train Loss: 0.000196 | Validation Loss: 0.000262\n",
            "Epoch 4726 | Train Loss: 0.000186 | Validation Loss: 0.000280\n",
            "Epoch 4727 | Train Loss: 0.000186 | Validation Loss: 0.000285\n",
            "Epoch 4728 | Train Loss: 0.000193 | Validation Loss: 0.000293\n",
            "Epoch 4729 | Train Loss: 0.000198 | Validation Loss: 0.000280\n",
            "Epoch 4730 | Train Loss: 0.000194 | Validation Loss: 0.000297\n",
            "Epoch 4731 | Train Loss: 0.000198 | Validation Loss: 0.000261\n",
            "Epoch 4732 | Train Loss: 0.000197 | Validation Loss: 0.000267\n",
            "Epoch 4733 | Train Loss: 0.000192 | Validation Loss: 0.000275\n",
            "Epoch 4734 | Train Loss: 0.000194 | Validation Loss: 0.000261\n",
            "Epoch 4735 | Train Loss: 0.000192 | Validation Loss: 0.000268\n",
            "Epoch 4736 | Train Loss: 0.000190 | Validation Loss: 0.000293\n",
            "Epoch 4737 | Train Loss: 0.000207 | Validation Loss: 0.000273\n",
            "Epoch 4738 | Train Loss: 0.000190 | Validation Loss: 0.000262\n",
            "Epoch 4739 | Train Loss: 0.000185 | Validation Loss: 0.000259\n",
            "Epoch 4740 | Train Loss: 0.000193 | Validation Loss: 0.000281\n",
            "Epoch 4741 | Train Loss: 0.000198 | Validation Loss: 0.000272\n",
            "Epoch 4742 | Train Loss: 0.000197 | Validation Loss: 0.000261\n",
            "Epoch 4743 | Train Loss: 0.000192 | Validation Loss: 0.000266\n",
            "Epoch 4744 | Train Loss: 0.000190 | Validation Loss: 0.000277\n",
            "Epoch 4745 | Train Loss: 0.000187 | Validation Loss: 0.000273\n",
            "Epoch 4746 | Train Loss: 0.000206 | Validation Loss: 0.000262\n",
            "Epoch 4747 | Train Loss: 0.000199 | Validation Loss: 0.000277\n",
            "Epoch 4748 | Train Loss: 0.000192 | Validation Loss: 0.000275\n",
            "Epoch 4749 | Train Loss: 0.000190 | Validation Loss: 0.000257\n",
            "Epoch 4750 | Train Loss: 0.000202 | Validation Loss: 0.000283\n",
            "Epoch 4751 | Train Loss: 0.000208 | Validation Loss: 0.000285\n",
            "Epoch 4752 | Train Loss: 0.000194 | Validation Loss: 0.000274\n",
            "Epoch 4753 | Train Loss: 0.000190 | Validation Loss: 0.000258\n",
            "Epoch 4754 | Train Loss: 0.000189 | Validation Loss: 0.000260\n",
            "Epoch 4755 | Train Loss: 0.000197 | Validation Loss: 0.000272\n",
            "Epoch 4756 | Train Loss: 0.000199 | Validation Loss: 0.000267\n",
            "Epoch 4757 | Train Loss: 0.000195 | Validation Loss: 0.000296\n",
            "Epoch 4758 | Train Loss: 0.000197 | Validation Loss: 0.000266\n",
            "Epoch 4759 | Train Loss: 0.000201 | Validation Loss: 0.000257\n",
            "Epoch 4760 | Train Loss: 0.000187 | Validation Loss: 0.000254\n",
            "Epoch 4761 | Train Loss: 0.000194 | Validation Loss: 0.000270\n",
            "Epoch 4762 | Train Loss: 0.000194 | Validation Loss: 0.000261\n",
            "Epoch 4763 | Train Loss: 0.000198 | Validation Loss: 0.000272\n",
            "Epoch 4764 | Train Loss: 0.000190 | Validation Loss: 0.000277\n",
            "Epoch 4765 | Train Loss: 0.000190 | Validation Loss: 0.000275\n",
            "Epoch 4766 | Train Loss: 0.000193 | Validation Loss: 0.000260\n",
            "Epoch 4767 | Train Loss: 0.000200 | Validation Loss: 0.000282\n",
            "Epoch 4768 | Train Loss: 0.000191 | Validation Loss: 0.000270\n",
            "Epoch 4769 | Train Loss: 0.000189 | Validation Loss: 0.000263\n",
            "Epoch 4770 | Train Loss: 0.000195 | Validation Loss: 0.000269\n",
            "Epoch 4771 | Train Loss: 0.000186 | Validation Loss: 0.000266\n",
            "Epoch 4772 | Train Loss: 0.000198 | Validation Loss: 0.000260\n",
            "Epoch 4773 | Train Loss: 0.000195 | Validation Loss: 0.000284\n",
            "Epoch 4774 | Train Loss: 0.000191 | Validation Loss: 0.000263\n",
            "Epoch 4775 | Train Loss: 0.000205 | Validation Loss: 0.000280\n",
            "Epoch 4776 | Train Loss: 0.000188 | Validation Loss: 0.000286\n",
            "Epoch 4777 | Train Loss: 0.000201 | Validation Loss: 0.000259\n",
            "Epoch 4778 | Train Loss: 0.000206 | Validation Loss: 0.000268\n",
            "Epoch 4779 | Train Loss: 0.000208 | Validation Loss: 0.000279\n",
            "Epoch 4780 | Train Loss: 0.000208 | Validation Loss: 0.000267\n",
            "Epoch 4781 | Train Loss: 0.000194 | Validation Loss: 0.000275\n",
            "Epoch 4782 | Train Loss: 0.000184 | Validation Loss: 0.000265\n",
            "Epoch 4783 | Train Loss: 0.000198 | Validation Loss: 0.000264\n",
            "Epoch 4784 | Train Loss: 0.000200 | Validation Loss: 0.000266\n",
            "Epoch 4785 | Train Loss: 0.000202 | Validation Loss: 0.000264\n",
            "Epoch 4786 | Train Loss: 0.000186 | Validation Loss: 0.000258\n",
            "Epoch 4787 | Train Loss: 0.000185 | Validation Loss: 0.000257\n",
            "Epoch 4788 | Train Loss: 0.000182 | Validation Loss: 0.000259\n",
            "Epoch 4789 | Train Loss: 0.000203 | Validation Loss: 0.000253\n",
            "Epoch 4790 | Train Loss: 0.000189 | Validation Loss: 0.000266\n",
            "Epoch 4791 | Train Loss: 0.000186 | Validation Loss: 0.000270\n",
            "Epoch 4792 | Train Loss: 0.000195 | Validation Loss: 0.000274\n",
            "Epoch 4793 | Train Loss: 0.000188 | Validation Loss: 0.000279\n",
            "Epoch 4794 | Train Loss: 0.000198 | Validation Loss: 0.000268\n",
            "Epoch 4795 | Train Loss: 0.000195 | Validation Loss: 0.000265\n",
            "Epoch 4796 | Train Loss: 0.000185 | Validation Loss: 0.000256\n",
            "Epoch 4797 | Train Loss: 0.000183 | Validation Loss: 0.000288\n",
            "Epoch 4798 | Train Loss: 0.000198 | Validation Loss: 0.000277\n",
            "Epoch 4799 | Train Loss: 0.000201 | Validation Loss: 0.000263\n",
            "Epoch 4800 | Train Loss: 0.000202 | Validation Loss: 0.000273\n",
            "Epoch 4801 | Train Loss: 0.000197 | Validation Loss: 0.000271\n",
            "Epoch 4802 | Train Loss: 0.000187 | Validation Loss: 0.000269\n",
            "Epoch 4803 | Train Loss: 0.000189 | Validation Loss: 0.000263\n",
            "Epoch 4804 | Train Loss: 0.000193 | Validation Loss: 0.000264\n",
            "Epoch 4805 | Train Loss: 0.000207 | Validation Loss: 0.000256\n",
            "Epoch 4806 | Train Loss: 0.000195 | Validation Loss: 0.000268\n",
            "Epoch 4807 | Train Loss: 0.000185 | Validation Loss: 0.000253\n",
            "Epoch 4808 | Train Loss: 0.000188 | Validation Loss: 0.000271\n",
            "Epoch 4809 | Train Loss: 0.000186 | Validation Loss: 0.000274\n",
            "Epoch 4810 | Train Loss: 0.000189 | Validation Loss: 0.000252\n",
            "Epoch 4811 | Train Loss: 0.000182 | Validation Loss: 0.000253\n",
            "Epoch 4812 | Train Loss: 0.000182 | Validation Loss: 0.000265\n",
            "Epoch 4813 | Train Loss: 0.000185 | Validation Loss: 0.000273\n",
            "Epoch 4814 | Train Loss: 0.000189 | Validation Loss: 0.000260\n",
            "Epoch 4815 | Train Loss: 0.000190 | Validation Loss: 0.000257\n",
            "Epoch 4816 | Train Loss: 0.000191 | Validation Loss: 0.000259\n",
            "Epoch 4817 | Train Loss: 0.000188 | Validation Loss: 0.000260\n",
            "Epoch 4818 | Train Loss: 0.000181 | Validation Loss: 0.000257\n",
            "Epoch 4819 | Train Loss: 0.000192 | Validation Loss: 0.000273\n",
            "Epoch 4820 | Train Loss: 0.000183 | Validation Loss: 0.000254\n",
            "Epoch 4821 | Train Loss: 0.000187 | Validation Loss: 0.000263\n",
            "Epoch 4822 | Train Loss: 0.000191 | Validation Loss: 0.000260\n",
            "Epoch 4823 | Train Loss: 0.000188 | Validation Loss: 0.000263\n",
            "Epoch 4824 | Train Loss: 0.000205 | Validation Loss: 0.000271\n",
            "Epoch 4825 | Train Loss: 0.000208 | Validation Loss: 0.000289\n",
            "Epoch 4826 | Train Loss: 0.000190 | Validation Loss: 0.000259\n",
            "Epoch 4827 | Train Loss: 0.000186 | Validation Loss: 0.000254\n",
            "Epoch 4828 | Train Loss: 0.000184 | Validation Loss: 0.000273\n",
            "Epoch 4829 | Train Loss: 0.000193 | Validation Loss: 0.000272\n",
            "Epoch 4830 | Train Loss: 0.000209 | Validation Loss: 0.000285\n",
            "Epoch 4831 | Train Loss: 0.000200 | Validation Loss: 0.000269\n",
            "Epoch 4832 | Train Loss: 0.000194 | Validation Loss: 0.000263\n",
            "Epoch 4833 | Train Loss: 0.000190 | Validation Loss: 0.000266\n",
            "Epoch 4834 | Train Loss: 0.000193 | Validation Loss: 0.000262\n",
            "Epoch 4835 | Train Loss: 0.000188 | Validation Loss: 0.000271\n",
            "Epoch 4836 | Train Loss: 0.000179 | Validation Loss: 0.000251\n",
            "Epoch 4837 | Train Loss: 0.000183 | Validation Loss: 0.000263\n",
            "Epoch 4838 | Train Loss: 0.000198 | Validation Loss: 0.000278\n",
            "Epoch 4839 | Train Loss: 0.000193 | Validation Loss: 0.000264\n",
            "Epoch 4840 | Train Loss: 0.000192 | Validation Loss: 0.000258\n",
            "Epoch 4841 | Train Loss: 0.000185 | Validation Loss: 0.000254\n",
            "Epoch 4842 | Train Loss: 0.000199 | Validation Loss: 0.000262\n",
            "Epoch 4843 | Train Loss: 0.000188 | Validation Loss: 0.000250\n",
            "Epoch 4844 | Train Loss: 0.000192 | Validation Loss: 0.000257\n",
            "Epoch 4845 | Train Loss: 0.000193 | Validation Loss: 0.000267\n",
            "Epoch 4846 | Train Loss: 0.000195 | Validation Loss: 0.000254\n",
            "Epoch 4847 | Train Loss: 0.000201 | Validation Loss: 0.000266\n",
            "Epoch 4848 | Train Loss: 0.000204 | Validation Loss: 0.000251\n",
            "saved!\n",
            "Epoch 4849 | Train Loss: 0.000193 | Validation Loss: 0.000235\n",
            "Epoch 4850 | Train Loss: 0.000186 | Validation Loss: 0.000260\n",
            "Epoch 4851 | Train Loss: 0.000183 | Validation Loss: 0.000260\n",
            "Epoch 4852 | Train Loss: 0.000190 | Validation Loss: 0.000259\n",
            "Epoch 4853 | Train Loss: 0.000192 | Validation Loss: 0.000262\n",
            "Epoch 4854 | Train Loss: 0.000202 | Validation Loss: 0.000269\n",
            "Epoch 4855 | Train Loss: 0.000186 | Validation Loss: 0.000258\n",
            "Epoch 4856 | Train Loss: 0.000197 | Validation Loss: 0.000259\n",
            "Epoch 4857 | Train Loss: 0.000187 | Validation Loss: 0.000251\n",
            "Epoch 4858 | Train Loss: 0.000182 | Validation Loss: 0.000263\n",
            "Epoch 4859 | Train Loss: 0.000196 | Validation Loss: 0.000278\n",
            "Epoch 4860 | Train Loss: 0.000191 | Validation Loss: 0.000259\n",
            "Epoch 4861 | Train Loss: 0.000195 | Validation Loss: 0.000266\n",
            "Epoch 4862 | Train Loss: 0.000188 | Validation Loss: 0.000268\n",
            "Epoch 4863 | Train Loss: 0.000205 | Validation Loss: 0.000268\n",
            "Epoch 4864 | Train Loss: 0.000187 | Validation Loss: 0.000272\n",
            "Epoch 4865 | Train Loss: 0.000187 | Validation Loss: 0.000243\n",
            "Epoch 4866 | Train Loss: 0.000181 | Validation Loss: 0.000256\n",
            "Epoch 4867 | Train Loss: 0.000186 | Validation Loss: 0.000255\n",
            "Epoch 4868 | Train Loss: 0.000189 | Validation Loss: 0.000245\n",
            "Epoch 4869 | Train Loss: 0.000189 | Validation Loss: 0.000254\n",
            "Epoch 4870 | Train Loss: 0.000191 | Validation Loss: 0.000264\n",
            "Epoch 4871 | Train Loss: 0.000193 | Validation Loss: 0.000270\n",
            "Epoch 4872 | Train Loss: 0.000190 | Validation Loss: 0.000273\n",
            "Epoch 4873 | Train Loss: 0.000191 | Validation Loss: 0.000264\n",
            "Epoch 4874 | Train Loss: 0.000188 | Validation Loss: 0.000264\n",
            "Epoch 4875 | Train Loss: 0.000196 | Validation Loss: 0.000297\n",
            "Epoch 4876 | Train Loss: 0.000207 | Validation Loss: 0.000271\n",
            "Epoch 4877 | Train Loss: 0.000191 | Validation Loss: 0.000249\n",
            "Epoch 4878 | Train Loss: 0.000190 | Validation Loss: 0.000265\n",
            "Epoch 4879 | Train Loss: 0.000198 | Validation Loss: 0.000259\n",
            "Epoch 4880 | Train Loss: 0.000194 | Validation Loss: 0.000285\n",
            "Epoch 4881 | Train Loss: 0.000186 | Validation Loss: 0.000265\n",
            "Epoch 4882 | Train Loss: 0.000185 | Validation Loss: 0.000277\n",
            "Epoch 4883 | Train Loss: 0.000186 | Validation Loss: 0.000242\n",
            "Epoch 4884 | Train Loss: 0.000194 | Validation Loss: 0.000267\n",
            "Epoch 4885 | Train Loss: 0.000196 | Validation Loss: 0.000258\n",
            "Epoch 4886 | Train Loss: 0.000192 | Validation Loss: 0.000251\n",
            "Epoch 4887 | Train Loss: 0.000187 | Validation Loss: 0.000255\n",
            "Epoch 4888 | Train Loss: 0.000193 | Validation Loss: 0.000267\n",
            "Epoch 4889 | Train Loss: 0.000201 | Validation Loss: 0.000276\n",
            "Epoch 4890 | Train Loss: 0.000195 | Validation Loss: 0.000273\n",
            "Epoch 4891 | Train Loss: 0.000197 | Validation Loss: 0.000242\n",
            "Epoch 4892 | Train Loss: 0.000193 | Validation Loss: 0.000255\n",
            "Epoch 4893 | Train Loss: 0.000191 | Validation Loss: 0.000264\n",
            "Epoch 4894 | Train Loss: 0.000190 | Validation Loss: 0.000271\n",
            "Epoch 4895 | Train Loss: 0.000194 | Validation Loss: 0.000385\n",
            "Epoch 4896 | Train Loss: 0.000194 | Validation Loss: 0.000359\n",
            "Epoch 4897 | Train Loss: 0.000188 | Validation Loss: 0.000359\n",
            "Epoch 4898 | Train Loss: 0.000182 | Validation Loss: 0.000268\n",
            "Epoch 4899 | Train Loss: 0.000189 | Validation Loss: 0.000274\n",
            "Epoch 4900 | Train Loss: 0.000193 | Validation Loss: 0.000265\n",
            "Epoch 4901 | Train Loss: 0.000198 | Validation Loss: 0.000312\n",
            "Epoch 4902 | Train Loss: 0.000191 | Validation Loss: 0.000260\n",
            "Epoch 4903 | Train Loss: 0.000191 | Validation Loss: 0.000285\n",
            "Epoch 4904 | Train Loss: 0.000203 | Validation Loss: 0.000274\n",
            "Epoch 4905 | Train Loss: 0.000193 | Validation Loss: 0.000261\n",
            "Epoch 4906 | Train Loss: 0.000200 | Validation Loss: 0.000386\n",
            "Epoch 4907 | Train Loss: 0.000202 | Validation Loss: 0.000279\n",
            "Epoch 4908 | Train Loss: 0.000213 | Validation Loss: 0.000379\n",
            "Epoch 4909 | Train Loss: 0.000196 | Validation Loss: 0.000371\n",
            "Epoch 4910 | Train Loss: 0.000195 | Validation Loss: 0.000253\n",
            "Epoch 4911 | Train Loss: 0.000188 | Validation Loss: 0.000265\n",
            "Epoch 4912 | Train Loss: 0.000195 | Validation Loss: 0.000291\n",
            "Epoch 4913 | Train Loss: 0.000198 | Validation Loss: 0.000288\n",
            "Epoch 4914 | Train Loss: 0.000197 | Validation Loss: 0.000270\n",
            "Epoch 4915 | Train Loss: 0.000200 | Validation Loss: 0.000256\n",
            "Epoch 4916 | Train Loss: 0.000202 | Validation Loss: 0.000267\n",
            "Epoch 4917 | Train Loss: 0.000194 | Validation Loss: 0.000252\n",
            "Epoch 4918 | Train Loss: 0.000190 | Validation Loss: 0.000269\n",
            "Epoch 4919 | Train Loss: 0.000194 | Validation Loss: 0.000273\n",
            "Epoch 4920 | Train Loss: 0.000205 | Validation Loss: 0.000252\n",
            "Epoch 4921 | Train Loss: 0.000199 | Validation Loss: 0.000265\n",
            "Epoch 4922 | Train Loss: 0.000181 | Validation Loss: 0.000258\n",
            "Epoch 4923 | Train Loss: 0.000190 | Validation Loss: 0.000248\n",
            "Epoch 4924 | Train Loss: 0.000184 | Validation Loss: 0.000266\n",
            "Epoch 4925 | Train Loss: 0.000196 | Validation Loss: 0.000253\n",
            "Epoch 4926 | Train Loss: 0.000180 | Validation Loss: 0.000249\n",
            "Epoch 4927 | Train Loss: 0.000184 | Validation Loss: 0.000248\n",
            "Epoch 4928 | Train Loss: 0.000180 | Validation Loss: 0.000262\n",
            "Epoch 4929 | Train Loss: 0.000187 | Validation Loss: 0.000269\n",
            "Epoch 4930 | Train Loss: 0.000190 | Validation Loss: 0.000284\n",
            "Epoch 4931 | Train Loss: 0.000202 | Validation Loss: 0.000280\n",
            "Epoch 4932 | Train Loss: 0.000190 | Validation Loss: 0.000266\n",
            "Epoch 4933 | Train Loss: 0.000208 | Validation Loss: 0.000288\n",
            "Epoch 4934 | Train Loss: 0.000202 | Validation Loss: 0.000269\n",
            "Epoch 4935 | Train Loss: 0.000209 | Validation Loss: 0.000515\n",
            "Epoch 4936 | Train Loss: 0.000198 | Validation Loss: 0.000276\n",
            "Epoch 4937 | Train Loss: 0.000201 | Validation Loss: 0.000262\n",
            "Epoch 4938 | Train Loss: 0.000189 | Validation Loss: 0.000265\n",
            "Epoch 4939 | Train Loss: 0.000182 | Validation Loss: 0.000244\n",
            "Epoch 4940 | Train Loss: 0.000182 | Validation Loss: 0.000253\n",
            "Epoch 4941 | Train Loss: 0.000199 | Validation Loss: 0.000239\n",
            "Epoch 4942 | Train Loss: 0.000192 | Validation Loss: 0.000268\n",
            "Epoch 4943 | Train Loss: 0.000188 | Validation Loss: 0.000258\n",
            "Epoch 4944 | Train Loss: 0.000183 | Validation Loss: 0.000257\n",
            "Epoch 4945 | Train Loss: 0.000188 | Validation Loss: 0.000282\n",
            "Epoch 4946 | Train Loss: 0.000185 | Validation Loss: 0.000252\n",
            "Epoch 4947 | Train Loss: 0.000193 | Validation Loss: 0.000247\n",
            "Epoch 4948 | Train Loss: 0.000186 | Validation Loss: 0.000266\n",
            "Epoch 4949 | Train Loss: 0.000182 | Validation Loss: 0.000268\n",
            "Epoch 4950 | Train Loss: 0.000193 | Validation Loss: 0.000260\n",
            "Epoch 4951 | Train Loss: 0.000200 | Validation Loss: 0.000261\n",
            "Epoch 4952 | Train Loss: 0.000192 | Validation Loss: 0.000260\n",
            "Epoch 4953 | Train Loss: 0.000182 | Validation Loss: 0.000252\n",
            "Epoch 4954 | Train Loss: 0.000187 | Validation Loss: 0.000265\n",
            "Epoch 4955 | Train Loss: 0.000192 | Validation Loss: 0.000260\n",
            "Epoch 4956 | Train Loss: 0.000195 | Validation Loss: 0.000254\n",
            "Epoch 4957 | Train Loss: 0.000191 | Validation Loss: 0.000265\n",
            "Epoch 4958 | Train Loss: 0.000193 | Validation Loss: 0.000271\n",
            "Epoch 4959 | Train Loss: 0.000193 | Validation Loss: 0.000308\n",
            "Epoch 4960 | Train Loss: 0.000191 | Validation Loss: 0.000263\n",
            "Epoch 4961 | Train Loss: 0.000206 | Validation Loss: 0.000268\n",
            "Epoch 4962 | Train Loss: 0.000196 | Validation Loss: 0.000270\n",
            "Epoch 4963 | Train Loss: 0.000195 | Validation Loss: 0.000269\n",
            "Epoch 4964 | Train Loss: 0.000190 | Validation Loss: 0.000256\n",
            "Epoch 4965 | Train Loss: 0.000203 | Validation Loss: 0.000256\n",
            "Epoch 4966 | Train Loss: 0.000217 | Validation Loss: 0.000264\n",
            "Epoch 4967 | Train Loss: 0.000212 | Validation Loss: 0.000280\n",
            "Epoch 4968 | Train Loss: 0.000189 | Validation Loss: 0.000261\n",
            "Epoch 4969 | Train Loss: 0.000190 | Validation Loss: 0.000269\n",
            "Epoch 4970 | Train Loss: 0.000193 | Validation Loss: 0.000259\n",
            "Epoch 4971 | Train Loss: 0.000196 | Validation Loss: 0.000255\n",
            "Epoch 4972 | Train Loss: 0.000180 | Validation Loss: 0.000253\n",
            "Epoch 4973 | Train Loss: 0.000188 | Validation Loss: 0.000275\n",
            "Epoch 4974 | Train Loss: 0.000201 | Validation Loss: 0.000272\n",
            "Epoch 4975 | Train Loss: 0.000189 | Validation Loss: 0.000266\n",
            "Epoch 4976 | Train Loss: 0.000191 | Validation Loss: 0.000279\n",
            "Epoch 4977 | Train Loss: 0.000186 | Validation Loss: 0.000281\n",
            "Epoch 4978 | Train Loss: 0.000191 | Validation Loss: 0.000269\n",
            "Epoch 4979 | Train Loss: 0.000188 | Validation Loss: 0.000264\n",
            "Epoch 4980 | Train Loss: 0.000187 | Validation Loss: 0.000270\n",
            "Epoch 4981 | Train Loss: 0.000189 | Validation Loss: 0.000267\n",
            "Epoch 4982 | Train Loss: 0.000202 | Validation Loss: 0.000262\n",
            "Epoch 4983 | Train Loss: 0.000184 | Validation Loss: 0.000249\n",
            "Epoch 4984 | Train Loss: 0.000183 | Validation Loss: 0.000258\n",
            "Epoch 4985 | Train Loss: 0.000200 | Validation Loss: 0.000378\n",
            "Epoch 4986 | Train Loss: 0.000213 | Validation Loss: 0.000391\n",
            "Epoch 4987 | Train Loss: 0.000202 | Validation Loss: 0.000275\n",
            "Epoch 4988 | Train Loss: 0.000206 | Validation Loss: 0.000290\n",
            "Epoch 4989 | Train Loss: 0.000197 | Validation Loss: 0.000281\n",
            "Epoch 4990 | Train Loss: 0.000192 | Validation Loss: 0.000378\n",
            "Epoch 4991 | Train Loss: 0.000186 | Validation Loss: 0.000265\n",
            "Epoch 4992 | Train Loss: 0.000199 | Validation Loss: 0.000277\n",
            "Epoch 4993 | Train Loss: 0.000214 | Validation Loss: 0.000257\n",
            "Epoch 4994 | Train Loss: 0.000217 | Validation Loss: 0.000277\n",
            "Epoch 4995 | Train Loss: 0.000206 | Validation Loss: 0.000462\n",
            "Epoch 4996 | Train Loss: 0.000206 | Validation Loss: 0.000268\n",
            "Epoch 4997 | Train Loss: 0.000194 | Validation Loss: 0.000278\n",
            "Epoch 4998 | Train Loss: 0.000185 | Validation Loss: 0.000260\n",
            "Epoch 4999 | Train Loss: 0.000185 | Validation Loss: 0.000249\n",
            "Epoch 5000 | Train Loss: 0.000196 | Validation Loss: 0.000270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4288835061.py:130: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  images = [imageio.imread(f) for f in self.image_files]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF oluşturuldu: /content/Uniform-Autoencoder-with-Latent-Flow-Matching/results/spiral/UAE_Spiral_latent_evolution.gif\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "trainer = Trainer(model, optimizer, device='cuda' if torch.cuda.is_available() else 'cpu', max_patience=max_patience)\n",
        "trainer.train(train_loader, val_loader, epochs=epochs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "aa9b753a-ab61-49cc-9d9f-e9c495e4cf67",
      "metadata": {
        "id": "aa9b753a-ab61-49cc-9d9f-e9c495e4cf67"
      },
      "outputs": [],
      "source": [
        "# Save to CSV file\n",
        "train_losses = trainer.train_cost\n",
        "val_losses = trainer.val_cost\n",
        "\n",
        "np.savetxt(\"/content/Uniform-Autoencoder-with-Latent-Flow-Matching/results/spiral/losses.csv\",\n",
        "           np.column_stack((train_losses, val_losses)),\n",
        "           delimiter=\",\",\n",
        "           header=\"train_loss,val_loss\",\n",
        "           comments=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -r /content/latent_plots"
      ],
      "metadata": {
        "id": "eKi8xeQIvwgn"
      },
      "id": "eKi8xeQIvwgn",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkADDf_ywyll"
      },
      "id": "NkADDf_ywyll",
      "execution_count": 65,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}