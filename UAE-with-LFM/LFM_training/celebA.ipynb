{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayfununal/Uniform-Autoencoder-with-Latent-Flow-Matching/blob/main/UAE-with-LFM/LFM_training/celebA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dKviD48_8rhp"
      },
      "id": "dKviD48_8rhp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tayfununal/Uniform-Autoencoder-with-Latent-Flow-Matching.git"
      ],
      "metadata": {
        "id": "ep7ObMgB12ub"
      },
      "id": "ep7ObMgB12ub",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
      "metadata": {
        "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch import Tensor\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "%run /content/Uniform-Autoencoder-with-Latent-Flow-Matching/models/celebA_model.ipynb\n",
        "%run /content/Uniform-Autoencoder-with-Latent-Flow-Matching/datasets/celebA_dataset.ipynb\n",
        "\n",
        "%run /content/Uniform-Autoencoder-with-Latent-Flow-Matching/UAE-with-LFM/models/celebA_flow_model.ipynb\n",
        "\n",
        "plt.rcParams['font.size'] = 14\n",
        "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data from drive\n",
        "import zipfile\n",
        "\n",
        "def unzip_to_data(zip_path, out_dir='./data'):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(out_dir)\n",
        "    print(f\"Zip extracted: {out_dir}\")\n",
        "\n",
        "unzip_to_data('/content/drive/MyDrive/CelebA/list_attr_celeba.csv.zip')\n",
        "unzip_to_data('/content/drive/MyDrive/CelebA/img_align_celeba.zip')\n",
        "unzip_to_data('/content/drive/MyDrive/CelebA/list_eval_partition.csv.zip')"
      ],
      "metadata": {
        "id": "uleSJpq_8_Rn"
      },
      "id": "uleSJpq_8_Rn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f",
      "metadata": {
        "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f"
      },
      "outputs": [],
      "source": [
        "# Hyper-Parameters & Settings\n",
        "batch_size = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07121ef5-db8d-4bd5-a327-a8646f696809",
      "metadata": {
        "id": "07121ef5-db8d-4bd5-a327-a8646f696809"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "root_dir = './data/img_align_celeba'\n",
        "\n",
        "train_dataset = CelebADataset(img_dir=root_dir, attr_path='./data/list_attr_celeba.csv', partition_path='./data/list_eval_partition.csv' , mode='train')\n",
        "val_dataset   = CelebADataset(img_dir=root_dir, attr_path='./data/list_attr_celeba.csv', partition_path='./data/list_eval_partition.csv', mode='val')\n",
        "test_dataset  = CelebADataset(img_dir=root_dir, attr_path='./data/list_attr_celeba.csv', partition_path='./data/list_eval_partition.csv', mode='test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5796a6ef-999d-4f94-92bd-7bab72392f53",
      "metadata": {
        "id": "5796a6ef-999d-4f94-92bd-7bab72392f53",
        "outputId": "03040355-dc16-4791-ed7e-c8e3b277c1d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "To_Uniform(\n",
              "  (encoder): SimpleEncoder(\n",
              "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "      (1): SiLU()\n",
              "      (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "      (4): Sigmoid()\n",
              "    )\n",
              "  )\n",
              "  (decoder): SimpleDecoder(\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "      (1): SiLU()\n",
              "      (2): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "    )\n",
              "    (deconv1): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (deconv2): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (deconv3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (deconv4): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (final_conv): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Load the UAE model\n",
        "\n",
        "path = '/content/results/UAE_CelebA'\n",
        "model = torch.load(path + '.model', weights_only=False, map_location='cpu')\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a3ed17-bf58-4cad-9955-56b57f3c4696",
      "metadata": {
        "id": "64a3ed17-bf58-4cad-9955-56b57f3c4696"
      },
      "outputs": [],
      "source": [
        "# Create the \"results\" folder\n",
        "os.makedirs(\"/content/results\", exist_ok=True)\n",
        "\n",
        "flow_name = './results/UAE_Latent_FM'\n",
        "flow = Flow().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(flow.parameters(), 0.0003)\n",
        "loss_fn = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7496296a-6c85-41e4-8be4-5fb318216d08",
      "metadata": {
        "id": "7496296a-6c85-41e4-8be4-5fb318216d08"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# --- 1. Encode the data ---\n",
        "def get_encoded_loader(data_loader, encoder, device='cpu'):\n",
        "    encoder.eval()\n",
        "    encoded = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x in data_loader:\n",
        "            x = x.to(device)\n",
        "            z = encoder(x)\n",
        "            encoded.append(z.cpu())\n",
        "\n",
        "    all_z = torch.cat(encoded, dim=0)\n",
        "    if data_loader == train_loader:\n",
        "      return torch.utils.data.DataLoader(all_z, batch_size=data_loader.batch_size, shuffle=True)\n",
        "    elif data_loader == val_loader:\n",
        "      return torch.utils.data.DataLoader(all_z, batch_size=data_loader.batch_size, shuffle=False)\n",
        "\n",
        "# --- 2. Create a loader with the encoded data ---\n",
        "encoded_train_loader = get_encoded_loader(train_loader, model.encoder, device=device)\n",
        "encoded_val_loader = get_encoded_loader(val_loader, model.encoder, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f799e7d-6d3a-4717-babb-a3b58e5ea575",
      "metadata": {
        "scrolled": true,
        "id": "8f799e7d-6d3a-4717-babb-a3b58e5ea575"
      },
      "outputs": [],
      "source": [
        "# --- 3. Flow Matching training ---\n",
        "train_cost = []\n",
        "val_cost = []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(1000):\n",
        "    epoch_losses = []\n",
        "\n",
        "    flow.train()\n",
        "    for z1 in encoded_train_loader:\n",
        "\n",
        "        with torch.no_grad():\n",
        "          z1 = z1.to(device)\n",
        "        z0 = torch.rand_like(z1)\n",
        "        t = torch.rand(len(z1), 1, device=device)\n",
        "\n",
        "        zt = (1 - t) * z0 + t * z1\n",
        "        dzt = z1 - z0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = flow(t=t, x_t=zt)\n",
        "        loss = loss_fn(pred, dzt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "    mean_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    train_cost.append(mean_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    flow.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for z1_val in encoded_val_loader:\n",
        "            z1_val = z1_val.to(device)\n",
        "            z0_val = torch.rand_like(z1_val)\n",
        "            t_val = torch.rand(len(z1_val), 1, device=device)\n",
        "\n",
        "            zt_val = (1 - t_val) * z0_val + t_val * z1_val\n",
        "            dzt_val = z1_val - z0_val\n",
        "\n",
        "            pred_val = flow(t=t_val, x_t=zt_val)\n",
        "            val_loss = loss_fn(pred_val, dzt_val)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "    val_cost.append(mean_val_loss)\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if mean_val_loss < best_val_loss:\n",
        "        print(\"Saved!\")\n",
        "        best_val_loss = mean_val_loss\n",
        "        torch.save(flow, flow_name + '.model')\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/1000 - Train Loss: {mean_train_loss:.4f} - Val Loss: {mean_val_loss:.4f}\")\n",
        "\n",
        "# Optionally, save the final epoch model separately\n",
        "torch.save(flow, flow_name + '_final.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59421793-1d36-443a-8b85-ba90b60402df",
      "metadata": {
        "id": "59421793-1d36-443a-8b85-ba90b60402df"
      },
      "outputs": [],
      "source": [
        "# Save to CSV file\n",
        "train_losses = train_cost\n",
        "val_losses = val_cost\n",
        "\n",
        "np.savetxt(\"results/losses.csv\",\n",
        "           np.column_stack((train_losses, val_losses)),\n",
        "           delimiter=\",\",\n",
        "           header=\"train_loss,val_loss\",\n",
        "           comments=\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}