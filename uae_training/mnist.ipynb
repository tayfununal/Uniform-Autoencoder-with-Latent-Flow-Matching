{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/arf/home/tunal/ondemand/PhD Thesis Starting/01_SON/Tik-4/Tez/05-MNIST/02-UAE_for_MNIST\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd()) # dosya yolunu ver\n",
    "%run ../Model.ipynb\n",
    "%run ../Dataset.ipynb\n",
    "\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7601ea5-682f-44a9-9b06-0f0efd697f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device='cpu', max_patience=20):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "        self.max_patience = max_patience\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience = 0\n",
    "\n",
    "        self.val_cost = []\n",
    "        self.train_cost = []\n",
    "    \n",
    "    def train(self, train_loader, val_loader=None, epochs=10, print_every=1, name=None):\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for x, x_, y in train_loader:  # label kullanılmıyor\n",
    "                x = x.to(self.device)\n",
    "                x_ = x_.to(self.device)\n",
    "                \n",
    "                z_hat_, x_hat_ = self.model(x_)\n",
    "                z_hat, x_hat = self.model(x)\n",
    "                \n",
    "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,784))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            if val_loader:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                \n",
    "                # Early stopping kontrolü\n",
    "                if val_loss < self.best_val_loss:\n",
    "                    print('saved!')\n",
    "                    torch.save(self.model, name + '.model')\n",
    "                    self.best_val_loss = val_loss\n",
    "                    self.patience = 0\n",
    "        \n",
    "                else:\n",
    "                    self.patience = self.patience + 1\n",
    "        \n",
    "                if self.patience > self.max_patience:\n",
    "                    break\n",
    "                    \n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch:3d} | Train Loss: {total_loss / len(train_loader):.6f} | Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "            self.val_cost.append(val_loss)\n",
    "            self.train_cost.append(total_loss / len(train_loader))\n",
    "            \n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, x_, _ in val_loader:\n",
    "                x = x.to(self.device)\n",
    "                x_ = x_.to(self.device)\n",
    "                \n",
    "                z_hat_, x_hat_ = self.model(x_)\n",
    "                z_hat, x_hat = self.model(x)\n",
    "                \n",
    "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,784))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        #print(f\"→ Validation Loss: {avg_loss:.6f}\")\n",
    "        self.model.train()\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6982ede7-a39e-433c-90a9-8850b1f73c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transform\n",
    "class NoiseTransform:\n",
    "    \"\"\"Add some noise.\"\"\"\n",
    "\n",
    "    def __init__(self, split_ratio=0.001, dim=784):\n",
    "\n",
    "        self.normal_dist = split_ratio*np.random.randn(dim,)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "      return x + self.normal_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters & Settings\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 0.001\n",
    "\n",
    "epochs = 200\n",
    "max_patience = 200\n",
    "\n",
    "split_ratio = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07121ef5-db8d-4bd5-a327-a8646f696809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = MNISTDataset(mode='train', transform=NoiseTransform(split_ratio))\n",
    "val_dataset = MNISTDataset(mode='val', transform=NoiseTransform(split_ratio))\n",
    "test_dataset = MNISTDataset(mode='test')\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaf4dc1c-c005-4a16-879b-d29531e4c7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_list = []\\ny_list = []\\n\\nfor x, _, y in train_dataset:\\n    X_list.append(x)\\n    y_list.append(y)\\n\\nX = torch.stack(X_list)\\ny = torch.tensor(y_list)\\n\\nprint('9 dan',X[y==9].shape[0], 'sayıda var')\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for x, _, y in train_dataset:\n",
    "    X_list.append(x)\n",
    "    y_list.append(y)\n",
    "\n",
    "X = torch.stack(X_list)\n",
    "y = torch.tensor(y_list)\n",
    "\n",
    "print('9 dan',X[y==9].shape[0], 'sayıda var')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9abf79c-a874-418b-b46f-aaa2fe649d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"results\" klasörünü oluştur (zaten varsa hata vermez)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Model\n",
    "name = 'results/UAE_MNIST'\n",
    "model = To_Uniform(\n",
    "                 encoder_layers=[784, 2000, 2000, 2000, 2000, 3],\n",
    "                 decoder_layers=[3, 2000, 2000, 2000, 2000, 784],\n",
    "                 encoder_act=nn.SiLU,\n",
    "                 decoder_act=nn.SiLU,\n",
    "                 final_encoder_act=nn.Sigmoid,\n",
    "                 final_decoder_act=nn.Sigmoid,\n",
    "                 use_batchnorm=True\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n",
      "Epoch   1 | Train Loss: 0.069952 | Validation Loss: 0.055738\n",
      "saved!\n",
      "Epoch   2 | Train Loss: 0.052645 | Validation Loss: 0.048323\n",
      "saved!\n",
      "Epoch   3 | Train Loss: 0.045943 | Validation Loss: 0.042402\n",
      "saved!\n",
      "Epoch   4 | Train Loss: 0.041840 | Validation Loss: 0.040205\n",
      "saved!\n",
      "Epoch   5 | Train Loss: 0.039141 | Validation Loss: 0.037783\n",
      "saved!\n",
      "Epoch   6 | Train Loss: 0.036669 | Validation Loss: 0.036122\n",
      "saved!\n",
      "Epoch   7 | Train Loss: 0.035486 | Validation Loss: 0.034558\n",
      "saved!\n",
      "Epoch   8 | Train Loss: 0.034210 | Validation Loss: 0.033657\n",
      "saved!\n",
      "Epoch   9 | Train Loss: 0.033925 | Validation Loss: 0.033628\n",
      "saved!\n",
      "Epoch  10 | Train Loss: 0.033241 | Validation Loss: 0.032158\n",
      "saved!\n",
      "Epoch  11 | Train Loss: 0.031846 | Validation Loss: 0.031777\n",
      "Epoch  12 | Train Loss: 0.031723 | Validation Loss: 0.032243\n",
      "saved!\n",
      "Epoch  13 | Train Loss: 0.031455 | Validation Loss: 0.031178\n",
      "saved!\n",
      "Epoch  14 | Train Loss: 0.031727 | Validation Loss: 0.030355\n",
      "saved!\n",
      "Epoch  15 | Train Loss: 0.030450 | Validation Loss: 0.030222\n",
      "saved!\n",
      "Epoch  16 | Train Loss: 0.029824 | Validation Loss: 0.029852\n",
      "saved!\n",
      "Epoch  17 | Train Loss: 0.030109 | Validation Loss: 0.029636\n",
      "Epoch  18 | Train Loss: 0.029824 | Validation Loss: 0.030283\n",
      "Epoch  19 | Train Loss: 0.030208 | Validation Loss: 0.029906\n",
      "Epoch  20 | Train Loss: 0.029323 | Validation Loss: 0.029869\n",
      "saved!\n",
      "Epoch  21 | Train Loss: 0.030352 | Validation Loss: 0.029122\n",
      "saved!\n",
      "Epoch  22 | Train Loss: 0.029094 | Validation Loss: 0.028978\n",
      "Epoch  23 | Train Loss: 0.029677 | Validation Loss: 0.029710\n",
      "saved!\n",
      "Epoch  24 | Train Loss: 0.028590 | Validation Loss: 0.028455\n",
      "Epoch  25 | Train Loss: 0.028395 | Validation Loss: 0.028642\n",
      "Epoch  26 | Train Loss: 0.028154 | Validation Loss: 0.028877\n",
      "Epoch  27 | Train Loss: 0.028378 | Validation Loss: 0.029774\n",
      "saved!\n",
      "Epoch  28 | Train Loss: 0.028173 | Validation Loss: 0.028435\n",
      "saved!\n",
      "Epoch  29 | Train Loss: 0.027753 | Validation Loss: 0.028217\n",
      "Epoch  30 | Train Loss: 0.028250 | Validation Loss: 0.028275\n",
      "saved!\n",
      "Epoch  31 | Train Loss: 0.027826 | Validation Loss: 0.028162\n",
      "Epoch  32 | Train Loss: 0.028244 | Validation Loss: 0.028709\n",
      "saved!\n",
      "Epoch  33 | Train Loss: 0.028128 | Validation Loss: 0.027899\n",
      "Epoch  34 | Train Loss: 0.027345 | Validation Loss: 0.028746\n",
      "Epoch  35 | Train Loss: 0.027818 | Validation Loss: 0.028118\n",
      "Epoch  36 | Train Loss: 0.027573 | Validation Loss: 0.028272\n",
      "Epoch  37 | Train Loss: 0.027735 | Validation Loss: 0.027952\n",
      "saved!\n",
      "Epoch  38 | Train Loss: 0.027372 | Validation Loss: 0.027699\n",
      "Epoch  39 | Train Loss: 0.027127 | Validation Loss: 0.027950\n",
      "Epoch  40 | Train Loss: 0.027127 | Validation Loss: 0.029000\n",
      "Epoch  41 | Train Loss: 0.026711 | Validation Loss: 0.027769\n",
      "saved!\n",
      "Epoch  42 | Train Loss: 0.026874 | Validation Loss: 0.027388\n",
      "Epoch  43 | Train Loss: 0.027820 | Validation Loss: 0.028092\n",
      "Epoch  44 | Train Loss: 0.027005 | Validation Loss: 0.027549\n",
      "saved!\n",
      "Epoch  45 | Train Loss: 0.026648 | Validation Loss: 0.027366\n",
      "saved!\n",
      "Epoch  46 | Train Loss: 0.026915 | Validation Loss: 0.027362\n",
      "Epoch  47 | Train Loss: 0.026427 | Validation Loss: 0.027614\n",
      "Epoch  48 | Train Loss: 0.026403 | Validation Loss: 0.027510\n",
      "saved!\n",
      "Epoch  49 | Train Loss: 0.026109 | Validation Loss: 0.027331\n",
      "saved!\n",
      "Epoch  50 | Train Loss: 0.026253 | Validation Loss: 0.027059\n",
      "Epoch  51 | Train Loss: 0.026555 | Validation Loss: 0.028021\n",
      "Epoch  52 | Train Loss: 0.025831 | Validation Loss: 0.027294\n",
      "saved!\n",
      "Epoch  53 | Train Loss: 0.026311 | Validation Loss: 0.027011\n",
      "Epoch  54 | Train Loss: 0.025829 | Validation Loss: 0.027858\n",
      "saved!\n",
      "Epoch  55 | Train Loss: 0.026026 | Validation Loss: 0.026936\n",
      "Epoch  56 | Train Loss: 0.026874 | Validation Loss: 0.029299\n",
      "Epoch  57 | Train Loss: 0.028215 | Validation Loss: 0.027832\n",
      "Epoch  58 | Train Loss: 0.026587 | Validation Loss: 0.027029\n",
      "Epoch  59 | Train Loss: 0.026012 | Validation Loss: 0.027327\n",
      "saved!\n",
      "Epoch  60 | Train Loss: 0.026049 | Validation Loss: 0.026869\n",
      "saved!\n",
      "Epoch  61 | Train Loss: 0.025637 | Validation Loss: 0.026840\n",
      "Epoch  62 | Train Loss: 0.025435 | Validation Loss: 0.027013\n",
      "Epoch  63 | Train Loss: 0.025729 | Validation Loss: 0.027721\n",
      "saved!\n",
      "Epoch  64 | Train Loss: 0.025664 | Validation Loss: 0.026735\n",
      "Epoch  65 | Train Loss: 0.025558 | Validation Loss: 0.027727\n",
      "Epoch  66 | Train Loss: 0.025564 | Validation Loss: 0.026737\n",
      "Epoch  67 | Train Loss: 0.025975 | Validation Loss: 0.027810\n",
      "saved!\n",
      "Epoch  68 | Train Loss: 0.025127 | Validation Loss: 0.026553\n",
      "Epoch  69 | Train Loss: 0.025034 | Validation Loss: 0.027221\n",
      "Epoch  70 | Train Loss: 0.025175 | Validation Loss: 0.027095\n",
      "Epoch  71 | Train Loss: 0.025286 | Validation Loss: 0.026950\n",
      "Epoch  72 | Train Loss: 0.024908 | Validation Loss: 0.026557\n",
      "Epoch  73 | Train Loss: 0.025259 | Validation Loss: 0.027223\n",
      "Epoch  74 | Train Loss: 0.025611 | Validation Loss: 0.026919\n",
      "Epoch  75 | Train Loss: 0.025290 | Validation Loss: 0.027273\n",
      "saved!\n",
      "Epoch  76 | Train Loss: 0.024914 | Validation Loss: 0.026535\n",
      "Epoch  77 | Train Loss: 0.025085 | Validation Loss: 0.027784\n",
      "Epoch  78 | Train Loss: 0.025288 | Validation Loss: 0.026795\n",
      "Epoch  79 | Train Loss: 0.025308 | Validation Loss: 0.026945\n",
      "Epoch  80 | Train Loss: 0.025109 | Validation Loss: 0.027159\n",
      "Epoch  81 | Train Loss: 0.024844 | Validation Loss: 0.027367\n",
      "saved!\n",
      "Epoch  82 | Train Loss: 0.024912 | Validation Loss: 0.026482\n",
      "Epoch  83 | Train Loss: 0.024796 | Validation Loss: 0.026952\n",
      "Epoch  84 | Train Loss: 0.024863 | Validation Loss: 0.027114\n",
      "saved!\n",
      "Epoch  85 | Train Loss: 0.024684 | Validation Loss: 0.026374\n",
      "saved!\n",
      "Epoch  86 | Train Loss: 0.024554 | Validation Loss: 0.026104\n",
      "Epoch  87 | Train Loss: 0.024396 | Validation Loss: 0.026507\n",
      "Epoch  88 | Train Loss: 0.024719 | Validation Loss: 0.027608\n",
      "Epoch  89 | Train Loss: 0.024393 | Validation Loss: 0.026426\n",
      "Epoch  90 | Train Loss: 0.024775 | Validation Loss: 0.026908\n",
      "Epoch  91 | Train Loss: 0.024847 | Validation Loss: 0.026603\n",
      "Epoch  92 | Train Loss: 0.024546 | Validation Loss: 0.026395\n",
      "Epoch  93 | Train Loss: 0.024334 | Validation Loss: 0.026189\n",
      "Epoch  94 | Train Loss: 0.023630 | Validation Loss: 0.026238\n",
      "Epoch  95 | Train Loss: 0.024113 | Validation Loss: 0.026333\n",
      "Epoch  96 | Train Loss: 0.023954 | Validation Loss: 0.026702\n",
      "Epoch  97 | Train Loss: 0.024337 | Validation Loss: 0.027199\n",
      "Epoch  98 | Train Loss: 0.024568 | Validation Loss: 0.026904\n",
      "Epoch  99 | Train Loss: 0.024365 | Validation Loss: 0.026558\n",
      "Epoch 100 | Train Loss: 0.024457 | Validation Loss: 0.026717\n",
      "Epoch 101 | Train Loss: 0.024878 | Validation Loss: 0.027023\n",
      "Epoch 102 | Train Loss: 0.024154 | Validation Loss: 0.026575\n",
      "Epoch 103 | Train Loss: 0.023871 | Validation Loss: 0.026371\n",
      "Epoch 104 | Train Loss: 0.024098 | Validation Loss: 0.026546\n",
      "Epoch 105 | Train Loss: 0.024025 | Validation Loss: 0.026511\n",
      "Epoch 106 | Train Loss: 0.024021 | Validation Loss: 0.027311\n",
      "Epoch 107 | Train Loss: 0.024401 | Validation Loss: 0.027215\n",
      "Epoch 108 | Train Loss: 0.024467 | Validation Loss: 0.026547\n",
      "Epoch 109 | Train Loss: 0.023938 | Validation Loss: 0.026819\n",
      "Epoch 110 | Train Loss: 0.024191 | Validation Loss: 0.026775\n",
      "Epoch 111 | Train Loss: 0.023818 | Validation Loss: 0.026937\n",
      "Epoch 112 | Train Loss: 0.023960 | Validation Loss: 0.027164\n",
      "Epoch 113 | Train Loss: 0.023450 | Validation Loss: 0.026528\n",
      "Epoch 114 | Train Loss: 0.024043 | Validation Loss: 0.027115\n",
      "Epoch 115 | Train Loss: 0.023724 | Validation Loss: 0.026635\n",
      "Epoch 116 | Train Loss: 0.023591 | Validation Loss: 0.026465\n",
      "Epoch 117 | Train Loss: 0.023399 | Validation Loss: 0.026770\n",
      "Epoch 118 | Train Loss: 0.023727 | Validation Loss: 0.026926\n",
      "Epoch 119 | Train Loss: 0.023908 | Validation Loss: 0.026823\n",
      "Epoch 120 | Train Loss: 0.023731 | Validation Loss: 0.026610\n",
      "Epoch 121 | Train Loss: 0.023143 | Validation Loss: 0.026462\n",
      "Epoch 122 | Train Loss: 0.023432 | Validation Loss: 0.026820\n",
      "Epoch 123 | Train Loss: 0.023927 | Validation Loss: 0.026968\n",
      "Epoch 124 | Train Loss: 0.025473 | Validation Loss: 0.027632\n",
      "Epoch 125 | Train Loss: 0.024395 | Validation Loss: 0.026706\n",
      "Epoch 126 | Train Loss: 0.024082 | Validation Loss: 0.026907\n",
      "Epoch 127 | Train Loss: 0.024055 | Validation Loss: 0.026683\n",
      "Epoch 128 | Train Loss: 0.024077 | Validation Loss: 0.026852\n",
      "Epoch 129 | Train Loss: 0.023727 | Validation Loss: 0.026704\n",
      "Epoch 130 | Train Loss: 0.023875 | Validation Loss: 0.026724\n",
      "Epoch 131 | Train Loss: 0.023263 | Validation Loss: 0.026668\n",
      "Epoch 132 | Train Loss: 0.023735 | Validation Loss: 0.027095\n",
      "Epoch 133 | Train Loss: 0.023547 | Validation Loss: 0.027218\n",
      "Epoch 134 | Train Loss: 0.024105 | Validation Loss: 0.026720\n",
      "Epoch 135 | Train Loss: 0.024265 | Validation Loss: 0.026964\n",
      "Epoch 136 | Train Loss: 0.023816 | Validation Loss: 0.027162\n",
      "Epoch 137 | Train Loss: 0.024550 | Validation Loss: 0.027122\n",
      "Epoch 138 | Train Loss: 0.023680 | Validation Loss: 0.026980\n",
      "Epoch 139 | Train Loss: 0.023421 | Validation Loss: 0.026978\n",
      "Epoch 140 | Train Loss: 0.023628 | Validation Loss: 0.026541\n",
      "Epoch 141 | Train Loss: 0.023964 | Validation Loss: 0.026576\n",
      "Epoch 142 | Train Loss: 0.024491 | Validation Loss: 0.027402\n",
      "Epoch 143 | Train Loss: 0.024200 | Validation Loss: 0.026812\n",
      "Epoch 144 | Train Loss: 0.023944 | Validation Loss: 0.026985\n",
      "Epoch 145 | Train Loss: 0.023246 | Validation Loss: 0.026484\n",
      "Epoch 146 | Train Loss: 0.023169 | Validation Loss: 0.027122\n",
      "Epoch 147 | Train Loss: 0.023011 | Validation Loss: 0.026342\n",
      "Epoch 148 | Train Loss: 0.023376 | Validation Loss: 0.027168\n",
      "Epoch 149 | Train Loss: 0.023301 | Validation Loss: 0.027213\n",
      "Epoch 150 | Train Loss: 0.023364 | Validation Loss: 0.026593\n",
      "Epoch 151 | Train Loss: 0.023279 | Validation Loss: 0.026761\n",
      "Epoch 152 | Train Loss: 0.022570 | Validation Loss: 0.026622\n",
      "Epoch 153 | Train Loss: 0.022915 | Validation Loss: 0.027009\n",
      "Epoch 154 | Train Loss: 0.022854 | Validation Loss: 0.026685\n",
      "Epoch 155 | Train Loss: 0.022754 | Validation Loss: 0.026949\n",
      "Epoch 156 | Train Loss: 0.022704 | Validation Loss: 0.026827\n",
      "Epoch 157 | Train Loss: 0.022549 | Validation Loss: 0.026679\n",
      "Epoch 158 | Train Loss: 0.023181 | Validation Loss: 0.027371\n",
      "Epoch 159 | Train Loss: 0.022681 | Validation Loss: 0.026408\n",
      "Epoch 160 | Train Loss: 0.022720 | Validation Loss: 0.026878\n",
      "Epoch 161 | Train Loss: 0.022785 | Validation Loss: 0.026880\n",
      "Epoch 162 | Train Loss: 0.022180 | Validation Loss: 0.026866\n",
      "Epoch 163 | Train Loss: 0.022795 | Validation Loss: 0.026811\n",
      "Epoch 164 | Train Loss: 0.022598 | Validation Loss: 0.026621\n",
      "Epoch 165 | Train Loss: 0.023028 | Validation Loss: 0.027496\n",
      "Epoch 166 | Train Loss: 0.022813 | Validation Loss: 0.026871\n",
      "Epoch 167 | Train Loss: 0.022075 | Validation Loss: 0.026834\n",
      "Epoch 168 | Train Loss: 0.022292 | Validation Loss: 0.026854\n",
      "Epoch 169 | Train Loss: 0.022781 | Validation Loss: 0.027431\n",
      "Epoch 170 | Train Loss: 0.024988 | Validation Loss: 0.028251\n",
      "Epoch 171 | Train Loss: 0.023485 | Validation Loss: 0.027605\n",
      "Epoch 172 | Train Loss: 0.023126 | Validation Loss: 0.026581\n",
      "Epoch 173 | Train Loss: 0.023060 | Validation Loss: 0.026629\n",
      "Epoch 174 | Train Loss: 0.024407 | Validation Loss: 0.027191\n",
      "Epoch 175 | Train Loss: 0.023407 | Validation Loss: 0.027085\n",
      "Epoch 176 | Train Loss: 0.022830 | Validation Loss: 0.026650\n",
      "Epoch 177 | Train Loss: 0.022607 | Validation Loss: 0.026788\n",
      "Epoch 178 | Train Loss: 0.023030 | Validation Loss: 0.027233\n",
      "Epoch 179 | Train Loss: 0.023439 | Validation Loss: 0.027396\n",
      "Epoch 180 | Train Loss: 0.022844 | Validation Loss: 0.026866\n",
      "Epoch 181 | Train Loss: 0.023675 | Validation Loss: 0.026909\n",
      "Epoch 182 | Train Loss: 0.022763 | Validation Loss: 0.026818\n",
      "Epoch 183 | Train Loss: 0.022795 | Validation Loss: 0.026965\n",
      "Epoch 184 | Train Loss: 0.023132 | Validation Loss: 0.027130\n",
      "Epoch 185 | Train Loss: 0.022600 | Validation Loss: 0.026928\n",
      "Epoch 186 | Train Loss: 0.022586 | Validation Loss: 0.027030\n",
      "Epoch 187 | Train Loss: 0.022721 | Validation Loss: 0.026855\n",
      "Epoch 188 | Train Loss: 0.022883 | Validation Loss: 0.027514\n",
      "Epoch 189 | Train Loss: 0.022770 | Validation Loss: 0.026868\n",
      "Epoch 190 | Train Loss: 0.022640 | Validation Loss: 0.027080\n",
      "Epoch 191 | Train Loss: 0.022670 | Validation Loss: 0.027485\n",
      "Epoch 192 | Train Loss: 0.023095 | Validation Loss: 0.027185\n",
      "Epoch 193 | Train Loss: 0.022282 | Validation Loss: 0.026894\n",
      "Epoch 194 | Train Loss: 0.022628 | Validation Loss: 0.027127\n",
      "Epoch 195 | Train Loss: 0.022275 | Validation Loss: 0.026767\n",
      "Epoch 196 | Train Loss: 0.021926 | Validation Loss: 0.027155\n",
      "Epoch 197 | Train Loss: 0.022958 | Validation Loss: 0.028019\n",
      "Epoch 198 | Train Loss: 0.022854 | Validation Loss: 0.026988\n",
      "Epoch 199 | Train Loss: 0.022349 | Validation Loss: 0.027086\n",
      "Epoch 200 | Train Loss: 0.022054 | Validation Loss: 0.026832\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer = Trainer(model, optimizer, device='cuda' if torch.cuda.is_available() else 'cpu', max_patience=max_patience)\n",
    "trainer.train(train_loader, val_loader, epochs=epochs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "999614d8-ad82-459f-8881-b9bf5fbda6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dosyasına kaydet\n",
    "train_losses = trainer.train_cost  # liste veya numpy array\n",
    "val_losses = trainer.val_cost\n",
    "\n",
    "np.savetxt(\"results/losses.csv\", \n",
    "           np.column_stack((train_losses, val_losses)), \n",
    "           delimiter=\",\", \n",
    "           header=\"train_loss,val_loss\", \n",
    "           comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269282a-16dc-406a-ae83-935ad085f826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
