{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/arf/home/tunal/ondemand/PhD Thesis Starting/01_SON/Tik-4/Tez/03-Spiral/02-UAE_for_Spiral-Copy1-Copy1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd()) # dosya yolunu ver\n",
    "%run ./Model.ipynb\n",
    "%run ../Dataset.ipynb\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7601ea5-682f-44a9-9b06-0f0efd697f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device='cpu', max_patience=20):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "        self.max_patience = max_patience\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience = 0\n",
    "\n",
    "        self.val_cost = []\n",
    "        self.train_cost = []\n",
    "    \n",
    "    def train(self, train_loader, val_loader=None, epochs=10, print_every=1, name=None):\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for x, x_, y in train_loader:  # label kullanılmıyor\n",
    "                x = x.to(self.device)\n",
    "                x_ = x_.to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    z_hat_ = self.model.encoder(x_)\n",
    "                z_hat, x_hat = self.model(x)\n",
    "                \n",
    "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,3))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Validation (opsiyonel)\n",
    "            if val_loader:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                \n",
    "                # Early stopping kontrolü\n",
    "                if val_loss < self.best_val_loss:\n",
    "                    print('saved!')\n",
    "                    torch.save(self.model, name + '.model')\n",
    "                    self.best_val_loss = val_loss\n",
    "                    self.patience = 0\n",
    "        \n",
    "                else:\n",
    "                    self.patience = self.patience + 1\n",
    "        \n",
    "                if self.patience > self.max_patience:\n",
    "                    break\n",
    "                    \n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch:3d} | Train Loss: {total_loss / len(train_loader):.6f} | Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "            self.val_cost.append(val_loss)\n",
    "            self.train_cost.append(total_loss / len(train_loader))\n",
    "            \n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, x_, _ in val_loader:\n",
    "                x = x.to(self.device)\n",
    "                x_ = x_.to(self.device)\n",
    "                \n",
    "                z_hat_, x_hat_ = self.model(x_)\n",
    "                z_hat, x_hat = self.model(x)\n",
    "                \n",
    "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,3))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        #print(f\"→ Validation Loss: {avg_loss:.6f}\")\n",
    "        self.model.train()\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6982ede7-a39e-433c-90a9-8850b1f73c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transform\n",
    "class NoiseTransform:\n",
    "    \"\"\"Add some noise.\"\"\"\n",
    "\n",
    "    def __init__(self, split_ratio=0.001, dim=3):\n",
    "\n",
    "        self.normal_dist = split_ratio*np.random.randn(dim,)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "      return x + self.normal_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters & Settings\n",
    "\n",
    "dataset_size = 2000\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "\n",
    "epochs = 1000\n",
    "max_patience = 1000\n",
    "\n",
    "split_ratio = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07121ef5-db8d-4bd5-a327-a8646f696809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = SpiralDataset(mode='train', n_samples=dataset_size, transform=NoiseTransform(split_ratio))\n",
    "val_dataset = SpiralDataset(mode='val', n_samples=dataset_size, transform=NoiseTransform(split_ratio))\n",
    "test_dataset = SpiralDataset(mode='test', n_samples=dataset_size)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9abf79c-a874-418b-b46f-aaa2fe649d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "name = 'results/UAE_Spiral'\n",
    "model = To_Uniform(\n",
    "                 encoder_layers=[3, 128, 128, 128, 2],\n",
    "                 decoder_layers=[2, 128, 128, 128, 3],\n",
    "                 encoder_act=nn.ReLU,\n",
    "                 decoder_act=nn.ReLU,\n",
    "                 final_encoder_act=nn.Sigmoid,\n",
    "                 final_decoder_act=nn.Sigmoid,\n",
    "                 use_batchnorm=True\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n",
      "Epoch   1 | Train Loss: 0.012885 | Validation Loss: 0.005378\n",
      "saved!\n",
      "Epoch   2 | Train Loss: 0.004595 | Validation Loss: 0.003818\n",
      "saved!\n",
      "Epoch   3 | Train Loss: 0.003359 | Validation Loss: 0.001563\n",
      "saved!\n",
      "Epoch   4 | Train Loss: 0.002046 | Validation Loss: 0.001093\n",
      "saved!\n",
      "Epoch   5 | Train Loss: 0.001565 | Validation Loss: 0.000959\n",
      "saved!\n",
      "Epoch   6 | Train Loss: 0.001519 | Validation Loss: 0.000641\n",
      "Epoch   7 | Train Loss: 0.001352 | Validation Loss: 0.000698\n",
      "saved!\n",
      "Epoch   8 | Train Loss: 0.001188 | Validation Loss: 0.000530\n",
      "Epoch   9 | Train Loss: 0.001112 | Validation Loss: 0.000594\n",
      "saved!\n",
      "Epoch  10 | Train Loss: 0.001451 | Validation Loss: 0.000496\n",
      "Epoch  11 | Train Loss: 0.001176 | Validation Loss: 0.000515\n",
      "Epoch  12 | Train Loss: 0.001137 | Validation Loss: 0.000672\n",
      "Epoch  13 | Train Loss: 0.001394 | Validation Loss: 0.000607\n",
      "saved!\n",
      "Epoch  14 | Train Loss: 0.001053 | Validation Loss: 0.000441\n",
      "Epoch  15 | Train Loss: 0.001329 | Validation Loss: 0.000598\n",
      "Epoch  16 | Train Loss: 0.000886 | Validation Loss: 0.000479\n",
      "Epoch  17 | Train Loss: 0.000961 | Validation Loss: 0.000475\n",
      "Epoch  18 | Train Loss: 0.001119 | Validation Loss: 0.000543\n",
      "Epoch  19 | Train Loss: 0.001107 | Validation Loss: 0.000504\n",
      "Epoch  20 | Train Loss: 0.000938 | Validation Loss: 0.000519\n",
      "Epoch  21 | Train Loss: 0.001023 | Validation Loss: 0.000654\n",
      "Epoch  22 | Train Loss: 0.001069 | Validation Loss: 0.000514\n",
      "Epoch  23 | Train Loss: 0.000986 | Validation Loss: 0.000535\n",
      "Epoch  24 | Train Loss: 0.001047 | Validation Loss: 0.000681\n",
      "Epoch  25 | Train Loss: 0.001237 | Validation Loss: 0.000470\n",
      "Epoch  26 | Train Loss: 0.000972 | Validation Loss: 0.000557\n",
      "Epoch  27 | Train Loss: 0.001011 | Validation Loss: 0.000546\n",
      "Epoch  28 | Train Loss: 0.001044 | Validation Loss: 0.000497\n",
      "Epoch  29 | Train Loss: 0.001016 | Validation Loss: 0.000602\n",
      "saved!\n",
      "Epoch  30 | Train Loss: 0.000897 | Validation Loss: 0.000411\n",
      "Epoch  31 | Train Loss: 0.000931 | Validation Loss: 0.000741\n",
      "Epoch  32 | Train Loss: 0.000897 | Validation Loss: 0.000541\n",
      "Epoch  33 | Train Loss: 0.001027 | Validation Loss: 0.000477\n",
      "Epoch  34 | Train Loss: 0.000733 | Validation Loss: 0.000426\n",
      "Epoch  35 | Train Loss: 0.001185 | Validation Loss: 0.000815\n",
      "Epoch  36 | Train Loss: 0.000929 | Validation Loss: 0.000501\n",
      "Epoch  37 | Train Loss: 0.001096 | Validation Loss: 0.000693\n",
      "Epoch  38 | Train Loss: 0.001000 | Validation Loss: 0.000589\n",
      "Epoch  39 | Train Loss: 0.000864 | Validation Loss: 0.000448\n",
      "Epoch  40 | Train Loss: 0.000915 | Validation Loss: 0.000475\n",
      "Epoch  41 | Train Loss: 0.000828 | Validation Loss: 0.000472\n",
      "saved!\n",
      "Epoch  42 | Train Loss: 0.000665 | Validation Loss: 0.000385\n",
      "Epoch  43 | Train Loss: 0.000909 | Validation Loss: 0.000467\n",
      "Epoch  44 | Train Loss: 0.000785 | Validation Loss: 0.000503\n",
      "Epoch  45 | Train Loss: 0.000886 | Validation Loss: 0.000460\n",
      "Epoch  46 | Train Loss: 0.000893 | Validation Loss: 0.000498\n",
      "Epoch  47 | Train Loss: 0.000690 | Validation Loss: 0.000504\n",
      "Epoch  48 | Train Loss: 0.000876 | Validation Loss: 0.000509\n",
      "Epoch  49 | Train Loss: 0.000766 | Validation Loss: 0.000517\n",
      "Epoch  50 | Train Loss: 0.000674 | Validation Loss: 0.000491\n",
      "Epoch  51 | Train Loss: 0.000847 | Validation Loss: 0.000499\n",
      "Epoch  52 | Train Loss: 0.000742 | Validation Loss: 0.000593\n",
      "Epoch  53 | Train Loss: 0.000688 | Validation Loss: 0.000422\n",
      "Epoch  54 | Train Loss: 0.000860 | Validation Loss: 0.000669\n",
      "Epoch  55 | Train Loss: 0.000827 | Validation Loss: 0.000547\n",
      "Epoch  56 | Train Loss: 0.000877 | Validation Loss: 0.000482\n",
      "Epoch  57 | Train Loss: 0.000777 | Validation Loss: 0.000515\n",
      "Epoch  58 | Train Loss: 0.000792 | Validation Loss: 0.000493\n",
      "Epoch  59 | Train Loss: 0.000919 | Validation Loss: 0.000402\n",
      "Epoch  60 | Train Loss: 0.000741 | Validation Loss: 0.000491\n",
      "Epoch  61 | Train Loss: 0.000882 | Validation Loss: 0.000464\n",
      "Epoch  62 | Train Loss: 0.000756 | Validation Loss: 0.000492\n",
      "Epoch  63 | Train Loss: 0.000833 | Validation Loss: 0.000440\n",
      "Epoch  64 | Train Loss: 0.000752 | Validation Loss: 0.000455\n",
      "Epoch  65 | Train Loss: 0.000725 | Validation Loss: 0.000409\n",
      "Epoch  66 | Train Loss: 0.000689 | Validation Loss: 0.000461\n",
      "Epoch  67 | Train Loss: 0.000772 | Validation Loss: 0.000409\n",
      "Epoch  68 | Train Loss: 0.000670 | Validation Loss: 0.000421\n",
      "Epoch  69 | Train Loss: 0.000704 | Validation Loss: 0.000547\n",
      "Epoch  70 | Train Loss: 0.000692 | Validation Loss: 0.000401\n",
      "Epoch  71 | Train Loss: 0.000742 | Validation Loss: 0.000544\n",
      "Epoch  72 | Train Loss: 0.000667 | Validation Loss: 0.000507\n",
      "Epoch  73 | Train Loss: 0.000810 | Validation Loss: 0.000438\n",
      "Epoch  74 | Train Loss: 0.000754 | Validation Loss: 0.000511\n",
      "Epoch  75 | Train Loss: 0.000624 | Validation Loss: 0.000445\n",
      "Epoch  76 | Train Loss: 0.000665 | Validation Loss: 0.000468\n",
      "Epoch  77 | Train Loss: 0.000615 | Validation Loss: 0.000421\n",
      "saved!\n",
      "Epoch  78 | Train Loss: 0.000578 | Validation Loss: 0.000357\n",
      "Epoch  79 | Train Loss: 0.000596 | Validation Loss: 0.000465\n",
      "Epoch  80 | Train Loss: 0.000713 | Validation Loss: 0.000401\n",
      "Epoch  81 | Train Loss: 0.000656 | Validation Loss: 0.000462\n",
      "Epoch  82 | Train Loss: 0.000693 | Validation Loss: 0.000459\n",
      "Epoch  83 | Train Loss: 0.000712 | Validation Loss: 0.000468\n",
      "Epoch  84 | Train Loss: 0.000645 | Validation Loss: 0.000462\n",
      "Epoch  85 | Train Loss: 0.000624 | Validation Loss: 0.000410\n",
      "Epoch  86 | Train Loss: 0.000734 | Validation Loss: 0.000440\n",
      "Epoch  87 | Train Loss: 0.000627 | Validation Loss: 0.000497\n",
      "Epoch  88 | Train Loss: 0.000641 | Validation Loss: 0.000497\n",
      "Epoch  89 | Train Loss: 0.000704 | Validation Loss: 0.000503\n",
      "Epoch  90 | Train Loss: 0.000691 | Validation Loss: 0.000433\n",
      "Epoch  91 | Train Loss: 0.000580 | Validation Loss: 0.000417\n",
      "Epoch  92 | Train Loss: 0.000687 | Validation Loss: 0.000493\n",
      "Epoch  93 | Train Loss: 0.000609 | Validation Loss: 0.000453\n",
      "Epoch  94 | Train Loss: 0.000771 | Validation Loss: 0.000544\n",
      "Epoch  95 | Train Loss: 0.000638 | Validation Loss: 0.000404\n",
      "Epoch  96 | Train Loss: 0.000788 | Validation Loss: 0.000533\n",
      "Epoch  97 | Train Loss: 0.000934 | Validation Loss: 0.000513\n",
      "Epoch  98 | Train Loss: 0.000672 | Validation Loss: 0.000552\n",
      "Epoch  99 | Train Loss: 0.000976 | Validation Loss: 0.000474\n",
      "Epoch 100 | Train Loss: 0.000712 | Validation Loss: 0.000428\n",
      "Epoch 101 | Train Loss: 0.000676 | Validation Loss: 0.000371\n",
      "Epoch 102 | Train Loss: 0.000535 | Validation Loss: 0.000406\n",
      "Epoch 103 | Train Loss: 0.000636 | Validation Loss: 0.000446\n",
      "Epoch 104 | Train Loss: 0.000596 | Validation Loss: 0.000380\n",
      "Epoch 105 | Train Loss: 0.000689 | Validation Loss: 0.000517\n",
      "Epoch 106 | Train Loss: 0.000689 | Validation Loss: 0.000395\n",
      "Epoch 107 | Train Loss: 0.000664 | Validation Loss: 0.000397\n",
      "Epoch 108 | Train Loss: 0.000656 | Validation Loss: 0.000409\n",
      "Epoch 109 | Train Loss: 0.000595 | Validation Loss: 0.000392\n",
      "Epoch 110 | Train Loss: 0.000615 | Validation Loss: 0.000567\n",
      "Epoch 111 | Train Loss: 0.000633 | Validation Loss: 0.000489\n",
      "Epoch 112 | Train Loss: 0.000590 | Validation Loss: 0.000411\n",
      "Epoch 113 | Train Loss: 0.000589 | Validation Loss: 0.000365\n",
      "Epoch 114 | Train Loss: 0.000833 | Validation Loss: 0.000753\n",
      "Epoch 115 | Train Loss: 0.001078 | Validation Loss: 0.000614\n",
      "Epoch 116 | Train Loss: 0.000615 | Validation Loss: 0.000511\n",
      "Epoch 117 | Train Loss: 0.000527 | Validation Loss: 0.000367\n",
      "Epoch 118 | Train Loss: 0.000539 | Validation Loss: 0.000368\n",
      "Epoch 119 | Train Loss: 0.000643 | Validation Loss: 0.000397\n",
      "Epoch 120 | Train Loss: 0.000637 | Validation Loss: 0.000510\n",
      "Epoch 121 | Train Loss: 0.000706 | Validation Loss: 0.000445\n",
      "Epoch 122 | Train Loss: 0.000611 | Validation Loss: 0.000407\n",
      "Epoch 123 | Train Loss: 0.000597 | Validation Loss: 0.000388\n",
      "Epoch 124 | Train Loss: 0.000611 | Validation Loss: 0.000438\n",
      "Epoch 125 | Train Loss: 0.000545 | Validation Loss: 0.000464\n",
      "Epoch 126 | Train Loss: 0.000595 | Validation Loss: 0.000427\n",
      "Epoch 127 | Train Loss: 0.000626 | Validation Loss: 0.000531\n",
      "Epoch 128 | Train Loss: 0.000626 | Validation Loss: 0.000407\n",
      "Epoch 129 | Train Loss: 0.000508 | Validation Loss: 0.000406\n",
      "Epoch 130 | Train Loss: 0.000591 | Validation Loss: 0.000384\n",
      "Epoch 131 | Train Loss: 0.000577 | Validation Loss: 0.000519\n",
      "Epoch 132 | Train Loss: 0.000681 | Validation Loss: 0.000401\n",
      "Epoch 133 | Train Loss: 0.000614 | Validation Loss: 0.000419\n",
      "Epoch 134 | Train Loss: 0.000549 | Validation Loss: 0.000407\n",
      "Epoch 135 | Train Loss: 0.000551 | Validation Loss: 0.000426\n",
      "Epoch 136 | Train Loss: 0.000528 | Validation Loss: 0.000411\n",
      "Epoch 137 | Train Loss: 0.000595 | Validation Loss: 0.000474\n",
      "Epoch 138 | Train Loss: 0.000756 | Validation Loss: 0.000452\n",
      "Epoch 139 | Train Loss: 0.000522 | Validation Loss: 0.000502\n",
      "Epoch 140 | Train Loss: 0.000640 | Validation Loss: 0.000465\n",
      "Epoch 141 | Train Loss: 0.000639 | Validation Loss: 0.000404\n",
      "Epoch 142 | Train Loss: 0.000597 | Validation Loss: 0.000412\n",
      "Epoch 143 | Train Loss: 0.000568 | Validation Loss: 0.000390\n",
      "Epoch 144 | Train Loss: 0.000587 | Validation Loss: 0.000468\n",
      "Epoch 145 | Train Loss: 0.000565 | Validation Loss: 0.000402\n",
      "Epoch 146 | Train Loss: 0.000593 | Validation Loss: 0.000497\n",
      "Epoch 147 | Train Loss: 0.000557 | Validation Loss: 0.000404\n",
      "Epoch 148 | Train Loss: 0.000580 | Validation Loss: 0.000488\n",
      "Epoch 149 | Train Loss: 0.000613 | Validation Loss: 0.000381\n",
      "Epoch 150 | Train Loss: 0.000545 | Validation Loss: 0.000392\n",
      "saved!\n",
      "Epoch 151 | Train Loss: 0.000576 | Validation Loss: 0.000355\n",
      "saved!\n",
      "Epoch 152 | Train Loss: 0.000576 | Validation Loss: 0.000353\n",
      "Epoch 153 | Train Loss: 0.000653 | Validation Loss: 0.000617\n",
      "Epoch 154 | Train Loss: 0.000654 | Validation Loss: 0.000392\n",
      "Epoch 155 | Train Loss: 0.000550 | Validation Loss: 0.000420\n",
      "Epoch 156 | Train Loss: 0.000569 | Validation Loss: 0.000360\n",
      "Epoch 157 | Train Loss: 0.000575 | Validation Loss: 0.000364\n",
      "Epoch 158 | Train Loss: 0.000549 | Validation Loss: 0.000415\n",
      "Epoch 159 | Train Loss: 0.000711 | Validation Loss: 0.000459\n",
      "Epoch 160 | Train Loss: 0.000555 | Validation Loss: 0.000423\n",
      "Epoch 161 | Train Loss: 0.000551 | Validation Loss: 0.000405\n",
      "saved!\n",
      "Epoch 162 | Train Loss: 0.000489 | Validation Loss: 0.000341\n",
      "Epoch 163 | Train Loss: 0.000513 | Validation Loss: 0.000376\n",
      "Epoch 164 | Train Loss: 0.000568 | Validation Loss: 0.000361\n",
      "Epoch 165 | Train Loss: 0.000551 | Validation Loss: 0.000442\n",
      "Epoch 166 | Train Loss: 0.000674 | Validation Loss: 0.000357\n",
      "Epoch 167 | Train Loss: 0.000615 | Validation Loss: 0.000448\n",
      "Epoch 168 | Train Loss: 0.000597 | Validation Loss: 0.000412\n",
      "Epoch 169 | Train Loss: 0.000530 | Validation Loss: 0.000435\n",
      "Epoch 170 | Train Loss: 0.000606 | Validation Loss: 0.000364\n",
      "Epoch 171 | Train Loss: 0.000620 | Validation Loss: 0.000404\n",
      "Epoch 172 | Train Loss: 0.000576 | Validation Loss: 0.000390\n",
      "Epoch 173 | Train Loss: 0.000491 | Validation Loss: 0.000383\n",
      "Epoch 174 | Train Loss: 0.000505 | Validation Loss: 0.000384\n",
      "Epoch 175 | Train Loss: 0.000517 | Validation Loss: 0.000362\n",
      "Epoch 176 | Train Loss: 0.000573 | Validation Loss: 0.000420\n",
      "Epoch 177 | Train Loss: 0.000517 | Validation Loss: 0.000400\n",
      "Epoch 178 | Train Loss: 0.000525 | Validation Loss: 0.000396\n",
      "Epoch 179 | Train Loss: 0.000496 | Validation Loss: 0.000390\n",
      "Epoch 180 | Train Loss: 0.000572 | Validation Loss: 0.000389\n",
      "Epoch 181 | Train Loss: 0.000493 | Validation Loss: 0.000358\n",
      "Epoch 182 | Train Loss: 0.000552 | Validation Loss: 0.000401\n",
      "Epoch 183 | Train Loss: 0.000538 | Validation Loss: 0.000413\n",
      "Epoch 184 | Train Loss: 0.000718 | Validation Loss: 0.000506\n",
      "Epoch 185 | Train Loss: 0.000566 | Validation Loss: 0.000399\n",
      "Epoch 186 | Train Loss: 0.000610 | Validation Loss: 0.000406\n",
      "Epoch 187 | Train Loss: 0.000506 | Validation Loss: 0.000378\n",
      "saved!\n",
      "Epoch 188 | Train Loss: 0.000503 | Validation Loss: 0.000325\n",
      "Epoch 189 | Train Loss: 0.000520 | Validation Loss: 0.000411\n",
      "Epoch 190 | Train Loss: 0.000537 | Validation Loss: 0.000587\n",
      "Epoch 191 | Train Loss: 0.000559 | Validation Loss: 0.000363\n",
      "Epoch 192 | Train Loss: 0.000589 | Validation Loss: 0.000434\n",
      "Epoch 193 | Train Loss: 0.000471 | Validation Loss: 0.000433\n",
      "Epoch 194 | Train Loss: 0.000503 | Validation Loss: 0.000374\n",
      "Epoch 195 | Train Loss: 0.000474 | Validation Loss: 0.000376\n",
      "Epoch 196 | Train Loss: 0.000513 | Validation Loss: 0.000388\n",
      "Epoch 197 | Train Loss: 0.000614 | Validation Loss: 0.000405\n",
      "Epoch 198 | Train Loss: 0.000492 | Validation Loss: 0.000397\n",
      "Epoch 199 | Train Loss: 0.000520 | Validation Loss: 0.000466\n",
      "Epoch 200 | Train Loss: 0.000468 | Validation Loss: 0.000365\n",
      "Epoch 201 | Train Loss: 0.000497 | Validation Loss: 0.000374\n",
      "Epoch 202 | Train Loss: 0.000537 | Validation Loss: 0.000352\n",
      "Epoch 203 | Train Loss: 0.000662 | Validation Loss: 0.000352\n",
      "Epoch 204 | Train Loss: 0.000548 | Validation Loss: 0.000445\n",
      "Epoch 205 | Train Loss: 0.000501 | Validation Loss: 0.000377\n",
      "Epoch 206 | Train Loss: 0.000619 | Validation Loss: 0.000434\n",
      "Epoch 207 | Train Loss: 0.000549 | Validation Loss: 0.000414\n",
      "Epoch 208 | Train Loss: 0.000539 | Validation Loss: 0.000394\n",
      "Epoch 209 | Train Loss: 0.000488 | Validation Loss: 0.000384\n",
      "Epoch 210 | Train Loss: 0.000676 | Validation Loss: 0.000486\n",
      "Epoch 211 | Train Loss: 0.000530 | Validation Loss: 0.000385\n",
      "Epoch 212 | Train Loss: 0.000674 | Validation Loss: 0.000496\n",
      "Epoch 213 | Train Loss: 0.000579 | Validation Loss: 0.000560\n",
      "Epoch 214 | Train Loss: 0.000690 | Validation Loss: 0.000403\n",
      "Epoch 215 | Train Loss: 0.000535 | Validation Loss: 0.000449\n",
      "Epoch 216 | Train Loss: 0.000528 | Validation Loss: 0.000417\n",
      "Epoch 217 | Train Loss: 0.000498 | Validation Loss: 0.000365\n",
      "Epoch 218 | Train Loss: 0.000532 | Validation Loss: 0.000401\n",
      "Epoch 219 | Train Loss: 0.000472 | Validation Loss: 0.000362\n",
      "Epoch 220 | Train Loss: 0.000578 | Validation Loss: 0.000438\n",
      "Epoch 221 | Train Loss: 0.000561 | Validation Loss: 0.000452\n",
      "Epoch 222 | Train Loss: 0.000539 | Validation Loss: 0.000361\n",
      "Epoch 223 | Train Loss: 0.000489 | Validation Loss: 0.000386\n",
      "Epoch 224 | Train Loss: 0.000579 | Validation Loss: 0.000424\n",
      "Epoch 225 | Train Loss: 0.000507 | Validation Loss: 0.000383\n",
      "Epoch 226 | Train Loss: 0.000480 | Validation Loss: 0.000462\n",
      "Epoch 227 | Train Loss: 0.000553 | Validation Loss: 0.000388\n",
      "Epoch 228 | Train Loss: 0.000533 | Validation Loss: 0.000394\n",
      "Epoch 229 | Train Loss: 0.000549 | Validation Loss: 0.000393\n",
      "Epoch 230 | Train Loss: 0.000484 | Validation Loss: 0.000377\n",
      "Epoch 231 | Train Loss: 0.000519 | Validation Loss: 0.000454\n",
      "Epoch 232 | Train Loss: 0.000563 | Validation Loss: 0.000363\n",
      "Epoch 233 | Train Loss: 0.000451 | Validation Loss: 0.000372\n",
      "Epoch 234 | Train Loss: 0.000485 | Validation Loss: 0.000441\n",
      "Epoch 235 | Train Loss: 0.000503 | Validation Loss: 0.000370\n",
      "Epoch 236 | Train Loss: 0.000547 | Validation Loss: 0.000343\n",
      "Epoch 237 | Train Loss: 0.000599 | Validation Loss: 0.000412\n",
      "Epoch 238 | Train Loss: 0.000481 | Validation Loss: 0.000377\n",
      "Epoch 239 | Train Loss: 0.000524 | Validation Loss: 0.000379\n",
      "Epoch 240 | Train Loss: 0.000490 | Validation Loss: 0.000330\n",
      "Epoch 241 | Train Loss: 0.000497 | Validation Loss: 0.000424\n",
      "Epoch 242 | Train Loss: 0.000646 | Validation Loss: 0.000354\n",
      "Epoch 243 | Train Loss: 0.000479 | Validation Loss: 0.000367\n",
      "Epoch 244 | Train Loss: 0.000457 | Validation Loss: 0.000390\n",
      "Epoch 245 | Train Loss: 0.000462 | Validation Loss: 0.000384\n",
      "Epoch 246 | Train Loss: 0.000525 | Validation Loss: 0.000399\n",
      "Epoch 247 | Train Loss: 0.000557 | Validation Loss: 0.000375\n",
      "Epoch 248 | Train Loss: 0.000506 | Validation Loss: 0.000342\n",
      "Epoch 249 | Train Loss: 0.000518 | Validation Loss: 0.000452\n",
      "Epoch 250 | Train Loss: 0.000541 | Validation Loss: 0.000468\n",
      "Epoch 251 | Train Loss: 0.000512 | Validation Loss: 0.000440\n",
      "Epoch 252 | Train Loss: 0.000527 | Validation Loss: 0.000470\n",
      "Epoch 253 | Train Loss: 0.000533 | Validation Loss: 0.000365\n",
      "Epoch 254 | Train Loss: 0.000457 | Validation Loss: 0.000359\n",
      "Epoch 255 | Train Loss: 0.000509 | Validation Loss: 0.000413\n",
      "Epoch 256 | Train Loss: 0.000492 | Validation Loss: 0.000410\n",
      "Epoch 257 | Train Loss: 0.000504 | Validation Loss: 0.000492\n",
      "Epoch 258 | Train Loss: 0.000576 | Validation Loss: 0.000350\n",
      "Epoch 259 | Train Loss: 0.000470 | Validation Loss: 0.000399\n",
      "Epoch 260 | Train Loss: 0.000444 | Validation Loss: 0.000380\n",
      "Epoch 261 | Train Loss: 0.000490 | Validation Loss: 0.001202\n",
      "Epoch 262 | Train Loss: 0.000505 | Validation Loss: 0.000412\n",
      "Epoch 263 | Train Loss: 0.000462 | Validation Loss: 0.000342\n",
      "Epoch 264 | Train Loss: 0.000580 | Validation Loss: 0.000396\n",
      "Epoch 265 | Train Loss: 0.000519 | Validation Loss: 0.000422\n",
      "Epoch 266 | Train Loss: 0.000513 | Validation Loss: 0.000456\n",
      "Epoch 267 | Train Loss: 0.000468 | Validation Loss: 0.000377\n",
      "Epoch 268 | Train Loss: 0.000468 | Validation Loss: 0.000346\n",
      "Epoch 269 | Train Loss: 0.000514 | Validation Loss: 0.000326\n",
      "Epoch 270 | Train Loss: 0.000507 | Validation Loss: 0.000357\n",
      "Epoch 271 | Train Loss: 0.000459 | Validation Loss: 0.000363\n",
      "Epoch 272 | Train Loss: 0.000553 | Validation Loss: 0.000391\n",
      "Epoch 273 | Train Loss: 0.000479 | Validation Loss: 0.000444\n",
      "Epoch 274 | Train Loss: 0.000469 | Validation Loss: 0.000365\n",
      "Epoch 275 | Train Loss: 0.000478 | Validation Loss: 0.000381\n",
      "Epoch 276 | Train Loss: 0.000492 | Validation Loss: 0.000380\n",
      "Epoch 277 | Train Loss: 0.000550 | Validation Loss: 0.000377\n",
      "Epoch 278 | Train Loss: 0.000502 | Validation Loss: 0.000357\n",
      "Epoch 279 | Train Loss: 0.000510 | Validation Loss: 0.000387\n",
      "Epoch 280 | Train Loss: 0.000470 | Validation Loss: 0.000336\n",
      "Epoch 281 | Train Loss: 0.000451 | Validation Loss: 0.000391\n",
      "Epoch 282 | Train Loss: 0.000532 | Validation Loss: 0.000456\n",
      "Epoch 283 | Train Loss: 0.000583 | Validation Loss: 0.000453\n",
      "Epoch 284 | Train Loss: 0.000517 | Validation Loss: 0.000431\n",
      "Epoch 285 | Train Loss: 0.000506 | Validation Loss: 0.000421\n",
      "Epoch 286 | Train Loss: 0.000467 | Validation Loss: 0.000453\n",
      "Epoch 287 | Train Loss: 0.000441 | Validation Loss: 0.000429\n",
      "Epoch 288 | Train Loss: 0.000524 | Validation Loss: 0.000462\n",
      "Epoch 289 | Train Loss: 0.000553 | Validation Loss: 0.000348\n",
      "Epoch 290 | Train Loss: 0.000491 | Validation Loss: 0.000396\n",
      "Epoch 291 | Train Loss: 0.000488 | Validation Loss: 0.000401\n",
      "Epoch 292 | Train Loss: 0.000481 | Validation Loss: 0.000407\n",
      "Epoch 293 | Train Loss: 0.000455 | Validation Loss: 0.000383\n",
      "Epoch 294 | Train Loss: 0.000449 | Validation Loss: 0.000387\n",
      "Epoch 295 | Train Loss: 0.000478 | Validation Loss: 0.000395\n",
      "Epoch 296 | Train Loss: 0.000485 | Validation Loss: 0.000420\n",
      "Epoch 297 | Train Loss: 0.000455 | Validation Loss: 0.000362\n",
      "Epoch 298 | Train Loss: 0.000478 | Validation Loss: 0.000353\n",
      "Epoch 299 | Train Loss: 0.000445 | Validation Loss: 0.000395\n",
      "Epoch 300 | Train Loss: 0.000455 | Validation Loss: 0.000366\n",
      "Epoch 301 | Train Loss: 0.000547 | Validation Loss: 0.000406\n",
      "Epoch 302 | Train Loss: 0.000454 | Validation Loss: 0.000372\n",
      "Epoch 303 | Train Loss: 0.000453 | Validation Loss: 0.000389\n",
      "Epoch 304 | Train Loss: 0.000546 | Validation Loss: 0.000450\n",
      "Epoch 305 | Train Loss: 0.000418 | Validation Loss: 0.000371\n",
      "Epoch 306 | Train Loss: 0.000637 | Validation Loss: 0.000367\n",
      "Epoch 307 | Train Loss: 0.000482 | Validation Loss: 0.000392\n",
      "Epoch 308 | Train Loss: 0.000479 | Validation Loss: 0.000365\n",
      "Epoch 309 | Train Loss: 0.000442 | Validation Loss: 0.000395\n",
      "Epoch 310 | Train Loss: 0.000474 | Validation Loss: 0.000391\n",
      "Epoch 311 | Train Loss: 0.000541 | Validation Loss: 0.000554\n",
      "Epoch 312 | Train Loss: 0.000509 | Validation Loss: 0.000391\n",
      "Epoch 313 | Train Loss: 0.000437 | Validation Loss: 0.000368\n",
      "Epoch 314 | Train Loss: 0.000409 | Validation Loss: 0.000330\n",
      "Epoch 315 | Train Loss: 0.000475 | Validation Loss: 0.000401\n",
      "Epoch 316 | Train Loss: 0.000483 | Validation Loss: 0.000370\n",
      "Epoch 317 | Train Loss: 0.000426 | Validation Loss: 0.000357\n",
      "Epoch 318 | Train Loss: 0.000413 | Validation Loss: 0.000415\n",
      "Epoch 319 | Train Loss: 0.000440 | Validation Loss: 0.000339\n",
      "Epoch 320 | Train Loss: 0.000480 | Validation Loss: 0.000377\n",
      "Epoch 321 | Train Loss: 0.000536 | Validation Loss: 0.000359\n",
      "Epoch 322 | Train Loss: 0.000498 | Validation Loss: 0.000417\n",
      "Epoch 323 | Train Loss: 0.000423 | Validation Loss: 0.000374\n",
      "Epoch 324 | Train Loss: 0.000439 | Validation Loss: 0.000346\n",
      "Epoch 325 | Train Loss: 0.000476 | Validation Loss: 0.000340\n",
      "Epoch 326 | Train Loss: 0.000568 | Validation Loss: 0.000419\n",
      "Epoch 327 | Train Loss: 0.000535 | Validation Loss: 0.000384\n",
      "Epoch 328 | Train Loss: 0.000471 | Validation Loss: 0.000461\n",
      "Epoch 329 | Train Loss: 0.000558 | Validation Loss: 0.000408\n",
      "Epoch 330 | Train Loss: 0.000453 | Validation Loss: 0.000440\n",
      "Epoch 331 | Train Loss: 0.000421 | Validation Loss: 0.000367\n",
      "Epoch 332 | Train Loss: 0.000456 | Validation Loss: 0.000403\n",
      "Epoch 333 | Train Loss: 0.000473 | Validation Loss: 0.000393\n",
      "Epoch 334 | Train Loss: 0.000457 | Validation Loss: 0.000346\n",
      "Epoch 335 | Train Loss: 0.000584 | Validation Loss: 0.000387\n",
      "Epoch 336 | Train Loss: 0.000490 | Validation Loss: 0.000367\n",
      "Epoch 337 | Train Loss: 0.000453 | Validation Loss: 0.000372\n",
      "Epoch 338 | Train Loss: 0.000432 | Validation Loss: 0.000362\n",
      "Epoch 339 | Train Loss: 0.000443 | Validation Loss: 0.000357\n",
      "Epoch 340 | Train Loss: 0.000528 | Validation Loss: 0.000491\n",
      "Epoch 341 | Train Loss: 0.000451 | Validation Loss: 0.000359\n",
      "Epoch 342 | Train Loss: 0.000515 | Validation Loss: 0.000375\n",
      "Epoch 343 | Train Loss: 0.000484 | Validation Loss: 0.000400\n",
      "Epoch 344 | Train Loss: 0.000460 | Validation Loss: 0.000349\n",
      "Epoch 345 | Train Loss: 0.000595 | Validation Loss: 0.000460\n",
      "Epoch 346 | Train Loss: 0.000559 | Validation Loss: 0.000346\n",
      "Epoch 347 | Train Loss: 0.000532 | Validation Loss: 0.000420\n",
      "Epoch 348 | Train Loss: 0.000538 | Validation Loss: 0.000361\n",
      "Epoch 349 | Train Loss: 0.000577 | Validation Loss: 0.000366\n",
      "Epoch 350 | Train Loss: 0.000463 | Validation Loss: 0.000395\n",
      "Epoch 351 | Train Loss: 0.000501 | Validation Loss: 0.000428\n",
      "Epoch 352 | Train Loss: 0.000454 | Validation Loss: 0.000409\n",
      "Epoch 353 | Train Loss: 0.000466 | Validation Loss: 0.000385\n",
      "Epoch 354 | Train Loss: 0.000430 | Validation Loss: 0.000383\n",
      "Epoch 355 | Train Loss: 0.000412 | Validation Loss: 0.000352\n",
      "Epoch 356 | Train Loss: 0.000413 | Validation Loss: 0.000377\n",
      "Epoch 357 | Train Loss: 0.000423 | Validation Loss: 0.000340\n",
      "Epoch 358 | Train Loss: 0.000435 | Validation Loss: 0.000341\n",
      "Epoch 359 | Train Loss: 0.000452 | Validation Loss: 0.000361\n",
      "Epoch 360 | Train Loss: 0.000479 | Validation Loss: 0.000366\n",
      "Epoch 361 | Train Loss: 0.000515 | Validation Loss: 0.000367\n",
      "Epoch 362 | Train Loss: 0.000467 | Validation Loss: 0.000355\n",
      "Epoch 363 | Train Loss: 0.000516 | Validation Loss: 0.000419\n",
      "Epoch 364 | Train Loss: 0.000546 | Validation Loss: 0.000374\n",
      "Epoch 365 | Train Loss: 0.000544 | Validation Loss: 0.000390\n",
      "Epoch 366 | Train Loss: 0.000500 | Validation Loss: 0.000401\n",
      "Epoch 367 | Train Loss: 0.000500 | Validation Loss: 0.000380\n",
      "Epoch 368 | Train Loss: 0.000461 | Validation Loss: 0.000466\n",
      "Epoch 369 | Train Loss: 0.000481 | Validation Loss: 0.000378\n",
      "Epoch 370 | Train Loss: 0.000484 | Validation Loss: 0.000465\n",
      "Epoch 371 | Train Loss: 0.000503 | Validation Loss: 0.000367\n",
      "Epoch 372 | Train Loss: 0.000433 | Validation Loss: 0.000345\n",
      "Epoch 373 | Train Loss: 0.000440 | Validation Loss: 0.000350\n",
      "Epoch 374 | Train Loss: 0.000470 | Validation Loss: 0.000434\n",
      "Epoch 375 | Train Loss: 0.000465 | Validation Loss: 0.000388\n",
      "Epoch 376 | Train Loss: 0.000567 | Validation Loss: 0.000389\n",
      "Epoch 377 | Train Loss: 0.000450 | Validation Loss: 0.000329\n",
      "Epoch 378 | Train Loss: 0.000429 | Validation Loss: 0.000375\n",
      "Epoch 379 | Train Loss: 0.000417 | Validation Loss: 0.000352\n",
      "Epoch 380 | Train Loss: 0.000466 | Validation Loss: 0.000407\n",
      "Epoch 381 | Train Loss: 0.000509 | Validation Loss: 0.000401\n",
      "Epoch 382 | Train Loss: 0.000458 | Validation Loss: 0.000407\n",
      "Epoch 383 | Train Loss: 0.000434 | Validation Loss: 0.000355\n",
      "Epoch 384 | Train Loss: 0.000414 | Validation Loss: 0.000364\n",
      "Epoch 385 | Train Loss: 0.000419 | Validation Loss: 0.000340\n",
      "Epoch 386 | Train Loss: 0.000496 | Validation Loss: 0.000362\n",
      "Epoch 387 | Train Loss: 0.000533 | Validation Loss: 0.000448\n",
      "Epoch 388 | Train Loss: 0.000499 | Validation Loss: 0.000401\n",
      "Epoch 389 | Train Loss: 0.000442 | Validation Loss: 0.000351\n",
      "Epoch 390 | Train Loss: 0.000527 | Validation Loss: 0.000685\n",
      "Epoch 391 | Train Loss: 0.000503 | Validation Loss: 0.000377\n",
      "Epoch 392 | Train Loss: 0.000558 | Validation Loss: 0.000463\n",
      "Epoch 393 | Train Loss: 0.000461 | Validation Loss: 0.000352\n",
      "Epoch 394 | Train Loss: 0.000586 | Validation Loss: 0.000379\n",
      "Epoch 395 | Train Loss: 0.000447 | Validation Loss: 0.000354\n",
      "Epoch 396 | Train Loss: 0.000456 | Validation Loss: 0.000371\n",
      "Epoch 397 | Train Loss: 0.000453 | Validation Loss: 0.000405\n",
      "Epoch 398 | Train Loss: 0.000455 | Validation Loss: 0.000382\n",
      "Epoch 399 | Train Loss: 0.000443 | Validation Loss: 0.000330\n",
      "Epoch 400 | Train Loss: 0.000394 | Validation Loss: 0.000355\n",
      "Epoch 401 | Train Loss: 0.000479 | Validation Loss: 0.000400\n",
      "Epoch 402 | Train Loss: 0.000509 | Validation Loss: 0.000386\n",
      "Epoch 403 | Train Loss: 0.000386 | Validation Loss: 0.000350\n",
      "Epoch 404 | Train Loss: 0.000456 | Validation Loss: 0.000382\n",
      "Epoch 405 | Train Loss: 0.000531 | Validation Loss: 0.000421\n",
      "Epoch 406 | Train Loss: 0.000525 | Validation Loss: 0.000437\n",
      "Epoch 407 | Train Loss: 0.000434 | Validation Loss: 0.000395\n",
      "Epoch 408 | Train Loss: 0.000495 | Validation Loss: 0.000383\n",
      "Epoch 409 | Train Loss: 0.000483 | Validation Loss: 0.000355\n",
      "Epoch 410 | Train Loss: 0.000508 | Validation Loss: 0.000405\n",
      "Epoch 411 | Train Loss: 0.000448 | Validation Loss: 0.000447\n",
      "Epoch 412 | Train Loss: 0.000517 | Validation Loss: 0.000387\n",
      "Epoch 413 | Train Loss: 0.000447 | Validation Loss: 0.000426\n",
      "Epoch 414 | Train Loss: 0.000446 | Validation Loss: 0.000370\n",
      "Epoch 415 | Train Loss: 0.000462 | Validation Loss: 0.000389\n",
      "Epoch 416 | Train Loss: 0.000434 | Validation Loss: 0.000347\n",
      "Epoch 417 | Train Loss: 0.000479 | Validation Loss: 0.000430\n",
      "Epoch 418 | Train Loss: 0.000422 | Validation Loss: 0.000380\n",
      "Epoch 419 | Train Loss: 0.000496 | Validation Loss: 0.000377\n",
      "Epoch 420 | Train Loss: 0.000500 | Validation Loss: 0.000439\n",
      "Epoch 421 | Train Loss: 0.000525 | Validation Loss: 0.000444\n",
      "Epoch 422 | Train Loss: 0.000541 | Validation Loss: 0.000421\n",
      "Epoch 423 | Train Loss: 0.000468 | Validation Loss: 0.000361\n",
      "Epoch 424 | Train Loss: 0.000402 | Validation Loss: 0.000341\n",
      "Epoch 425 | Train Loss: 0.000488 | Validation Loss: 0.000403\n",
      "Epoch 426 | Train Loss: 0.000453 | Validation Loss: 0.000338\n",
      "Epoch 427 | Train Loss: 0.000419 | Validation Loss: 0.000372\n",
      "Epoch 428 | Train Loss: 0.000466 | Validation Loss: 0.000383\n",
      "Epoch 429 | Train Loss: 0.000461 | Validation Loss: 0.000371\n",
      "Epoch 430 | Train Loss: 0.000448 | Validation Loss: 0.000368\n",
      "Epoch 431 | Train Loss: 0.000500 | Validation Loss: 0.000408\n",
      "Epoch 432 | Train Loss: 0.000457 | Validation Loss: 0.000367\n",
      "Epoch 433 | Train Loss: 0.000517 | Validation Loss: 0.000470\n",
      "Epoch 434 | Train Loss: 0.000558 | Validation Loss: 0.000380\n",
      "Epoch 435 | Train Loss: 0.000426 | Validation Loss: 0.000377\n",
      "Epoch 436 | Train Loss: 0.000464 | Validation Loss: 0.000382\n",
      "Epoch 437 | Train Loss: 0.000425 | Validation Loss: 0.000349\n",
      "Epoch 438 | Train Loss: 0.000409 | Validation Loss: 0.000344\n",
      "Epoch 439 | Train Loss: 0.000438 | Validation Loss: 0.000358\n",
      "Epoch 440 | Train Loss: 0.000627 | Validation Loss: 0.000445\n",
      "Epoch 441 | Train Loss: 0.000482 | Validation Loss: 0.000514\n",
      "Epoch 442 | Train Loss: 0.000417 | Validation Loss: 0.000338\n",
      "Epoch 443 | Train Loss: 0.000466 | Validation Loss: 0.000424\n",
      "Epoch 444 | Train Loss: 0.000401 | Validation Loss: 0.000340\n",
      "Epoch 445 | Train Loss: 0.000520 | Validation Loss: 0.000356\n",
      "Epoch 446 | Train Loss: 0.000468 | Validation Loss: 0.000367\n",
      "Epoch 447 | Train Loss: 0.000413 | Validation Loss: 0.000399\n",
      "Epoch 448 | Train Loss: 0.000509 | Validation Loss: 0.000485\n",
      "saved!\n",
      "Epoch 449 | Train Loss: 0.000405 | Validation Loss: 0.000325\n",
      "Epoch 450 | Train Loss: 0.000442 | Validation Loss: 0.000445\n",
      "Epoch 451 | Train Loss: 0.000577 | Validation Loss: 0.000488\n",
      "Epoch 452 | Train Loss: 0.000489 | Validation Loss: 0.000415\n",
      "Epoch 453 | Train Loss: 0.000542 | Validation Loss: 0.000420\n",
      "Epoch 454 | Train Loss: 0.000462 | Validation Loss: 0.000384\n",
      "Epoch 455 | Train Loss: 0.000479 | Validation Loss: 0.000359\n",
      "Epoch 456 | Train Loss: 0.000434 | Validation Loss: 0.000362\n",
      "Epoch 457 | Train Loss: 0.000453 | Validation Loss: 0.000379\n",
      "Epoch 458 | Train Loss: 0.000429 | Validation Loss: 0.000400\n",
      "Epoch 459 | Train Loss: 0.000447 | Validation Loss: 0.000342\n",
      "Epoch 460 | Train Loss: 0.000429 | Validation Loss: 0.000375\n",
      "Epoch 461 | Train Loss: 0.000490 | Validation Loss: 0.000378\n",
      "Epoch 462 | Train Loss: 0.000489 | Validation Loss: 0.000405\n",
      "Epoch 463 | Train Loss: 0.000457 | Validation Loss: 0.000359\n",
      "Epoch 464 | Train Loss: 0.000437 | Validation Loss: 0.000367\n",
      "Epoch 465 | Train Loss: 0.000480 | Validation Loss: 0.000410\n",
      "Epoch 466 | Train Loss: 0.000508 | Validation Loss: 0.000380\n",
      "Epoch 467 | Train Loss: 0.000530 | Validation Loss: 0.000379\n",
      "Epoch 468 | Train Loss: 0.000423 | Validation Loss: 0.000396\n",
      "Epoch 469 | Train Loss: 0.000452 | Validation Loss: 0.000341\n",
      "Epoch 470 | Train Loss: 0.000415 | Validation Loss: 0.000359\n",
      "Epoch 471 | Train Loss: 0.000391 | Validation Loss: 0.000326\n",
      "Epoch 472 | Train Loss: 0.000416 | Validation Loss: 0.000352\n",
      "Epoch 473 | Train Loss: 0.000453 | Validation Loss: 0.000341\n",
      "Epoch 474 | Train Loss: 0.000393 | Validation Loss: 0.000380\n",
      "Epoch 475 | Train Loss: 0.000616 | Validation Loss: 0.000464\n",
      "Epoch 476 | Train Loss: 0.000558 | Validation Loss: 0.000517\n",
      "Epoch 477 | Train Loss: 0.000543 | Validation Loss: 0.000412\n",
      "Epoch 478 | Train Loss: 0.000504 | Validation Loss: 0.000356\n",
      "Epoch 479 | Train Loss: 0.000429 | Validation Loss: 0.000384\n",
      "Epoch 480 | Train Loss: 0.000407 | Validation Loss: 0.000371\n",
      "Epoch 481 | Train Loss: 0.000466 | Validation Loss: 0.000393\n",
      "Epoch 482 | Train Loss: 0.000431 | Validation Loss: 0.000402\n",
      "Epoch 483 | Train Loss: 0.000433 | Validation Loss: 0.000339\n",
      "Epoch 484 | Train Loss: 0.000436 | Validation Loss: 0.000342\n",
      "Epoch 485 | Train Loss: 0.000449 | Validation Loss: 0.000349\n",
      "Epoch 486 | Train Loss: 0.000405 | Validation Loss: 0.000331\n",
      "saved!\n",
      "Epoch 487 | Train Loss: 0.000406 | Validation Loss: 0.000310\n",
      "Epoch 488 | Train Loss: 0.000471 | Validation Loss: 0.000375\n",
      "Epoch 489 | Train Loss: 0.000470 | Validation Loss: 0.000319\n",
      "Epoch 490 | Train Loss: 0.000412 | Validation Loss: 0.000348\n",
      "Epoch 491 | Train Loss: 0.000409 | Validation Loss: 0.000454\n",
      "Epoch 492 | Train Loss: 0.000456 | Validation Loss: 0.000344\n",
      "Epoch 493 | Train Loss: 0.000401 | Validation Loss: 0.000365\n",
      "Epoch 494 | Train Loss: 0.000512 | Validation Loss: 0.000346\n",
      "Epoch 495 | Train Loss: 0.000485 | Validation Loss: 0.000546\n",
      "Epoch 496 | Train Loss: 0.000508 | Validation Loss: 0.000368\n",
      "Epoch 497 | Train Loss: 0.000436 | Validation Loss: 0.000364\n",
      "Epoch 498 | Train Loss: 0.000487 | Validation Loss: 0.000388\n",
      "Epoch 499 | Train Loss: 0.000449 | Validation Loss: 0.000349\n",
      "Epoch 500 | Train Loss: 0.000484 | Validation Loss: 0.000416\n",
      "Epoch 501 | Train Loss: 0.000474 | Validation Loss: 0.000380\n",
      "Epoch 502 | Train Loss: 0.000457 | Validation Loss: 0.000385\n",
      "Epoch 503 | Train Loss: 0.000438 | Validation Loss: 0.000353\n",
      "Epoch 504 | Train Loss: 0.000389 | Validation Loss: 0.000379\n",
      "Epoch 505 | Train Loss: 0.000436 | Validation Loss: 0.000340\n",
      "Epoch 506 | Train Loss: 0.000433 | Validation Loss: 0.000341\n",
      "Epoch 507 | Train Loss: 0.000547 | Validation Loss: 0.000435\n",
      "Epoch 508 | Train Loss: 0.000468 | Validation Loss: 0.000343\n",
      "Epoch 509 | Train Loss: 0.000459 | Validation Loss: 0.000353\n",
      "Epoch 510 | Train Loss: 0.000423 | Validation Loss: 0.000376\n",
      "Epoch 511 | Train Loss: 0.000444 | Validation Loss: 0.000344\n",
      "Epoch 512 | Train Loss: 0.000428 | Validation Loss: 0.000382\n",
      "Epoch 513 | Train Loss: 0.000414 | Validation Loss: 0.000382\n",
      "Epoch 514 | Train Loss: 0.000433 | Validation Loss: 0.000350\n",
      "Epoch 515 | Train Loss: 0.000431 | Validation Loss: 0.000396\n",
      "Epoch 516 | Train Loss: 0.000413 | Validation Loss: 0.000337\n",
      "Epoch 517 | Train Loss: 0.000399 | Validation Loss: 0.000353\n",
      "Epoch 518 | Train Loss: 0.000412 | Validation Loss: 0.000328\n",
      "Epoch 519 | Train Loss: 0.000399 | Validation Loss: 0.000370\n",
      "Epoch 520 | Train Loss: 0.000416 | Validation Loss: 0.000342\n",
      "Epoch 521 | Train Loss: 0.000450 | Validation Loss: 0.000367\n",
      "Epoch 522 | Train Loss: 0.000426 | Validation Loss: 0.000443\n",
      "Epoch 523 | Train Loss: 0.000420 | Validation Loss: 0.000403\n",
      "Epoch 524 | Train Loss: 0.000406 | Validation Loss: 0.000419\n",
      "Epoch 525 | Train Loss: 0.000449 | Validation Loss: 0.000388\n",
      "Epoch 526 | Train Loss: 0.000430 | Validation Loss: 0.000361\n",
      "Epoch 527 | Train Loss: 0.000459 | Validation Loss: 0.000436\n",
      "Epoch 528 | Train Loss: 0.000503 | Validation Loss: 0.000369\n",
      "Epoch 529 | Train Loss: 0.000517 | Validation Loss: 0.000411\n",
      "Epoch 530 | Train Loss: 0.000423 | Validation Loss: 0.000345\n",
      "Epoch 531 | Train Loss: 0.000450 | Validation Loss: 0.000392\n",
      "Epoch 532 | Train Loss: 0.000449 | Validation Loss: 0.000339\n",
      "Epoch 533 | Train Loss: 0.000387 | Validation Loss: 0.000351\n",
      "Epoch 534 | Train Loss: 0.000432 | Validation Loss: 0.000344\n",
      "Epoch 535 | Train Loss: 0.000426 | Validation Loss: 0.000336\n",
      "Epoch 536 | Train Loss: 0.000388 | Validation Loss: 0.000366\n",
      "Epoch 537 | Train Loss: 0.000411 | Validation Loss: 0.000386\n",
      "Epoch 538 | Train Loss: 0.000403 | Validation Loss: 0.000378\n",
      "Epoch 539 | Train Loss: 0.000391 | Validation Loss: 0.000426\n",
      "Epoch 540 | Train Loss: 0.000422 | Validation Loss: 0.000426\n",
      "Epoch 541 | Train Loss: 0.000397 | Validation Loss: 0.000374\n",
      "Epoch 542 | Train Loss: 0.000413 | Validation Loss: 0.000330\n",
      "Epoch 543 | Train Loss: 0.000559 | Validation Loss: 0.000396\n",
      "Epoch 544 | Train Loss: 0.000517 | Validation Loss: 0.000391\n",
      "Epoch 545 | Train Loss: 0.000463 | Validation Loss: 0.000359\n",
      "Epoch 546 | Train Loss: 0.000432 | Validation Loss: 0.000356\n",
      "Epoch 547 | Train Loss: 0.000415 | Validation Loss: 0.000349\n",
      "Epoch 548 | Train Loss: 0.000403 | Validation Loss: 0.000410\n",
      "Epoch 549 | Train Loss: 0.000484 | Validation Loss: 0.000410\n",
      "Epoch 550 | Train Loss: 0.000514 | Validation Loss: 0.000445\n",
      "Epoch 551 | Train Loss: 0.000414 | Validation Loss: 0.000355\n",
      "Epoch 552 | Train Loss: 0.000412 | Validation Loss: 0.000363\n",
      "Epoch 553 | Train Loss: 0.000428 | Validation Loss: 0.000335\n",
      "Epoch 554 | Train Loss: 0.000425 | Validation Loss: 0.000440\n",
      "Epoch 555 | Train Loss: 0.000413 | Validation Loss: 0.000354\n",
      "Epoch 556 | Train Loss: 0.000512 | Validation Loss: 0.000392\n",
      "Epoch 557 | Train Loss: 0.000449 | Validation Loss: 0.000361\n",
      "Epoch 558 | Train Loss: 0.000428 | Validation Loss: 0.000391\n",
      "Epoch 559 | Train Loss: 0.000438 | Validation Loss: 0.000318\n",
      "Epoch 560 | Train Loss: 0.000482 | Validation Loss: 0.000445\n",
      "Epoch 561 | Train Loss: 0.000382 | Validation Loss: 0.000374\n",
      "Epoch 562 | Train Loss: 0.000389 | Validation Loss: 0.000324\n",
      "Epoch 563 | Train Loss: 0.000392 | Validation Loss: 0.000338\n",
      "Epoch 564 | Train Loss: 0.000467 | Validation Loss: 0.000338\n",
      "Epoch 565 | Train Loss: 0.000410 | Validation Loss: 0.000371\n",
      "Epoch 566 | Train Loss: 0.000389 | Validation Loss: 0.000353\n",
      "Epoch 567 | Train Loss: 0.000409 | Validation Loss: 0.000373\n",
      "Epoch 568 | Train Loss: 0.000455 | Validation Loss: 0.000341\n",
      "Epoch 569 | Train Loss: 0.000505 | Validation Loss: 0.000370\n",
      "Epoch 570 | Train Loss: 0.000451 | Validation Loss: 0.000390\n",
      "Epoch 571 | Train Loss: 0.000496 | Validation Loss: 0.000398\n",
      "Epoch 572 | Train Loss: 0.000496 | Validation Loss: 0.000401\n",
      "Epoch 573 | Train Loss: 0.000442 | Validation Loss: 0.000407\n",
      "Epoch 574 | Train Loss: 0.000397 | Validation Loss: 0.000339\n",
      "Epoch 575 | Train Loss: 0.000392 | Validation Loss: 0.000396\n",
      "Epoch 576 | Train Loss: 0.000462 | Validation Loss: 0.000364\n",
      "Epoch 577 | Train Loss: 0.000434 | Validation Loss: 0.000369\n",
      "Epoch 578 | Train Loss: 0.000462 | Validation Loss: 0.000400\n",
      "Epoch 579 | Train Loss: 0.000422 | Validation Loss: 0.000400\n",
      "Epoch 580 | Train Loss: 0.000509 | Validation Loss: 0.000402\n",
      "Epoch 581 | Train Loss: 0.000433 | Validation Loss: 0.000354\n",
      "Epoch 582 | Train Loss: 0.000447 | Validation Loss: 0.000395\n",
      "Epoch 583 | Train Loss: 0.000447 | Validation Loss: 0.000329\n",
      "Epoch 584 | Train Loss: 0.000403 | Validation Loss: 0.000350\n",
      "Epoch 585 | Train Loss: 0.000445 | Validation Loss: 0.000405\n",
      "Epoch 586 | Train Loss: 0.000507 | Validation Loss: 0.000480\n",
      "Epoch 587 | Train Loss: 0.000459 | Validation Loss: 0.000337\n",
      "Epoch 588 | Train Loss: 0.000426 | Validation Loss: 0.000390\n",
      "Epoch 589 | Train Loss: 0.000483 | Validation Loss: 0.000378\n",
      "Epoch 590 | Train Loss: 0.000499 | Validation Loss: 0.000379\n",
      "Epoch 591 | Train Loss: 0.000382 | Validation Loss: 0.000381\n",
      "Epoch 592 | Train Loss: 0.000430 | Validation Loss: 0.000358\n",
      "Epoch 593 | Train Loss: 0.000452 | Validation Loss: 0.000389\n",
      "Epoch 594 | Train Loss: 0.000446 | Validation Loss: 0.000387\n",
      "Epoch 595 | Train Loss: 0.000449 | Validation Loss: 0.000376\n",
      "Epoch 596 | Train Loss: 0.000411 | Validation Loss: 0.000361\n",
      "Epoch 597 | Train Loss: 0.000414 | Validation Loss: 0.000348\n",
      "Epoch 598 | Train Loss: 0.000434 | Validation Loss: 0.000357\n",
      "Epoch 599 | Train Loss: 0.000424 | Validation Loss: 0.000390\n",
      "Epoch 600 | Train Loss: 0.000414 | Validation Loss: 0.000415\n",
      "Epoch 601 | Train Loss: 0.000391 | Validation Loss: 0.000333\n",
      "Epoch 602 | Train Loss: 0.000414 | Validation Loss: 0.000321\n",
      "Epoch 603 | Train Loss: 0.000445 | Validation Loss: 0.000338\n",
      "Epoch 604 | Train Loss: 0.000457 | Validation Loss: 0.000403\n",
      "Epoch 605 | Train Loss: 0.000449 | Validation Loss: 0.000385\n",
      "Epoch 606 | Train Loss: 0.000397 | Validation Loss: 0.000335\n",
      "Epoch 607 | Train Loss: 0.000461 | Validation Loss: 0.000614\n",
      "Epoch 608 | Train Loss: 0.000474 | Validation Loss: 0.000399\n",
      "Epoch 609 | Train Loss: 0.000407 | Validation Loss: 0.000405\n",
      "Epoch 610 | Train Loss: 0.000400 | Validation Loss: 0.000397\n",
      "Epoch 611 | Train Loss: 0.000439 | Validation Loss: 0.000387\n",
      "Epoch 612 | Train Loss: 0.000463 | Validation Loss: 0.000357\n",
      "Epoch 613 | Train Loss: 0.000435 | Validation Loss: 0.000380\n",
      "Epoch 614 | Train Loss: 0.000428 | Validation Loss: 0.000381\n",
      "Epoch 615 | Train Loss: 0.000421 | Validation Loss: 0.000361\n",
      "Epoch 616 | Train Loss: 0.000389 | Validation Loss: 0.000368\n",
      "Epoch 617 | Train Loss: 0.000390 | Validation Loss: 0.000388\n",
      "Epoch 618 | Train Loss: 0.000455 | Validation Loss: 0.000437\n",
      "Epoch 619 | Train Loss: 0.000465 | Validation Loss: 0.000442\n",
      "Epoch 620 | Train Loss: 0.000425 | Validation Loss: 0.000350\n",
      "Epoch 621 | Train Loss: 0.000401 | Validation Loss: 0.000400\n",
      "Epoch 622 | Train Loss: 0.000437 | Validation Loss: 0.000344\n",
      "Epoch 623 | Train Loss: 0.000396 | Validation Loss: 0.000352\n",
      "Epoch 624 | Train Loss: 0.000397 | Validation Loss: 0.000347\n",
      "Epoch 625 | Train Loss: 0.000420 | Validation Loss: 0.000390\n",
      "Epoch 626 | Train Loss: 0.000433 | Validation Loss: 0.000410\n",
      "Epoch 627 | Train Loss: 0.000460 | Validation Loss: 0.000343\n",
      "Epoch 628 | Train Loss: 0.000403 | Validation Loss: 0.000358\n",
      "Epoch 629 | Train Loss: 0.000426 | Validation Loss: 0.000414\n",
      "Epoch 630 | Train Loss: 0.000384 | Validation Loss: 0.000340\n",
      "Epoch 631 | Train Loss: 0.000405 | Validation Loss: 0.000404\n",
      "Epoch 632 | Train Loss: 0.000424 | Validation Loss: 0.000360\n",
      "Epoch 633 | Train Loss: 0.000409 | Validation Loss: 0.000378\n",
      "Epoch 634 | Train Loss: 0.000427 | Validation Loss: 0.000364\n",
      "Epoch 635 | Train Loss: 0.000397 | Validation Loss: 0.000359\n",
      "Epoch 636 | Train Loss: 0.000378 | Validation Loss: 0.000341\n",
      "Epoch 637 | Train Loss: 0.000400 | Validation Loss: 0.000418\n",
      "Epoch 638 | Train Loss: 0.000413 | Validation Loss: 0.000414\n",
      "Epoch 639 | Train Loss: 0.000401 | Validation Loss: 0.000368\n",
      "Epoch 640 | Train Loss: 0.000383 | Validation Loss: 0.000339\n",
      "Epoch 641 | Train Loss: 0.000408 | Validation Loss: 0.000468\n",
      "Epoch 642 | Train Loss: 0.000424 | Validation Loss: 0.000342\n",
      "Epoch 643 | Train Loss: 0.000433 | Validation Loss: 0.000375\n",
      "Epoch 644 | Train Loss: 0.000445 | Validation Loss: 0.000383\n",
      "Epoch 645 | Train Loss: 0.000400 | Validation Loss: 0.000345\n",
      "Epoch 646 | Train Loss: 0.000397 | Validation Loss: 0.000401\n",
      "Epoch 647 | Train Loss: 0.000424 | Validation Loss: 0.000345\n",
      "Epoch 648 | Train Loss: 0.000416 | Validation Loss: 0.000383\n",
      "Epoch 649 | Train Loss: 0.000496 | Validation Loss: 0.000410\n",
      "Epoch 650 | Train Loss: 0.000432 | Validation Loss: 0.000345\n",
      "Epoch 651 | Train Loss: 0.000438 | Validation Loss: 0.000366\n",
      "Epoch 652 | Train Loss: 0.000435 | Validation Loss: 0.000407\n",
      "Epoch 653 | Train Loss: 0.000471 | Validation Loss: 0.000339\n",
      "Epoch 654 | Train Loss: 0.000365 | Validation Loss: 0.000339\n",
      "Epoch 655 | Train Loss: 0.000395 | Validation Loss: 0.000333\n",
      "Epoch 656 | Train Loss: 0.000407 | Validation Loss: 0.000387\n",
      "Epoch 657 | Train Loss: 0.000370 | Validation Loss: 0.000334\n",
      "Epoch 658 | Train Loss: 0.000476 | Validation Loss: 0.000382\n",
      "Epoch 659 | Train Loss: 0.000414 | Validation Loss: 0.000339\n",
      "Epoch 660 | Train Loss: 0.000389 | Validation Loss: 0.000312\n",
      "Epoch 661 | Train Loss: 0.000424 | Validation Loss: 0.000387\n",
      "Epoch 662 | Train Loss: 0.000479 | Validation Loss: 0.000362\n",
      "Epoch 663 | Train Loss: 0.000506 | Validation Loss: 0.000375\n",
      "Epoch 664 | Train Loss: 0.000411 | Validation Loss: 0.000339\n",
      "Epoch 665 | Train Loss: 0.000502 | Validation Loss: 0.000352\n",
      "Epoch 666 | Train Loss: 0.000407 | Validation Loss: 0.000368\n",
      "Epoch 667 | Train Loss: 0.000417 | Validation Loss: 0.000363\n",
      "Epoch 668 | Train Loss: 0.000403 | Validation Loss: 0.000435\n",
      "Epoch 669 | Train Loss: 0.000452 | Validation Loss: 0.000428\n",
      "Epoch 670 | Train Loss: 0.000446 | Validation Loss: 0.000370\n",
      "Epoch 671 | Train Loss: 0.000489 | Validation Loss: 0.000356\n",
      "Epoch 672 | Train Loss: 0.000439 | Validation Loss: 0.000356\n",
      "Epoch 673 | Train Loss: 0.000408 | Validation Loss: 0.000366\n",
      "Epoch 674 | Train Loss: 0.000377 | Validation Loss: 0.000335\n",
      "Epoch 675 | Train Loss: 0.000396 | Validation Loss: 0.000348\n",
      "Epoch 676 | Train Loss: 0.000416 | Validation Loss: 0.000330\n",
      "Epoch 677 | Train Loss: 0.000418 | Validation Loss: 0.000358\n",
      "Epoch 678 | Train Loss: 0.000382 | Validation Loss: 0.000376\n",
      "Epoch 679 | Train Loss: 0.000432 | Validation Loss: 0.000389\n",
      "Epoch 680 | Train Loss: 0.000544 | Validation Loss: 0.000454\n",
      "Epoch 681 | Train Loss: 0.000453 | Validation Loss: 0.000366\n",
      "Epoch 682 | Train Loss: 0.000415 | Validation Loss: 0.000334\n",
      "Epoch 683 | Train Loss: 0.000378 | Validation Loss: 0.000328\n",
      "Epoch 684 | Train Loss: 0.000400 | Validation Loss: 0.000358\n",
      "Epoch 685 | Train Loss: 0.000427 | Validation Loss: 0.000376\n",
      "Epoch 686 | Train Loss: 0.000387 | Validation Loss: 0.000358\n",
      "Epoch 687 | Train Loss: 0.000412 | Validation Loss: 0.000380\n",
      "Epoch 688 | Train Loss: 0.000391 | Validation Loss: 0.000399\n",
      "Epoch 689 | Train Loss: 0.000399 | Validation Loss: 0.000336\n",
      "Epoch 690 | Train Loss: 0.000410 | Validation Loss: 0.000380\n",
      "Epoch 691 | Train Loss: 0.000435 | Validation Loss: 0.000341\n",
      "Epoch 692 | Train Loss: 0.000374 | Validation Loss: 0.000393\n",
      "Epoch 693 | Train Loss: 0.000449 | Validation Loss: 0.000355\n",
      "Epoch 694 | Train Loss: 0.000400 | Validation Loss: 0.000341\n",
      "Epoch 695 | Train Loss: 0.000453 | Validation Loss: 0.000390\n",
      "Epoch 696 | Train Loss: 0.000413 | Validation Loss: 0.000358\n",
      "Epoch 697 | Train Loss: 0.000372 | Validation Loss: 0.000346\n",
      "Epoch 698 | Train Loss: 0.000386 | Validation Loss: 0.000339\n",
      "Epoch 699 | Train Loss: 0.000594 | Validation Loss: 0.000353\n",
      "Epoch 700 | Train Loss: 0.000428 | Validation Loss: 0.000341\n",
      "Epoch 701 | Train Loss: 0.000381 | Validation Loss: 0.000350\n",
      "Epoch 702 | Train Loss: 0.000621 | Validation Loss: 0.000464\n",
      "Epoch 703 | Train Loss: 0.000435 | Validation Loss: 0.000341\n",
      "Epoch 704 | Train Loss: 0.000421 | Validation Loss: 0.000341\n",
      "Epoch 705 | Train Loss: 0.000432 | Validation Loss: 0.000327\n",
      "Epoch 706 | Train Loss: 0.000385 | Validation Loss: 0.000371\n",
      "Epoch 707 | Train Loss: 0.000401 | Validation Loss: 0.000408\n",
      "Epoch 708 | Train Loss: 0.000401 | Validation Loss: 0.000396\n",
      "Epoch 709 | Train Loss: 0.000410 | Validation Loss: 0.000364\n",
      "Epoch 710 | Train Loss: 0.000506 | Validation Loss: 0.000360\n",
      "Epoch 711 | Train Loss: 0.000564 | Validation Loss: 0.000376\n",
      "Epoch 712 | Train Loss: 0.000442 | Validation Loss: 0.000460\n",
      "Epoch 713 | Train Loss: 0.000461 | Validation Loss: 0.000374\n",
      "Epoch 714 | Train Loss: 0.000386 | Validation Loss: 0.000345\n",
      "Epoch 715 | Train Loss: 0.000412 | Validation Loss: 0.000343\n",
      "Epoch 716 | Train Loss: 0.000437 | Validation Loss: 0.000386\n",
      "Epoch 717 | Train Loss: 0.000410 | Validation Loss: 0.000389\n",
      "Epoch 718 | Train Loss: 0.000479 | Validation Loss: 0.000375\n",
      "Epoch 719 | Train Loss: 0.000523 | Validation Loss: 0.000401\n",
      "Epoch 720 | Train Loss: 0.000477 | Validation Loss: 0.000353\n",
      "Epoch 721 | Train Loss: 0.000491 | Validation Loss: 0.000363\n",
      "Epoch 722 | Train Loss: 0.000398 | Validation Loss: 0.000344\n",
      "Epoch 723 | Train Loss: 0.000457 | Validation Loss: 0.000368\n",
      "Epoch 724 | Train Loss: 0.000415 | Validation Loss: 0.000341\n",
      "Epoch 725 | Train Loss: 0.000396 | Validation Loss: 0.000342\n",
      "Epoch 726 | Train Loss: 0.000404 | Validation Loss: 0.000387\n",
      "Epoch 727 | Train Loss: 0.000418 | Validation Loss: 0.000404\n",
      "Epoch 728 | Train Loss: 0.000432 | Validation Loss: 0.000377\n",
      "Epoch 729 | Train Loss: 0.000400 | Validation Loss: 0.000383\n",
      "Epoch 730 | Train Loss: 0.000382 | Validation Loss: 0.000395\n",
      "Epoch 731 | Train Loss: 0.000397 | Validation Loss: 0.000370\n",
      "Epoch 732 | Train Loss: 0.000458 | Validation Loss: 0.000414\n",
      "Epoch 733 | Train Loss: 0.000404 | Validation Loss: 0.000387\n",
      "Epoch 734 | Train Loss: 0.000405 | Validation Loss: 0.000373\n",
      "Epoch 735 | Train Loss: 0.000444 | Validation Loss: 0.000392\n",
      "Epoch 736 | Train Loss: 0.000405 | Validation Loss: 0.000397\n",
      "Epoch 737 | Train Loss: 0.000425 | Validation Loss: 0.000313\n",
      "Epoch 738 | Train Loss: 0.000386 | Validation Loss: 0.000390\n",
      "Epoch 739 | Train Loss: 0.000411 | Validation Loss: 0.000346\n",
      "Epoch 740 | Train Loss: 0.000397 | Validation Loss: 0.000338\n",
      "Epoch 741 | Train Loss: 0.000368 | Validation Loss: 0.000346\n",
      "Epoch 742 | Train Loss: 0.000417 | Validation Loss: 0.000343\n",
      "Epoch 743 | Train Loss: 0.000404 | Validation Loss: 0.000384\n",
      "Epoch 744 | Train Loss: 0.000480 | Validation Loss: 0.000385\n",
      "Epoch 745 | Train Loss: 0.000433 | Validation Loss: 0.000376\n",
      "Epoch 746 | Train Loss: 0.000377 | Validation Loss: 0.000356\n",
      "Epoch 747 | Train Loss: 0.000358 | Validation Loss: 0.000380\n",
      "Epoch 748 | Train Loss: 0.000396 | Validation Loss: 0.000341\n",
      "Epoch 749 | Train Loss: 0.000367 | Validation Loss: 0.000355\n",
      "Epoch 750 | Train Loss: 0.000441 | Validation Loss: 0.000384\n",
      "Epoch 751 | Train Loss: 0.000374 | Validation Loss: 0.000372\n",
      "Epoch 752 | Train Loss: 0.000405 | Validation Loss: 0.000337\n",
      "Epoch 753 | Train Loss: 0.000424 | Validation Loss: 0.000430\n",
      "Epoch 754 | Train Loss: 0.000406 | Validation Loss: 0.000339\n",
      "Epoch 755 | Train Loss: 0.000458 | Validation Loss: 0.000369\n",
      "Epoch 756 | Train Loss: 0.000399 | Validation Loss: 0.000311\n",
      "Epoch 757 | Train Loss: 0.000393 | Validation Loss: 0.000382\n",
      "Epoch 758 | Train Loss: 0.000431 | Validation Loss: 0.000375\n",
      "Epoch 759 | Train Loss: 0.000397 | Validation Loss: 0.000322\n",
      "Epoch 760 | Train Loss: 0.000371 | Validation Loss: 0.000343\n",
      "Epoch 761 | Train Loss: 0.000459 | Validation Loss: 0.000362\n",
      "Epoch 762 | Train Loss: 0.000450 | Validation Loss: 0.000334\n",
      "Epoch 763 | Train Loss: 0.000495 | Validation Loss: 0.000481\n",
      "Epoch 764 | Train Loss: 0.000421 | Validation Loss: 0.000326\n",
      "Epoch 765 | Train Loss: 0.000390 | Validation Loss: 0.000397\n",
      "Epoch 766 | Train Loss: 0.000372 | Validation Loss: 0.000417\n",
      "Epoch 767 | Train Loss: 0.000384 | Validation Loss: 0.000335\n",
      "Epoch 768 | Train Loss: 0.000486 | Validation Loss: 0.000386\n",
      "Epoch 769 | Train Loss: 0.000398 | Validation Loss: 0.000354\n",
      "Epoch 770 | Train Loss: 0.000423 | Validation Loss: 0.000351\n",
      "Epoch 771 | Train Loss: 0.000412 | Validation Loss: 0.000362\n",
      "Epoch 772 | Train Loss: 0.000397 | Validation Loss: 0.000368\n",
      "Epoch 773 | Train Loss: 0.000386 | Validation Loss: 0.000359\n",
      "Epoch 774 | Train Loss: 0.000379 | Validation Loss: 0.000369\n",
      "Epoch 775 | Train Loss: 0.000383 | Validation Loss: 0.000395\n",
      "Epoch 776 | Train Loss: 0.000452 | Validation Loss: 0.000361\n",
      "Epoch 777 | Train Loss: 0.000373 | Validation Loss: 0.000327\n",
      "Epoch 778 | Train Loss: 0.000563 | Validation Loss: 0.000385\n",
      "Epoch 779 | Train Loss: 0.000427 | Validation Loss: 0.000361\n",
      "Epoch 780 | Train Loss: 0.000378 | Validation Loss: 0.000369\n",
      "Epoch 781 | Train Loss: 0.000431 | Validation Loss: 0.000359\n",
      "Epoch 782 | Train Loss: 0.000433 | Validation Loss: 0.000352\n",
      "Epoch 783 | Train Loss: 0.000409 | Validation Loss: 0.000363\n",
      "Epoch 784 | Train Loss: 0.000392 | Validation Loss: 0.000396\n",
      "Epoch 785 | Train Loss: 0.000395 | Validation Loss: 0.000358\n",
      "Epoch 786 | Train Loss: 0.000453 | Validation Loss: 0.000346\n",
      "Epoch 787 | Train Loss: 0.000406 | Validation Loss: 0.000327\n",
      "Epoch 788 | Train Loss: 0.000376 | Validation Loss: 0.000351\n",
      "Epoch 789 | Train Loss: 0.000401 | Validation Loss: 0.000355\n",
      "Epoch 790 | Train Loss: 0.000425 | Validation Loss: 0.000347\n",
      "Epoch 791 | Train Loss: 0.000395 | Validation Loss: 0.000366\n",
      "Epoch 792 | Train Loss: 0.000403 | Validation Loss: 0.000386\n",
      "Epoch 793 | Train Loss: 0.000607 | Validation Loss: 0.000423\n",
      "Epoch 794 | Train Loss: 0.000428 | Validation Loss: 0.000329\n",
      "Epoch 795 | Train Loss: 0.000408 | Validation Loss: 0.000350\n",
      "Epoch 796 | Train Loss: 0.000390 | Validation Loss: 0.000385\n",
      "Epoch 797 | Train Loss: 0.000412 | Validation Loss: 0.000349\n",
      "Epoch 798 | Train Loss: 0.000399 | Validation Loss: 0.000315\n",
      "Epoch 799 | Train Loss: 0.000382 | Validation Loss: 0.000343\n",
      "Epoch 800 | Train Loss: 0.000383 | Validation Loss: 0.000330\n",
      "Epoch 801 | Train Loss: 0.000375 | Validation Loss: 0.000351\n",
      "Epoch 802 | Train Loss: 0.000446 | Validation Loss: 0.000360\n",
      "Epoch 803 | Train Loss: 0.000382 | Validation Loss: 0.000370\n",
      "Epoch 804 | Train Loss: 0.000461 | Validation Loss: 0.000360\n",
      "Epoch 805 | Train Loss: 0.000412 | Validation Loss: 0.000346\n",
      "Epoch 806 | Train Loss: 0.000409 | Validation Loss: 0.000370\n",
      "Epoch 807 | Train Loss: 0.000368 | Validation Loss: 0.000331\n",
      "Epoch 808 | Train Loss: 0.000428 | Validation Loss: 0.000347\n",
      "Epoch 809 | Train Loss: 0.000419 | Validation Loss: 0.000411\n",
      "Epoch 810 | Train Loss: 0.000378 | Validation Loss: 0.000369\n",
      "Epoch 811 | Train Loss: 0.000427 | Validation Loss: 0.000395\n",
      "Epoch 812 | Train Loss: 0.000382 | Validation Loss: 0.000339\n",
      "Epoch 813 | Train Loss: 0.000374 | Validation Loss: 0.000423\n",
      "Epoch 814 | Train Loss: 0.000390 | Validation Loss: 0.000341\n",
      "Epoch 815 | Train Loss: 0.000382 | Validation Loss: 0.000356\n",
      "Epoch 816 | Train Loss: 0.000380 | Validation Loss: 0.000334\n",
      "Epoch 817 | Train Loss: 0.000400 | Validation Loss: 0.000406\n",
      "Epoch 818 | Train Loss: 0.000371 | Validation Loss: 0.000427\n",
      "Epoch 819 | Train Loss: 0.000368 | Validation Loss: 0.000366\n",
      "Epoch 820 | Train Loss: 0.000384 | Validation Loss: 0.000367\n",
      "Epoch 821 | Train Loss: 0.000398 | Validation Loss: 0.000349\n",
      "Epoch 822 | Train Loss: 0.000380 | Validation Loss: 0.000354\n",
      "Epoch 823 | Train Loss: 0.000400 | Validation Loss: 0.000366\n",
      "Epoch 824 | Train Loss: 0.000366 | Validation Loss: 0.000387\n",
      "Epoch 825 | Train Loss: 0.000373 | Validation Loss: 0.000365\n",
      "Epoch 826 | Train Loss: 0.000400 | Validation Loss: 0.000375\n",
      "Epoch 827 | Train Loss: 0.000371 | Validation Loss: 0.000345\n",
      "Epoch 828 | Train Loss: 0.000419 | Validation Loss: 0.000347\n",
      "Epoch 829 | Train Loss: 0.000382 | Validation Loss: 0.000333\n",
      "Epoch 830 | Train Loss: 0.000357 | Validation Loss: 0.000318\n",
      "Epoch 831 | Train Loss: 0.000383 | Validation Loss: 0.000394\n",
      "Epoch 832 | Train Loss: 0.000363 | Validation Loss: 0.000358\n",
      "Epoch 833 | Train Loss: 0.000467 | Validation Loss: 0.000361\n",
      "Epoch 834 | Train Loss: 0.000495 | Validation Loss: 0.000352\n",
      "Epoch 835 | Train Loss: 0.000433 | Validation Loss: 0.000375\n",
      "Epoch 836 | Train Loss: 0.000402 | Validation Loss: 0.000349\n",
      "Epoch 837 | Train Loss: 0.000396 | Validation Loss: 0.000370\n",
      "Epoch 838 | Train Loss: 0.000350 | Validation Loss: 0.000326\n",
      "Epoch 839 | Train Loss: 0.000349 | Validation Loss: 0.000389\n",
      "Epoch 840 | Train Loss: 0.000439 | Validation Loss: 0.000385\n",
      "Epoch 841 | Train Loss: 0.000413 | Validation Loss: 0.000424\n",
      "Epoch 842 | Train Loss: 0.000415 | Validation Loss: 0.000352\n",
      "Epoch 843 | Train Loss: 0.000395 | Validation Loss: 0.000361\n",
      "Epoch 844 | Train Loss: 0.000412 | Validation Loss: 0.000384\n",
      "Epoch 845 | Train Loss: 0.000436 | Validation Loss: 0.000373\n",
      "Epoch 846 | Train Loss: 0.000369 | Validation Loss: 0.000377\n",
      "Epoch 847 | Train Loss: 0.000372 | Validation Loss: 0.000409\n",
      "Epoch 848 | Train Loss: 0.000361 | Validation Loss: 0.000383\n",
      "Epoch 849 | Train Loss: 0.000377 | Validation Loss: 0.000371\n",
      "Epoch 850 | Train Loss: 0.000375 | Validation Loss: 0.000338\n",
      "Epoch 851 | Train Loss: 0.000368 | Validation Loss: 0.000334\n",
      "Epoch 852 | Train Loss: 0.000377 | Validation Loss: 0.000322\n",
      "Epoch 853 | Train Loss: 0.000351 | Validation Loss: 0.000370\n",
      "Epoch 854 | Train Loss: 0.000391 | Validation Loss: 0.000381\n",
      "Epoch 855 | Train Loss: 0.000426 | Validation Loss: 0.000426\n",
      "Epoch 856 | Train Loss: 0.000382 | Validation Loss: 0.000355\n",
      "Epoch 857 | Train Loss: 0.000445 | Validation Loss: 0.000338\n",
      "Epoch 858 | Train Loss: 0.000418 | Validation Loss: 0.000368\n",
      "Epoch 859 | Train Loss: 0.000369 | Validation Loss: 0.000337\n",
      "Epoch 860 | Train Loss: 0.000358 | Validation Loss: 0.000327\n",
      "Epoch 861 | Train Loss: 0.000378 | Validation Loss: 0.000439\n",
      "Epoch 862 | Train Loss: 0.000383 | Validation Loss: 0.000407\n",
      "Epoch 863 | Train Loss: 0.000424 | Validation Loss: 0.000342\n",
      "Epoch 864 | Train Loss: 0.000369 | Validation Loss: 0.000356\n",
      "Epoch 865 | Train Loss: 0.000399 | Validation Loss: 0.000382\n",
      "Epoch 866 | Train Loss: 0.000387 | Validation Loss: 0.000353\n",
      "Epoch 867 | Train Loss: 0.000381 | Validation Loss: 0.000372\n",
      "Epoch 868 | Train Loss: 0.000374 | Validation Loss: 0.000373\n",
      "Epoch 869 | Train Loss: 0.000412 | Validation Loss: 0.000346\n",
      "Epoch 870 | Train Loss: 0.000488 | Validation Loss: 0.000412\n",
      "Epoch 871 | Train Loss: 0.000412 | Validation Loss: 0.000358\n",
      "Epoch 872 | Train Loss: 0.000380 | Validation Loss: 0.000381\n",
      "Epoch 873 | Train Loss: 0.000399 | Validation Loss: 0.000353\n",
      "Epoch 874 | Train Loss: 0.000406 | Validation Loss: 0.000365\n",
      "Epoch 875 | Train Loss: 0.000431 | Validation Loss: 0.000338\n",
      "Epoch 876 | Train Loss: 0.000394 | Validation Loss: 0.000332\n",
      "Epoch 877 | Train Loss: 0.000355 | Validation Loss: 0.000312\n",
      "Epoch 878 | Train Loss: 0.000407 | Validation Loss: 0.000334\n",
      "Epoch 879 | Train Loss: 0.000391 | Validation Loss: 0.000406\n",
      "Epoch 880 | Train Loss: 0.000375 | Validation Loss: 0.000343\n",
      "Epoch 881 | Train Loss: 0.000375 | Validation Loss: 0.000405\n",
      "saved!\n",
      "Epoch 882 | Train Loss: 0.000359 | Validation Loss: 0.000304\n",
      "Epoch 883 | Train Loss: 0.000397 | Validation Loss: 0.000400\n",
      "Epoch 884 | Train Loss: 0.000399 | Validation Loss: 0.000345\n",
      "Epoch 885 | Train Loss: 0.000378 | Validation Loss: 0.000348\n",
      "Epoch 886 | Train Loss: 0.000376 | Validation Loss: 0.000311\n",
      "Epoch 887 | Train Loss: 0.000449 | Validation Loss: 0.000424\n",
      "Epoch 888 | Train Loss: 0.000376 | Validation Loss: 0.000334\n",
      "Epoch 889 | Train Loss: 0.000377 | Validation Loss: 0.000328\n",
      "Epoch 890 | Train Loss: 0.000372 | Validation Loss: 0.000343\n",
      "Epoch 891 | Train Loss: 0.000427 | Validation Loss: 0.000357\n",
      "Epoch 892 | Train Loss: 0.000383 | Validation Loss: 0.000366\n",
      "Epoch 893 | Train Loss: 0.000404 | Validation Loss: 0.000394\n",
      "Epoch 894 | Train Loss: 0.000465 | Validation Loss: 0.000507\n",
      "Epoch 895 | Train Loss: 0.000516 | Validation Loss: 0.000383\n",
      "Epoch 896 | Train Loss: 0.000470 | Validation Loss: 0.000519\n",
      "Epoch 897 | Train Loss: 0.000497 | Validation Loss: 0.000434\n",
      "Epoch 898 | Train Loss: 0.000438 | Validation Loss: 0.000365\n",
      "Epoch 899 | Train Loss: 0.000416 | Validation Loss: 0.000345\n",
      "Epoch 900 | Train Loss: 0.000419 | Validation Loss: 0.000455\n",
      "Epoch 901 | Train Loss: 0.000402 | Validation Loss: 0.000388\n",
      "Epoch 902 | Train Loss: 0.000360 | Validation Loss: 0.000346\n",
      "Epoch 903 | Train Loss: 0.000421 | Validation Loss: 0.000350\n",
      "Epoch 904 | Train Loss: 0.000372 | Validation Loss: 0.000335\n",
      "Epoch 905 | Train Loss: 0.000389 | Validation Loss: 0.000345\n",
      "Epoch 906 | Train Loss: 0.000408 | Validation Loss: 0.000361\n",
      "Epoch 907 | Train Loss: 0.000409 | Validation Loss: 0.000412\n",
      "Epoch 908 | Train Loss: 0.000427 | Validation Loss: 0.000402\n",
      "Epoch 909 | Train Loss: 0.000359 | Validation Loss: 0.000317\n",
      "Epoch 910 | Train Loss: 0.000390 | Validation Loss: 0.000390\n",
      "Epoch 911 | Train Loss: 0.000367 | Validation Loss: 0.000387\n",
      "Epoch 912 | Train Loss: 0.000386 | Validation Loss: 0.000401\n",
      "Epoch 913 | Train Loss: 0.000392 | Validation Loss: 0.000415\n",
      "Epoch 914 | Train Loss: 0.000381 | Validation Loss: 0.000369\n",
      "Epoch 915 | Train Loss: 0.000365 | Validation Loss: 0.000404\n",
      "Epoch 916 | Train Loss: 0.000374 | Validation Loss: 0.000393\n",
      "Epoch 917 | Train Loss: 0.000369 | Validation Loss: 0.000342\n",
      "Epoch 918 | Train Loss: 0.000348 | Validation Loss: 0.000344\n",
      "Epoch 919 | Train Loss: 0.000361 | Validation Loss: 0.000355\n",
      "Epoch 920 | Train Loss: 0.000352 | Validation Loss: 0.000377\n",
      "Epoch 921 | Train Loss: 0.000364 | Validation Loss: 0.000400\n",
      "Epoch 922 | Train Loss: 0.000374 | Validation Loss: 0.000367\n",
      "Epoch 923 | Train Loss: 0.000362 | Validation Loss: 0.000327\n",
      "Epoch 924 | Train Loss: 0.000377 | Validation Loss: 0.000364\n",
      "Epoch 925 | Train Loss: 0.000367 | Validation Loss: 0.000365\n",
      "Epoch 926 | Train Loss: 0.000456 | Validation Loss: 0.000416\n",
      "Epoch 927 | Train Loss: 0.000488 | Validation Loss: 0.000346\n",
      "Epoch 928 | Train Loss: 0.000444 | Validation Loss: 0.000372\n",
      "Epoch 929 | Train Loss: 0.000369 | Validation Loss: 0.000358\n",
      "Epoch 930 | Train Loss: 0.000481 | Validation Loss: 0.000496\n",
      "Epoch 931 | Train Loss: 0.000520 | Validation Loss: 0.000451\n",
      "Epoch 932 | Train Loss: 0.000422 | Validation Loss: 0.000344\n",
      "Epoch 933 | Train Loss: 0.000419 | Validation Loss: 0.000325\n",
      "Epoch 934 | Train Loss: 0.000406 | Validation Loss: 0.000456\n",
      "Epoch 935 | Train Loss: 0.000404 | Validation Loss: 0.000343\n",
      "Epoch 936 | Train Loss: 0.000424 | Validation Loss: 0.000405\n",
      "Epoch 937 | Train Loss: 0.000396 | Validation Loss: 0.000382\n",
      "Epoch 938 | Train Loss: 0.000356 | Validation Loss: 0.000350\n",
      "Epoch 939 | Train Loss: 0.000413 | Validation Loss: 0.000385\n",
      "Epoch 940 | Train Loss: 0.000412 | Validation Loss: 0.000362\n",
      "Epoch 941 | Train Loss: 0.000449 | Validation Loss: 0.000344\n",
      "Epoch 942 | Train Loss: 0.000405 | Validation Loss: 0.000392\n",
      "Epoch 943 | Train Loss: 0.000409 | Validation Loss: 0.000375\n",
      "Epoch 944 | Train Loss: 0.000373 | Validation Loss: 0.000343\n",
      "Epoch 945 | Train Loss: 0.000474 | Validation Loss: 0.000379\n",
      "Epoch 946 | Train Loss: 0.000423 | Validation Loss: 0.000365\n",
      "Epoch 947 | Train Loss: 0.000381 | Validation Loss: 0.000346\n",
      "Epoch 948 | Train Loss: 0.000395 | Validation Loss: 0.000322\n",
      "Epoch 949 | Train Loss: 0.000380 | Validation Loss: 0.000345\n",
      "Epoch 950 | Train Loss: 0.000391 | Validation Loss: 0.000360\n",
      "Epoch 951 | Train Loss: 0.000392 | Validation Loss: 0.000329\n",
      "Epoch 952 | Train Loss: 0.000376 | Validation Loss: 0.000347\n",
      "Epoch 953 | Train Loss: 0.000409 | Validation Loss: 0.000396\n",
      "Epoch 954 | Train Loss: 0.000383 | Validation Loss: 0.000358\n",
      "Epoch 955 | Train Loss: 0.000352 | Validation Loss: 0.000323\n",
      "Epoch 956 | Train Loss: 0.000365 | Validation Loss: 0.000345\n",
      "Epoch 957 | Train Loss: 0.000444 | Validation Loss: 0.000366\n",
      "Epoch 958 | Train Loss: 0.000481 | Validation Loss: 0.000432\n",
      "Epoch 959 | Train Loss: 0.000417 | Validation Loss: 0.000340\n",
      "Epoch 960 | Train Loss: 0.000450 | Validation Loss: 0.000437\n",
      "Epoch 961 | Train Loss: 0.000386 | Validation Loss: 0.000380\n",
      "Epoch 962 | Train Loss: 0.000500 | Validation Loss: 0.000389\n",
      "Epoch 963 | Train Loss: 0.000453 | Validation Loss: 0.000408\n",
      "Epoch 964 | Train Loss: 0.000405 | Validation Loss: 0.000371\n",
      "Epoch 965 | Train Loss: 0.000389 | Validation Loss: 0.000377\n",
      "Epoch 966 | Train Loss: 0.000499 | Validation Loss: 0.000452\n",
      "Epoch 967 | Train Loss: 0.000433 | Validation Loss: 0.000380\n",
      "Epoch 968 | Train Loss: 0.000400 | Validation Loss: 0.000359\n",
      "Epoch 969 | Train Loss: 0.000371 | Validation Loss: 0.000360\n",
      "Epoch 970 | Train Loss: 0.000389 | Validation Loss: 0.000440\n",
      "Epoch 971 | Train Loss: 0.000386 | Validation Loss: 0.000314\n",
      "Epoch 972 | Train Loss: 0.000408 | Validation Loss: 0.000364\n",
      "Epoch 973 | Train Loss: 0.000415 | Validation Loss: 0.000335\n",
      "Epoch 974 | Train Loss: 0.000408 | Validation Loss: 0.000367\n",
      "Epoch 975 | Train Loss: 0.000370 | Validation Loss: 0.000339\n",
      "Epoch 976 | Train Loss: 0.000402 | Validation Loss: 0.000396\n",
      "Epoch 977 | Train Loss: 0.000406 | Validation Loss: 0.000375\n",
      "Epoch 978 | Train Loss: 0.000377 | Validation Loss: 0.000417\n",
      "Epoch 979 | Train Loss: 0.000425 | Validation Loss: 0.000399\n",
      "Epoch 980 | Train Loss: 0.000403 | Validation Loss: 0.000401\n",
      "Epoch 981 | Train Loss: 0.000381 | Validation Loss: 0.000372\n",
      "Epoch 982 | Train Loss: 0.000361 | Validation Loss: 0.000351\n",
      "Epoch 983 | Train Loss: 0.000385 | Validation Loss: 0.000357\n",
      "Epoch 984 | Train Loss: 0.000378 | Validation Loss: 0.000367\n",
      "Epoch 985 | Train Loss: 0.000397 | Validation Loss: 0.000378\n",
      "Epoch 986 | Train Loss: 0.000399 | Validation Loss: 0.000413\n",
      "Epoch 987 | Train Loss: 0.000349 | Validation Loss: 0.000352\n",
      "Epoch 988 | Train Loss: 0.000356 | Validation Loss: 0.000310\n",
      "Epoch 989 | Train Loss: 0.000372 | Validation Loss: 0.000370\n",
      "Epoch 990 | Train Loss: 0.000388 | Validation Loss: 0.000391\n",
      "Epoch 991 | Train Loss: 0.000385 | Validation Loss: 0.000380\n",
      "Epoch 992 | Train Loss: 0.000378 | Validation Loss: 0.000381\n",
      "Epoch 993 | Train Loss: 0.000428 | Validation Loss: 0.000395\n",
      "Epoch 994 | Train Loss: 0.000424 | Validation Loss: 0.000386\n",
      "Epoch 995 | Train Loss: 0.000390 | Validation Loss: 0.000331\n",
      "Epoch 996 | Train Loss: 0.000410 | Validation Loss: 0.000367\n",
      "Epoch 997 | Train Loss: 0.000415 | Validation Loss: 0.000414\n",
      "Epoch 998 | Train Loss: 0.000371 | Validation Loss: 0.000355\n",
      "Epoch 999 | Train Loss: 0.000386 | Validation Loss: 0.000412\n",
      "Epoch 1000 | Train Loss: 0.000394 | Validation Loss: 0.000370\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer = Trainer(model, optimizer, device='cuda' if torch.cuda.is_available() else 'cpu', max_patience=max_patience)\n",
    "trainer.train(train_loader, val_loader, epochs=epochs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa9b753a-ab61-49cc-9d9f-e9c495e4cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dosyasına kaydet\n",
    "train_losses = trainer.train_cost  # liste veya numpy array\n",
    "val_losses = trainer.val_cost\n",
    "\n",
    "np.savetxt(\"results/losses.csv\", \n",
    "           np.column_stack((train_losses, val_losses)),\n",
    "           delimiter=\",\", \n",
    "           header=\"train_loss,val_loss\", \n",
    "           comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5635678-c6be-447d-ae24-deda5d7f27dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26a3e0-dc1b-4854-9f7d-dbfc7049ac00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
