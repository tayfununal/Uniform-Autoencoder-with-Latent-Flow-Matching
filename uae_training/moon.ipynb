{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/arf/home/tunal/ondemand/PhD Thesis Starting/01_SON/Tik-4/Tez/01-Moon/02-UAE_for_Moon\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd()) # dosya yolunu ver\n",
    "%run ../Model.ipynb\n",
    "%run ../Dataset.ipynb\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7601ea5-682f-44a9-9b06-0f0efd697f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device='cpu', max_patience=20):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "        self.max_patience = max_patience\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience = 0\n",
    "\n",
    "        self.val_cost = []\n",
    "        self.train_cost = []\n",
    "    \n",
    "    def train(self, train_loader, val_loader=None, epochs=10, print_every=1, name=None):\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for x, x_, _ in train_loader:  # label kullanılmıyor\n",
    "                x = x.to(self.device)\n",
    "                x_ = x_.to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    z_hat_ = self.model.encoder(x_)\n",
    "                z_hat, x_hat = self.model(x)\n",
    "                \n",
    "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,2))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Validation (opsiyonel)\n",
    "            if val_loader:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                \n",
    "                # Early stopping kontrolü\n",
    "                if val_loss < self.best_val_loss:\n",
    "                    print('saved!')\n",
    "                    torch.save(self.model, name + '.model')\n",
    "                    self.best_val_loss = val_loss\n",
    "                    self.patience = 0\n",
    "        \n",
    "                else:\n",
    "                    self.patience = self.patience + 1\n",
    "        \n",
    "                if self.patience > self.max_patience:\n",
    "                    break\n",
    "                    \n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch:3d} | Train Loss: {total_loss / len(train_loader):.6f} | Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "            self.val_cost.append(val_loss)\n",
    "            self.train_cost.append(total_loss / len(train_loader))\n",
    "            \n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, x_, _ in val_loader:\n",
    "                x = x.to(self.device)\n",
    "                x_ = x_.to(self.device)\n",
    "                \n",
    "                z_hat_ = self.model.encoder(x_)\n",
    "                z_hat, x_hat = self.model(x)\n",
    "                \n",
    "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,2))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        #print(f\"→ Validation Loss: {avg_loss:.6f}\")\n",
    "        self.model.train()\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6982ede7-a39e-433c-90a9-8850b1f73c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transform\n",
    "class NoiseTransform:\n",
    "    \"\"\"Add some noise.\"\"\"\n",
    "\n",
    "    def __init__(self, split_ratio=0.001, dim=2):\n",
    "\n",
    "        self.normal_dist = split_ratio*np.random.randn(dim,)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "      return x + self.normal_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters & Settings\n",
    "\n",
    "dataset_size = 1000\n",
    "batch_size = 500\n",
    "lr = 0.003\n",
    "\n",
    "epochs = 1000\n",
    "max_patience = 1000\n",
    "\n",
    "split_ratio = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07121ef5-db8d-4bd5-a327-a8646f696809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = MoonDataset(mode='train', n_samples=dataset_size, transform=NoiseTransform(split_ratio))\n",
    "val_dataset = MoonDataset(mode='val', n_samples=dataset_size, transform=NoiseTransform(split_ratio))\n",
    "test_dataset = MoonDataset(mode='test', n_samples=dataset_size)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6287fa6-ab49-4e49-a79c-911863e72ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"results\" klasörünü oluştur (zaten varsa hata vermez)\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9abf79c-a874-418b-b46f-aaa2fe649d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "name = 'results/UAE_Moon'\n",
    "\n",
    "model = To_Uniform(\n",
    "                 input_dim=2,\n",
    "                 latent_dim=1,\n",
    "                 output_dim=2,\n",
    "                 encoder_hidden=64,\n",
    "                 decoder_hidden=64,\n",
    "                 encoder_act=nn.SiLU,\n",
    "                 decoder_act=nn.SiLU,\n",
    "                 final_encoder_act=nn.Sigmoid,\n",
    "                 final_decoder_act=nn.Sigmoid\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n",
      "Epoch   1 | Train Loss: 0.051078 | Validation Loss: 0.059366\n",
      "saved!\n",
      "Epoch   2 | Train Loss: 0.047724 | Validation Loss: 0.057508\n",
      "Epoch   3 | Train Loss: 0.045106 | Validation Loss: 0.057808\n",
      "saved!\n",
      "Epoch   4 | Train Loss: 0.041582 | Validation Loss: 0.057149\n",
      "saved!\n",
      "Epoch   5 | Train Loss: 0.038984 | Validation Loss: 0.052496\n",
      "saved!\n",
      "Epoch   6 | Train Loss: 0.033478 | Validation Loss: 0.043985\n",
      "saved!\n",
      "Epoch   7 | Train Loss: 0.027888 | Validation Loss: 0.033794\n",
      "saved!\n",
      "Epoch   8 | Train Loss: 0.021733 | Validation Loss: 0.023639\n",
      "saved!\n",
      "Epoch   9 | Train Loss: 0.017512 | Validation Loss: 0.018411\n",
      "Epoch  10 | Train Loss: 0.016387 | Validation Loss: 0.018657\n",
      "Epoch  11 | Train Loss: 0.016498 | Validation Loss: 0.018697\n",
      "saved!\n",
      "Epoch  12 | Train Loss: 0.015521 | Validation Loss: 0.015951\n",
      "saved!\n",
      "Epoch  13 | Train Loss: 0.013241 | Validation Loss: 0.013766\n",
      "Epoch  14 | Train Loss: 0.012110 | Validation Loss: 0.015366\n",
      "Epoch  15 | Train Loss: 0.011253 | Validation Loss: 0.015148\n",
      "Epoch  16 | Train Loss: 0.010998 | Validation Loss: 0.014024\n",
      "saved!\n",
      "Epoch  17 | Train Loss: 0.010842 | Validation Loss: 0.013312\n",
      "saved!\n",
      "Epoch  18 | Train Loss: 0.011411 | Validation Loss: 0.012100\n",
      "saved!\n",
      "Epoch  19 | Train Loss: 0.010776 | Validation Loss: 0.010991\n",
      "saved!\n",
      "Epoch  20 | Train Loss: 0.010515 | Validation Loss: 0.010577\n",
      "saved!\n",
      "Epoch  21 | Train Loss: 0.009602 | Validation Loss: 0.010210\n",
      "Epoch  22 | Train Loss: 0.009835 | Validation Loss: 0.010455\n",
      "saved!\n",
      "Epoch  23 | Train Loss: 0.010127 | Validation Loss: 0.010009\n",
      "Epoch  24 | Train Loss: 0.009542 | Validation Loss: 0.010507\n",
      "saved!\n",
      "Epoch  25 | Train Loss: 0.009994 | Validation Loss: 0.009899\n",
      "Epoch  26 | Train Loss: 0.009635 | Validation Loss: 0.010259\n",
      "saved!\n",
      "Epoch  27 | Train Loss: 0.010007 | Validation Loss: 0.009789\n",
      "Epoch  28 | Train Loss: 0.009662 | Validation Loss: 0.010199\n",
      "Epoch  29 | Train Loss: 0.009827 | Validation Loss: 0.009860\n",
      "Epoch  30 | Train Loss: 0.009916 | Validation Loss: 0.009989\n",
      "Epoch  31 | Train Loss: 0.009418 | Validation Loss: 0.009956\n",
      "Epoch  32 | Train Loss: 0.009421 | Validation Loss: 0.009963\n",
      "Epoch  33 | Train Loss: 0.009378 | Validation Loss: 0.010039\n",
      "Epoch  34 | Train Loss: 0.009622 | Validation Loss: 0.009975\n",
      "Epoch  35 | Train Loss: 0.008983 | Validation Loss: 0.010060\n",
      "Epoch  36 | Train Loss: 0.009525 | Validation Loss: 0.009964\n",
      "Epoch  37 | Train Loss: 0.009464 | Validation Loss: 0.009889\n",
      "Epoch  38 | Train Loss: 0.009716 | Validation Loss: 0.009955\n",
      "Epoch  39 | Train Loss: 0.009401 | Validation Loss: 0.009937\n",
      "Epoch  40 | Train Loss: 0.009677 | Validation Loss: 0.009812\n",
      "Epoch  41 | Train Loss: 0.009736 | Validation Loss: 0.009995\n",
      "Epoch  42 | Train Loss: 0.010361 | Validation Loss: 0.009874\n",
      "Epoch  43 | Train Loss: 0.009304 | Validation Loss: 0.010315\n",
      "Epoch  44 | Train Loss: 0.010182 | Validation Loss: 0.009957\n",
      "Epoch  45 | Train Loss: 0.009499 | Validation Loss: 0.010092\n",
      "Epoch  46 | Train Loss: 0.009559 | Validation Loss: 0.009925\n",
      "Epoch  47 | Train Loss: 0.009504 | Validation Loss: 0.009934\n",
      "Epoch  48 | Train Loss: 0.009870 | Validation Loss: 0.010111\n",
      "Epoch  49 | Train Loss: 0.009351 | Validation Loss: 0.009880\n",
      "Epoch  50 | Train Loss: 0.009588 | Validation Loss: 0.009983\n",
      "saved!\n",
      "Epoch  51 | Train Loss: 0.009486 | Validation Loss: 0.009784\n",
      "Epoch  52 | Train Loss: 0.009515 | Validation Loss: 0.009823\n",
      "saved!\n",
      "Epoch  53 | Train Loss: 0.009475 | Validation Loss: 0.009648\n",
      "Epoch  54 | Train Loss: 0.009225 | Validation Loss: 0.009821\n",
      "saved!\n",
      "Epoch  55 | Train Loss: 0.009504 | Validation Loss: 0.009599\n",
      "Epoch  56 | Train Loss: 0.009599 | Validation Loss: 0.009903\n",
      "Epoch  57 | Train Loss: 0.009620 | Validation Loss: 0.009604\n",
      "Epoch  58 | Train Loss: 0.009150 | Validation Loss: 0.009792\n",
      "Epoch  59 | Train Loss: 0.009428 | Validation Loss: 0.009672\n",
      "Epoch  60 | Train Loss: 0.009459 | Validation Loss: 0.009647\n",
      "Epoch  61 | Train Loss: 0.009359 | Validation Loss: 0.009654\n",
      "saved!\n",
      "Epoch  62 | Train Loss: 0.009514 | Validation Loss: 0.009510\n",
      "saved!\n",
      "Epoch  63 | Train Loss: 0.009310 | Validation Loss: 0.009474\n",
      "Epoch  64 | Train Loss: 0.009207 | Validation Loss: 0.009577\n",
      "saved!\n",
      "Epoch  65 | Train Loss: 0.009074 | Validation Loss: 0.009433\n",
      "Epoch  66 | Train Loss: 0.009259 | Validation Loss: 0.009477\n",
      "Epoch  67 | Train Loss: 0.008938 | Validation Loss: 0.009433\n",
      "saved!\n",
      "Epoch  68 | Train Loss: 0.009739 | Validation Loss: 0.009377\n",
      "Epoch  69 | Train Loss: 0.008871 | Validation Loss: 0.009388\n",
      "saved!\n",
      "Epoch  70 | Train Loss: 0.009010 | Validation Loss: 0.009333\n",
      "saved!\n",
      "Epoch  71 | Train Loss: 0.009569 | Validation Loss: 0.009332\n",
      "saved!\n",
      "Epoch  72 | Train Loss: 0.009632 | Validation Loss: 0.009317\n",
      "saved!\n",
      "Epoch  73 | Train Loss: 0.008895 | Validation Loss: 0.009216\n",
      "saved!\n",
      "Epoch  74 | Train Loss: 0.009121 | Validation Loss: 0.009193\n",
      "saved!\n",
      "Epoch  75 | Train Loss: 0.009206 | Validation Loss: 0.009076\n",
      "Epoch  76 | Train Loss: 0.008831 | Validation Loss: 0.009297\n",
      "saved!\n",
      "Epoch  77 | Train Loss: 0.009429 | Validation Loss: 0.009036\n",
      "saved!\n",
      "Epoch  78 | Train Loss: 0.008545 | Validation Loss: 0.009011\n",
      "saved!\n",
      "Epoch  79 | Train Loss: 0.008712 | Validation Loss: 0.008881\n",
      "Epoch  80 | Train Loss: 0.008835 | Validation Loss: 0.008898\n",
      "Epoch  81 | Train Loss: 0.008882 | Validation Loss: 0.008925\n",
      "Epoch  82 | Train Loss: 0.008806 | Validation Loss: 0.008954\n",
      "saved!\n",
      "Epoch  83 | Train Loss: 0.008814 | Validation Loss: 0.008773\n",
      "saved!\n",
      "Epoch  84 | Train Loss: 0.009432 | Validation Loss: 0.008743\n",
      "Epoch  85 | Train Loss: 0.008925 | Validation Loss: 0.008772\n",
      "Epoch  86 | Train Loss: 0.008856 | Validation Loss: 0.008752\n",
      "saved!\n",
      "Epoch  87 | Train Loss: 0.008489 | Validation Loss: 0.008470\n",
      "saved!\n",
      "Epoch  88 | Train Loss: 0.008386 | Validation Loss: 0.008406\n",
      "Epoch  89 | Train Loss: 0.008655 | Validation Loss: 0.008445\n",
      "saved!\n",
      "Epoch  90 | Train Loss: 0.008369 | Validation Loss: 0.008332\n",
      "saved!\n",
      "Epoch  91 | Train Loss: 0.008071 | Validation Loss: 0.008282\n",
      "saved!\n",
      "Epoch  92 | Train Loss: 0.008509 | Validation Loss: 0.008194\n",
      "Epoch  93 | Train Loss: 0.008179 | Validation Loss: 0.008199\n",
      "saved!\n",
      "Epoch  94 | Train Loss: 0.008050 | Validation Loss: 0.008156\n",
      "saved!\n",
      "Epoch  95 | Train Loss: 0.008192 | Validation Loss: 0.008095\n",
      "saved!\n",
      "Epoch  96 | Train Loss: 0.008376 | Validation Loss: 0.008060\n",
      "Epoch  97 | Train Loss: 0.008189 | Validation Loss: 0.008392\n",
      "Epoch  98 | Train Loss: 0.008457 | Validation Loss: 0.008211\n",
      "Epoch  99 | Train Loss: 0.009040 | Validation Loss: 0.008217\n",
      "Epoch 100 | Train Loss: 0.008777 | Validation Loss: 0.008517\n",
      "Epoch 101 | Train Loss: 0.008192 | Validation Loss: 0.008237\n",
      "Epoch 102 | Train Loss: 0.008462 | Validation Loss: 0.008135\n",
      "Epoch 103 | Train Loss: 0.008749 | Validation Loss: 0.009046\n",
      "Epoch 104 | Train Loss: 0.008704 | Validation Loss: 0.008229\n",
      "Epoch 105 | Train Loss: 0.009147 | Validation Loss: 0.008693\n",
      "Epoch 106 | Train Loss: 0.012034 | Validation Loss: 0.009201\n",
      "Epoch 107 | Train Loss: 0.008985 | Validation Loss: 0.010521\n",
      "Epoch 108 | Train Loss: 0.010017 | Validation Loss: 0.008194\n",
      "Epoch 109 | Train Loss: 0.008444 | Validation Loss: 0.009321\n",
      "Epoch 110 | Train Loss: 0.009376 | Validation Loss: 0.008468\n",
      "Epoch 111 | Train Loss: 0.008725 | Validation Loss: 0.009969\n",
      "Epoch 112 | Train Loss: 0.009362 | Validation Loss: 0.008645\n",
      "Epoch 113 | Train Loss: 0.008240 | Validation Loss: 0.008321\n",
      "Epoch 114 | Train Loss: 0.008005 | Validation Loss: 0.008239\n",
      "Epoch 115 | Train Loss: 0.007881 | Validation Loss: 0.008230\n",
      "saved!\n",
      "Epoch 116 | Train Loss: 0.007849 | Validation Loss: 0.007986\n",
      "saved!\n",
      "Epoch 117 | Train Loss: 0.008200 | Validation Loss: 0.007963\n",
      "Epoch 118 | Train Loss: 0.007892 | Validation Loss: 0.008086\n",
      "Epoch 119 | Train Loss: 0.008058 | Validation Loss: 0.008069\n",
      "saved!\n",
      "Epoch 120 | Train Loss: 0.007927 | Validation Loss: 0.007847\n",
      "saved!\n",
      "Epoch 121 | Train Loss: 0.007812 | Validation Loss: 0.007757\n",
      "Epoch 122 | Train Loss: 0.007663 | Validation Loss: 0.007829\n",
      "saved!\n",
      "Epoch 123 | Train Loss: 0.007810 | Validation Loss: 0.007713\n",
      "saved!\n",
      "Epoch 124 | Train Loss: 0.007718 | Validation Loss: 0.007683\n",
      "saved!\n",
      "Epoch 125 | Train Loss: 0.007554 | Validation Loss: 0.007648\n",
      "Epoch 126 | Train Loss: 0.007504 | Validation Loss: 0.007652\n",
      "Epoch 127 | Train Loss: 0.007700 | Validation Loss: 0.007653\n",
      "Epoch 128 | Train Loss: 0.007668 | Validation Loss: 0.007677\n",
      "saved!\n",
      "Epoch 129 | Train Loss: 0.007796 | Validation Loss: 0.007640\n",
      "Epoch 130 | Train Loss: 0.007637 | Validation Loss: 0.007812\n",
      "saved!\n",
      "Epoch 131 | Train Loss: 0.007643 | Validation Loss: 0.007571\n",
      "Epoch 132 | Train Loss: 0.007662 | Validation Loss: 0.007782\n",
      "Epoch 133 | Train Loss: 0.007754 | Validation Loss: 0.007692\n",
      "Epoch 134 | Train Loss: 0.008246 | Validation Loss: 0.007840\n",
      "Epoch 135 | Train Loss: 0.007551 | Validation Loss: 0.007741\n",
      "Epoch 136 | Train Loss: 0.007588 | Validation Loss: 0.007695\n",
      "saved!\n",
      "Epoch 137 | Train Loss: 0.007650 | Validation Loss: 0.007515\n",
      "Epoch 138 | Train Loss: 0.007456 | Validation Loss: 0.007804\n",
      "Epoch 139 | Train Loss: 0.007728 | Validation Loss: 0.007531\n",
      "Epoch 140 | Train Loss: 0.007648 | Validation Loss: 0.007903\n",
      "saved!\n",
      "Epoch 141 | Train Loss: 0.007360 | Validation Loss: 0.007479\n",
      "Epoch 142 | Train Loss: 0.007304 | Validation Loss: 0.007531\n",
      "saved!\n",
      "Epoch 143 | Train Loss: 0.007238 | Validation Loss: 0.007370\n",
      "Epoch 144 | Train Loss: 0.007268 | Validation Loss: 0.007471\n",
      "saved!\n",
      "Epoch 145 | Train Loss: 0.007379 | Validation Loss: 0.007369\n",
      "saved!\n",
      "Epoch 146 | Train Loss: 0.007456 | Validation Loss: 0.007366\n",
      "Epoch 147 | Train Loss: 0.007028 | Validation Loss: 0.007430\n",
      "Epoch 148 | Train Loss: 0.007326 | Validation Loss: 0.007388\n",
      "saved!\n",
      "Epoch 149 | Train Loss: 0.007373 | Validation Loss: 0.007275\n",
      "Epoch 150 | Train Loss: 0.007061 | Validation Loss: 0.007361\n",
      "saved!\n",
      "Epoch 151 | Train Loss: 0.007271 | Validation Loss: 0.007147\n",
      "Epoch 152 | Train Loss: 0.006918 | Validation Loss: 0.007407\n",
      "Epoch 153 | Train Loss: 0.007323 | Validation Loss: 0.007179\n",
      "saved!\n",
      "Epoch 154 | Train Loss: 0.006958 | Validation Loss: 0.007022\n",
      "Epoch 155 | Train Loss: 0.006721 | Validation Loss: 0.007171\n",
      "saved!\n",
      "Epoch 156 | Train Loss: 0.007132 | Validation Loss: 0.007021\n",
      "saved!\n",
      "Epoch 157 | Train Loss: 0.006793 | Validation Loss: 0.007003\n",
      "saved!\n",
      "Epoch 158 | Train Loss: 0.006743 | Validation Loss: 0.006923\n",
      "saved!\n",
      "Epoch 159 | Train Loss: 0.006646 | Validation Loss: 0.006734\n",
      "saved!\n",
      "Epoch 160 | Train Loss: 0.006562 | Validation Loss: 0.006542\n",
      "saved!\n",
      "Epoch 161 | Train Loss: 0.006121 | Validation Loss: 0.006386\n",
      "Epoch 162 | Train Loss: 0.006205 | Validation Loss: 0.006389\n",
      "Epoch 163 | Train Loss: 0.005787 | Validation Loss: 0.006564\n",
      "saved!\n",
      "Epoch 164 | Train Loss: 0.006238 | Validation Loss: 0.006144\n",
      "saved!\n",
      "Epoch 165 | Train Loss: 0.005691 | Validation Loss: 0.005748\n",
      "saved!\n",
      "Epoch 166 | Train Loss: 0.006441 | Validation Loss: 0.005689\n",
      "Epoch 167 | Train Loss: 0.005225 | Validation Loss: 0.006218\n",
      "saved!\n",
      "Epoch 168 | Train Loss: 0.005087 | Validation Loss: 0.005437\n",
      "saved!\n",
      "Epoch 169 | Train Loss: 0.005430 | Validation Loss: 0.004783\n",
      "Epoch 170 | Train Loss: 0.004593 | Validation Loss: 0.004829\n",
      "Epoch 171 | Train Loss: 0.004362 | Validation Loss: 0.004851\n",
      "saved!\n",
      "Epoch 172 | Train Loss: 0.004175 | Validation Loss: 0.004374\n",
      "saved!\n",
      "Epoch 173 | Train Loss: 0.004041 | Validation Loss: 0.004059\n",
      "Epoch 174 | Train Loss: 0.003954 | Validation Loss: 0.004192\n",
      "saved!\n",
      "Epoch 175 | Train Loss: 0.003996 | Validation Loss: 0.003797\n",
      "saved!\n",
      "Epoch 176 | Train Loss: 0.003608 | Validation Loss: 0.003753\n",
      "saved!\n",
      "Epoch 177 | Train Loss: 0.003648 | Validation Loss: 0.003574\n",
      "Epoch 178 | Train Loss: 0.003383 | Validation Loss: 0.003837\n",
      "saved!\n",
      "Epoch 179 | Train Loss: 0.004003 | Validation Loss: 0.003433\n",
      "Epoch 180 | Train Loss: 0.003355 | Validation Loss: 0.003627\n",
      "saved!\n",
      "Epoch 181 | Train Loss: 0.003373 | Validation Loss: 0.003166\n",
      "saved!\n",
      "Epoch 182 | Train Loss: 0.003154 | Validation Loss: 0.002974\n",
      "Epoch 183 | Train Loss: 0.002970 | Validation Loss: 0.003949\n",
      "Epoch 184 | Train Loss: 0.003160 | Validation Loss: 0.003145\n",
      "saved!\n",
      "Epoch 185 | Train Loss: 0.003728 | Validation Loss: 0.002876\n",
      "Epoch 186 | Train Loss: 0.003990 | Validation Loss: 0.003890\n",
      "Epoch 187 | Train Loss: 0.003317 | Validation Loss: 0.003654\n",
      "Epoch 188 | Train Loss: 0.003995 | Validation Loss: 0.003926\n",
      "Epoch 189 | Train Loss: 0.003273 | Validation Loss: 0.004646\n",
      "Epoch 190 | Train Loss: 0.003294 | Validation Loss: 0.003522\n",
      "Epoch 191 | Train Loss: 0.003079 | Validation Loss: 0.003471\n",
      "Epoch 192 | Train Loss: 0.003185 | Validation Loss: 0.003617\n",
      "Epoch 193 | Train Loss: 0.003723 | Validation Loss: 0.004027\n",
      "Epoch 194 | Train Loss: 0.003117 | Validation Loss: 0.003626\n",
      "Epoch 195 | Train Loss: 0.003020 | Validation Loss: 0.003119\n",
      "Epoch 196 | Train Loss: 0.002725 | Validation Loss: 0.003261\n",
      "Epoch 197 | Train Loss: 0.003974 | Validation Loss: 0.003624\n",
      "Epoch 198 | Train Loss: 0.002790 | Validation Loss: 0.003895\n",
      "Epoch 199 | Train Loss: 0.003473 | Validation Loss: 0.003031\n",
      "Epoch 200 | Train Loss: 0.002917 | Validation Loss: 0.003458\n",
      "Epoch 201 | Train Loss: 0.002906 | Validation Loss: 0.003363\n",
      "Epoch 202 | Train Loss: 0.002699 | Validation Loss: 0.003012\n",
      "Epoch 203 | Train Loss: 0.004391 | Validation Loss: 0.003013\n",
      "Epoch 204 | Train Loss: 0.004472 | Validation Loss: 0.003650\n",
      "Epoch 205 | Train Loss: 0.002919 | Validation Loss: 0.003443\n",
      "Epoch 206 | Train Loss: 0.003190 | Validation Loss: 0.003430\n",
      "Epoch 207 | Train Loss: 0.003182 | Validation Loss: 0.003641\n",
      "Epoch 208 | Train Loss: 0.003583 | Validation Loss: 0.003544\n",
      "Epoch 209 | Train Loss: 0.002768 | Validation Loss: 0.002975\n",
      "Epoch 210 | Train Loss: 0.002501 | Validation Loss: 0.003182\n",
      "Epoch 211 | Train Loss: 0.002829 | Validation Loss: 0.003360\n",
      "Epoch 212 | Train Loss: 0.002923 | Validation Loss: 0.003160\n",
      "Epoch 213 | Train Loss: 0.002943 | Validation Loss: 0.003008\n",
      "Epoch 214 | Train Loss: 0.002513 | Validation Loss: 0.003404\n",
      "saved!\n",
      "Epoch 215 | Train Loss: 0.003384 | Validation Loss: 0.002841\n",
      "Epoch 216 | Train Loss: 0.002485 | Validation Loss: 0.003624\n",
      "Epoch 217 | Train Loss: 0.003323 | Validation Loss: 0.003383\n",
      "Epoch 218 | Train Loss: 0.002663 | Validation Loss: 0.002972\n",
      "Epoch 219 | Train Loss: 0.003064 | Validation Loss: 0.003105\n",
      "Epoch 220 | Train Loss: 0.002771 | Validation Loss: 0.004108\n",
      "Epoch 221 | Train Loss: 0.003250 | Validation Loss: 0.003021\n",
      "Epoch 222 | Train Loss: 0.002689 | Validation Loss: 0.002850\n",
      "Epoch 223 | Train Loss: 0.002764 | Validation Loss: 0.003159\n",
      "Epoch 224 | Train Loss: 0.002864 | Validation Loss: 0.003603\n",
      "Epoch 225 | Train Loss: 0.002664 | Validation Loss: 0.003101\n",
      "Epoch 226 | Train Loss: 0.003111 | Validation Loss: 0.003262\n",
      "saved!\n",
      "Epoch 227 | Train Loss: 0.002594 | Validation Loss: 0.002712\n",
      "Epoch 228 | Train Loss: 0.002729 | Validation Loss: 0.003069\n",
      "Epoch 229 | Train Loss: 0.002694 | Validation Loss: 0.003661\n",
      "Epoch 230 | Train Loss: 0.004080 | Validation Loss: 0.003040\n",
      "Epoch 231 | Train Loss: 0.002441 | Validation Loss: 0.003175\n",
      "Epoch 232 | Train Loss: 0.002878 | Validation Loss: 0.003272\n",
      "Epoch 233 | Train Loss: 0.003371 | Validation Loss: 0.003748\n",
      "Epoch 234 | Train Loss: 0.003119 | Validation Loss: 0.003687\n",
      "Epoch 235 | Train Loss: 0.002996 | Validation Loss: 0.002728\n",
      "Epoch 236 | Train Loss: 0.003396 | Validation Loss: 0.003167\n",
      "Epoch 237 | Train Loss: 0.003143 | Validation Loss: 0.004020\n",
      "Epoch 238 | Train Loss: 0.003410 | Validation Loss: 0.002794\n",
      "Epoch 239 | Train Loss: 0.003385 | Validation Loss: 0.003076\n",
      "Epoch 240 | Train Loss: 0.002903 | Validation Loss: 0.003924\n",
      "Epoch 241 | Train Loss: 0.003165 | Validation Loss: 0.002796\n",
      "Epoch 242 | Train Loss: 0.003747 | Validation Loss: 0.003140\n",
      "Epoch 243 | Train Loss: 0.002750 | Validation Loss: 0.003386\n",
      "Epoch 244 | Train Loss: 0.002967 | Validation Loss: 0.003299\n",
      "saved!\n",
      "Epoch 245 | Train Loss: 0.002538 | Validation Loss: 0.002451\n",
      "Epoch 246 | Train Loss: 0.003096 | Validation Loss: 0.002894\n",
      "Epoch 247 | Train Loss: 0.003025 | Validation Loss: 0.003386\n",
      "Epoch 248 | Train Loss: 0.002654 | Validation Loss: 0.003175\n",
      "Epoch 249 | Train Loss: 0.002807 | Validation Loss: 0.002978\n",
      "Epoch 250 | Train Loss: 0.002915 | Validation Loss: 0.002662\n",
      "Epoch 251 | Train Loss: 0.002736 | Validation Loss: 0.002903\n",
      "Epoch 252 | Train Loss: 0.002469 | Validation Loss: 0.003442\n",
      "Epoch 253 | Train Loss: 0.002433 | Validation Loss: 0.002828\n",
      "Epoch 254 | Train Loss: 0.002313 | Validation Loss: 0.002766\n",
      "Epoch 255 | Train Loss: 0.003929 | Validation Loss: 0.003364\n",
      "Epoch 256 | Train Loss: 0.004309 | Validation Loss: 0.002885\n",
      "Epoch 257 | Train Loss: 0.002321 | Validation Loss: 0.003167\n",
      "Epoch 258 | Train Loss: 0.003606 | Validation Loss: 0.003235\n",
      "Epoch 259 | Train Loss: 0.003305 | Validation Loss: 0.003650\n",
      "Epoch 260 | Train Loss: 0.002473 | Validation Loss: 0.003144\n",
      "Epoch 261 | Train Loss: 0.002665 | Validation Loss: 0.002478\n",
      "Epoch 262 | Train Loss: 0.004114 | Validation Loss: 0.002808\n",
      "Epoch 263 | Train Loss: 0.002296 | Validation Loss: 0.003726\n",
      "Epoch 264 | Train Loss: 0.002450 | Validation Loss: 0.002713\n",
      "Epoch 265 | Train Loss: 0.002342 | Validation Loss: 0.002826\n",
      "Epoch 266 | Train Loss: 0.002499 | Validation Loss: 0.002826\n",
      "Epoch 267 | Train Loss: 0.002214 | Validation Loss: 0.002934\n",
      "Epoch 268 | Train Loss: 0.002599 | Validation Loss: 0.003124\n",
      "Epoch 269 | Train Loss: 0.002407 | Validation Loss: 0.002948\n",
      "saved!\n",
      "Epoch 270 | Train Loss: 0.002614 | Validation Loss: 0.002446\n",
      "Epoch 271 | Train Loss: 0.002105 | Validation Loss: 0.002989\n",
      "Epoch 272 | Train Loss: 0.002085 | Validation Loss: 0.003039\n",
      "saved!\n",
      "Epoch 273 | Train Loss: 0.002221 | Validation Loss: 0.002388\n",
      "Epoch 274 | Train Loss: 0.002086 | Validation Loss: 0.002446\n",
      "Epoch 275 | Train Loss: 0.002084 | Validation Loss: 0.002759\n",
      "Epoch 276 | Train Loss: 0.002504 | Validation Loss: 0.002662\n",
      "Epoch 277 | Train Loss: 0.002005 | Validation Loss: 0.002463\n",
      "Epoch 278 | Train Loss: 0.002665 | Validation Loss: 0.002576\n",
      "Epoch 279 | Train Loss: 0.003146 | Validation Loss: 0.003005\n",
      "Epoch 280 | Train Loss: 0.002130 | Validation Loss: 0.002599\n",
      "Epoch 281 | Train Loss: 0.002596 | Validation Loss: 0.002433\n",
      "Epoch 282 | Train Loss: 0.002081 | Validation Loss: 0.002792\n",
      "Epoch 283 | Train Loss: 0.002409 | Validation Loss: 0.002419\n",
      "Epoch 284 | Train Loss: 0.002067 | Validation Loss: 0.002503\n",
      "Epoch 285 | Train Loss: 0.002126 | Validation Loss: 0.002652\n",
      "Epoch 286 | Train Loss: 0.002067 | Validation Loss: 0.002686\n",
      "saved!\n",
      "Epoch 287 | Train Loss: 0.002089 | Validation Loss: 0.002353\n",
      "saved!\n",
      "Epoch 288 | Train Loss: 0.002208 | Validation Loss: 0.002295\n",
      "Epoch 289 | Train Loss: 0.002556 | Validation Loss: 0.002951\n",
      "Epoch 290 | Train Loss: 0.002198 | Validation Loss: 0.002662\n",
      "Epoch 291 | Train Loss: 0.002728 | Validation Loss: 0.002357\n",
      "Epoch 292 | Train Loss: 0.002190 | Validation Loss: 0.002742\n",
      "Epoch 293 | Train Loss: 0.002828 | Validation Loss: 0.002418\n",
      "Epoch 294 | Train Loss: 0.002452 | Validation Loss: 0.002893\n",
      "Epoch 295 | Train Loss: 0.002500 | Validation Loss: 0.002828\n",
      "Epoch 296 | Train Loss: 0.002424 | Validation Loss: 0.002616\n",
      "Epoch 297 | Train Loss: 0.002174 | Validation Loss: 0.002664\n",
      "Epoch 298 | Train Loss: 0.002264 | Validation Loss: 0.002393\n",
      "Epoch 299 | Train Loss: 0.002256 | Validation Loss: 0.002863\n",
      "Epoch 300 | Train Loss: 0.002412 | Validation Loss: 0.002609\n",
      "Epoch 301 | Train Loss: 0.002081 | Validation Loss: 0.002512\n",
      "Epoch 302 | Train Loss: 0.002141 | Validation Loss: 0.002710\n",
      "Epoch 303 | Train Loss: 0.002190 | Validation Loss: 0.002669\n",
      "Epoch 304 | Train Loss: 0.003411 | Validation Loss: 0.002696\n",
      "Epoch 305 | Train Loss: 0.002441 | Validation Loss: 0.003052\n",
      "Epoch 306 | Train Loss: 0.002574 | Validation Loss: 0.002525\n",
      "Epoch 307 | Train Loss: 0.002579 | Validation Loss: 0.002569\n",
      "Epoch 308 | Train Loss: 0.004053 | Validation Loss: 0.002815\n",
      "Epoch 309 | Train Loss: 0.002276 | Validation Loss: 0.003200\n",
      "Epoch 310 | Train Loss: 0.002197 | Validation Loss: 0.002583\n",
      "Epoch 311 | Train Loss: 0.002041 | Validation Loss: 0.002496\n",
      "Epoch 312 | Train Loss: 0.001936 | Validation Loss: 0.002603\n",
      "Epoch 313 | Train Loss: 0.002193 | Validation Loss: 0.002953\n",
      "Epoch 314 | Train Loss: 0.002103 | Validation Loss: 0.002434\n",
      "Epoch 315 | Train Loss: 0.002135 | Validation Loss: 0.002462\n",
      "Epoch 316 | Train Loss: 0.001992 | Validation Loss: 0.002620\n",
      "Epoch 317 | Train Loss: 0.002506 | Validation Loss: 0.002779\n",
      "Epoch 318 | Train Loss: 0.002378 | Validation Loss: 0.002753\n",
      "saved!\n",
      "Epoch 319 | Train Loss: 0.002007 | Validation Loss: 0.002283\n",
      "Epoch 320 | Train Loss: 0.002202 | Validation Loss: 0.002328\n",
      "Epoch 321 | Train Loss: 0.002115 | Validation Loss: 0.002573\n",
      "Epoch 322 | Train Loss: 0.001934 | Validation Loss: 0.002935\n",
      "Epoch 323 | Train Loss: 0.001990 | Validation Loss: 0.002589\n",
      "Epoch 324 | Train Loss: 0.002190 | Validation Loss: 0.002488\n",
      "Epoch 325 | Train Loss: 0.001981 | Validation Loss: 0.002416\n",
      "Epoch 326 | Train Loss: 0.001927 | Validation Loss: 0.002683\n",
      "Epoch 327 | Train Loss: 0.002202 | Validation Loss: 0.002585\n",
      "Epoch 328 | Train Loss: 0.001879 | Validation Loss: 0.002446\n",
      "Epoch 329 | Train Loss: 0.002356 | Validation Loss: 0.002465\n",
      "Epoch 330 | Train Loss: 0.001877 | Validation Loss: 0.002298\n",
      "saved!\n",
      "Epoch 331 | Train Loss: 0.001910 | Validation Loss: 0.002109\n",
      "Epoch 332 | Train Loss: 0.001941 | Validation Loss: 0.002232\n",
      "Epoch 333 | Train Loss: 0.002047 | Validation Loss: 0.002255\n",
      "Epoch 334 | Train Loss: 0.002052 | Validation Loss: 0.002394\n",
      "Epoch 335 | Train Loss: 0.003212 | Validation Loss: 0.002464\n",
      "Epoch 336 | Train Loss: 0.002078 | Validation Loss: 0.002569\n",
      "Epoch 337 | Train Loss: 0.002194 | Validation Loss: 0.002523\n",
      "Epoch 338 | Train Loss: 0.001971 | Validation Loss: 0.002718\n",
      "Epoch 339 | Train Loss: 0.002416 | Validation Loss: 0.002684\n",
      "Epoch 340 | Train Loss: 0.002302 | Validation Loss: 0.002648\n",
      "Epoch 341 | Train Loss: 0.001776 | Validation Loss: 0.002459\n",
      "Epoch 342 | Train Loss: 0.001880 | Validation Loss: 0.002159\n",
      "Epoch 343 | Train Loss: 0.001848 | Validation Loss: 0.002430\n",
      "Epoch 344 | Train Loss: 0.001900 | Validation Loss: 0.003014\n",
      "Epoch 345 | Train Loss: 0.002324 | Validation Loss: 0.002707\n",
      "Epoch 346 | Train Loss: 0.002325 | Validation Loss: 0.002568\n",
      "Epoch 347 | Train Loss: 0.001956 | Validation Loss: 0.002613\n",
      "Epoch 348 | Train Loss: 0.002253 | Validation Loss: 0.002441\n",
      "Epoch 349 | Train Loss: 0.001967 | Validation Loss: 0.003145\n",
      "Epoch 350 | Train Loss: 0.002274 | Validation Loss: 0.002610\n",
      "Epoch 351 | Train Loss: 0.002029 | Validation Loss: 0.002647\n",
      "Epoch 352 | Train Loss: 0.002094 | Validation Loss: 0.002308\n",
      "Epoch 353 | Train Loss: 0.002246 | Validation Loss: 0.002845\n",
      "Epoch 354 | Train Loss: 0.002618 | Validation Loss: 0.003160\n",
      "Epoch 355 | Train Loss: 0.002922 | Validation Loss: 0.002557\n",
      "Epoch 356 | Train Loss: 0.002142 | Validation Loss: 0.003258\n",
      "Epoch 357 | Train Loss: 0.002951 | Validation Loss: 0.002437\n",
      "Epoch 358 | Train Loss: 0.002885 | Validation Loss: 0.003148\n",
      "Epoch 359 | Train Loss: 0.002199 | Validation Loss: 0.002929\n",
      "Epoch 360 | Train Loss: 0.001924 | Validation Loss: 0.003352\n",
      "Epoch 361 | Train Loss: 0.002780 | Validation Loss: 0.002323\n",
      "Epoch 362 | Train Loss: 0.001856 | Validation Loss: 0.003389\n",
      "Epoch 363 | Train Loss: 0.002560 | Validation Loss: 0.002385\n",
      "Epoch 364 | Train Loss: 0.001897 | Validation Loss: 0.002907\n",
      "Epoch 365 | Train Loss: 0.002019 | Validation Loss: 0.002982\n",
      "Epoch 366 | Train Loss: 0.002045 | Validation Loss: 0.002427\n",
      "Epoch 367 | Train Loss: 0.002923 | Validation Loss: 0.002621\n",
      "Epoch 368 | Train Loss: 0.001863 | Validation Loss: 0.002724\n",
      "Epoch 369 | Train Loss: 0.001929 | Validation Loss: 0.002605\n",
      "Epoch 370 | Train Loss: 0.001887 | Validation Loss: 0.002449\n",
      "Epoch 371 | Train Loss: 0.001701 | Validation Loss: 0.002614\n",
      "saved!\n",
      "Epoch 372 | Train Loss: 0.001775 | Validation Loss: 0.002050\n",
      "Epoch 373 | Train Loss: 0.001772 | Validation Loss: 0.002183\n",
      "Epoch 374 | Train Loss: 0.001753 | Validation Loss: 0.002424\n",
      "Epoch 375 | Train Loss: 0.001897 | Validation Loss: 0.002566\n",
      "Epoch 376 | Train Loss: 0.002338 | Validation Loss: 0.002260\n",
      "Epoch 377 | Train Loss: 0.002276 | Validation Loss: 0.002576\n",
      "saved!\n",
      "Epoch 378 | Train Loss: 0.002509 | Validation Loss: 0.002001\n",
      "Epoch 379 | Train Loss: 0.001817 | Validation Loss: 0.002983\n",
      "Epoch 380 | Train Loss: 0.002217 | Validation Loss: 0.002504\n",
      "Epoch 381 | Train Loss: 0.001826 | Validation Loss: 0.003130\n",
      "Epoch 382 | Train Loss: 0.002169 | Validation Loss: 0.002088\n",
      "Epoch 383 | Train Loss: 0.001717 | Validation Loss: 0.002523\n",
      "Epoch 384 | Train Loss: 0.002823 | Validation Loss: 0.002264\n",
      "Epoch 385 | Train Loss: 0.002050 | Validation Loss: 0.003042\n",
      "Epoch 386 | Train Loss: 0.002786 | Validation Loss: 0.002363\n",
      "Epoch 387 | Train Loss: 0.001815 | Validation Loss: 0.002088\n",
      "Epoch 388 | Train Loss: 0.001619 | Validation Loss: 0.002087\n",
      "Epoch 389 | Train Loss: 0.001748 | Validation Loss: 0.002034\n",
      "Epoch 390 | Train Loss: 0.001690 | Validation Loss: 0.002299\n",
      "Epoch 391 | Train Loss: 0.001715 | Validation Loss: 0.002282\n",
      "saved!\n",
      "Epoch 392 | Train Loss: 0.001674 | Validation Loss: 0.001976\n",
      "Epoch 393 | Train Loss: 0.001818 | Validation Loss: 0.002005\n",
      "Epoch 394 | Train Loss: 0.001765 | Validation Loss: 0.002246\n",
      "Epoch 395 | Train Loss: 0.001925 | Validation Loss: 0.002631\n",
      "Epoch 396 | Train Loss: 0.001729 | Validation Loss: 0.002323\n",
      "Epoch 397 | Train Loss: 0.002409 | Validation Loss: 0.002044\n",
      "Epoch 398 | Train Loss: 0.001667 | Validation Loss: 0.002179\n",
      "Epoch 399 | Train Loss: 0.002160 | Validation Loss: 0.002267\n",
      "Epoch 400 | Train Loss: 0.001609 | Validation Loss: 0.002729\n",
      "Epoch 401 | Train Loss: 0.001906 | Validation Loss: 0.002302\n",
      "Epoch 402 | Train Loss: 0.001661 | Validation Loss: 0.002038\n",
      "Epoch 403 | Train Loss: 0.001800 | Validation Loss: 0.002063\n",
      "Epoch 404 | Train Loss: 0.001648 | Validation Loss: 0.002186\n",
      "Epoch 405 | Train Loss: 0.001641 | Validation Loss: 0.002426\n",
      "Epoch 406 | Train Loss: 0.001628 | Validation Loss: 0.002163\n",
      "Epoch 407 | Train Loss: 0.001615 | Validation Loss: 0.002080\n",
      "Epoch 408 | Train Loss: 0.002138 | Validation Loss: 0.002052\n",
      "Epoch 409 | Train Loss: 0.001665 | Validation Loss: 0.002443\n",
      "Epoch 410 | Train Loss: 0.001751 | Validation Loss: 0.002122\n",
      "Epoch 411 | Train Loss: 0.001731 | Validation Loss: 0.002319\n",
      "Epoch 412 | Train Loss: 0.001883 | Validation Loss: 0.001988\n",
      "Epoch 413 | Train Loss: 0.001835 | Validation Loss: 0.002454\n",
      "Epoch 414 | Train Loss: 0.002017 | Validation Loss: 0.002408\n",
      "Epoch 415 | Train Loss: 0.001967 | Validation Loss: 0.002623\n",
      "Epoch 416 | Train Loss: 0.001776 | Validation Loss: 0.002538\n",
      "Epoch 417 | Train Loss: 0.001969 | Validation Loss: 0.002100\n",
      "Epoch 418 | Train Loss: 0.002048 | Validation Loss: 0.002534\n",
      "Epoch 419 | Train Loss: 0.001868 | Validation Loss: 0.002497\n",
      "Epoch 420 | Train Loss: 0.002373 | Validation Loss: 0.002458\n",
      "Epoch 421 | Train Loss: 0.001842 | Validation Loss: 0.002202\n",
      "Epoch 422 | Train Loss: 0.001901 | Validation Loss: 0.002294\n",
      "Epoch 423 | Train Loss: 0.001827 | Validation Loss: 0.002747\n",
      "Epoch 424 | Train Loss: 0.002092 | Validation Loss: 0.002533\n",
      "Epoch 425 | Train Loss: 0.002184 | Validation Loss: 0.002752\n",
      "Epoch 426 | Train Loss: 0.001924 | Validation Loss: 0.001988\n",
      "Epoch 427 | Train Loss: 0.001700 | Validation Loss: 0.002328\n",
      "Epoch 428 | Train Loss: 0.001682 | Validation Loss: 0.002371\n",
      "Epoch 429 | Train Loss: 0.001946 | Validation Loss: 0.002178\n",
      "saved!\n",
      "Epoch 430 | Train Loss: 0.001560 | Validation Loss: 0.001904\n",
      "Epoch 431 | Train Loss: 0.002082 | Validation Loss: 0.001969\n",
      "Epoch 432 | Train Loss: 0.002072 | Validation Loss: 0.002044\n",
      "Epoch 433 | Train Loss: 0.002618 | Validation Loss: 0.002247\n",
      "Epoch 434 | Train Loss: 0.001589 | Validation Loss: 0.002742\n",
      "Epoch 435 | Train Loss: 0.001830 | Validation Loss: 0.002158\n",
      "Epoch 436 | Train Loss: 0.001625 | Validation Loss: 0.002402\n",
      "Epoch 437 | Train Loss: 0.001503 | Validation Loss: 0.002056\n",
      "Epoch 438 | Train Loss: 0.003482 | Validation Loss: 0.002148\n",
      "Epoch 439 | Train Loss: 0.001728 | Validation Loss: 0.002175\n",
      "Epoch 440 | Train Loss: 0.003538 | Validation Loss: 0.002592\n",
      "Epoch 441 | Train Loss: 0.001899 | Validation Loss: 0.002725\n",
      "Epoch 442 | Train Loss: 0.002482 | Validation Loss: 0.002744\n",
      "Epoch 443 | Train Loss: 0.002492 | Validation Loss: 0.002915\n",
      "Epoch 444 | Train Loss: 0.001903 | Validation Loss: 0.002617\n",
      "Epoch 445 | Train Loss: 0.001625 | Validation Loss: 0.002524\n",
      "Epoch 446 | Train Loss: 0.002640 | Validation Loss: 0.002333\n",
      "Epoch 447 | Train Loss: 0.003717 | Validation Loss: 0.002758\n",
      "Epoch 448 | Train Loss: 0.002222 | Validation Loss: 0.002658\n",
      "Epoch 449 | Train Loss: 0.002957 | Validation Loss: 0.002075\n",
      "Epoch 450 | Train Loss: 0.001970 | Validation Loss: 0.003192\n",
      "Epoch 451 | Train Loss: 0.002657 | Validation Loss: 0.002295\n",
      "Epoch 452 | Train Loss: 0.001702 | Validation Loss: 0.002400\n",
      "Epoch 453 | Train Loss: 0.001629 | Validation Loss: 0.002278\n",
      "Epoch 454 | Train Loss: 0.001544 | Validation Loss: 0.002185\n",
      "Epoch 455 | Train Loss: 0.001585 | Validation Loss: 0.002331\n",
      "Epoch 456 | Train Loss: 0.001731 | Validation Loss: 0.002585\n",
      "Epoch 457 | Train Loss: 0.001426 | Validation Loss: 0.002164\n",
      "Epoch 458 | Train Loss: 0.001823 | Validation Loss: 0.001946\n",
      "Epoch 459 | Train Loss: 0.001902 | Validation Loss: 0.002244\n",
      "Epoch 460 | Train Loss: 0.001433 | Validation Loss: 0.002092\n",
      "Epoch 461 | Train Loss: 0.001378 | Validation Loss: 0.002065\n",
      "Epoch 462 | Train Loss: 0.001563 | Validation Loss: 0.002075\n",
      "Epoch 463 | Train Loss: 0.001635 | Validation Loss: 0.002015\n",
      "saved!\n",
      "Epoch 464 | Train Loss: 0.002065 | Validation Loss: 0.001858\n",
      "Epoch 465 | Train Loss: 0.001919 | Validation Loss: 0.001962\n",
      "Epoch 466 | Train Loss: 0.001602 | Validation Loss: 0.002206\n",
      "Epoch 467 | Train Loss: 0.001492 | Validation Loss: 0.002080\n",
      "saved!\n",
      "Epoch 468 | Train Loss: 0.001476 | Validation Loss: 0.001830\n",
      "Epoch 469 | Train Loss: 0.001370 | Validation Loss: 0.002094\n",
      "Epoch 470 | Train Loss: 0.001496 | Validation Loss: 0.001903\n",
      "Epoch 471 | Train Loss: 0.001618 | Validation Loss: 0.002190\n",
      "Epoch 472 | Train Loss: 0.001565 | Validation Loss: 0.002271\n",
      "Epoch 473 | Train Loss: 0.002683 | Validation Loss: 0.001908\n",
      "Epoch 474 | Train Loss: 0.001674 | Validation Loss: 0.002734\n",
      "Epoch 475 | Train Loss: 0.003927 | Validation Loss: 0.002038\n",
      "Epoch 476 | Train Loss: 0.002504 | Validation Loss: 0.003017\n",
      "Epoch 477 | Train Loss: 0.002587 | Validation Loss: 0.001985\n",
      "Epoch 478 | Train Loss: 0.001728 | Validation Loss: 0.003212\n",
      "Epoch 479 | Train Loss: 0.002271 | Validation Loss: 0.002312\n",
      "Epoch 480 | Train Loss: 0.001787 | Validation Loss: 0.002393\n",
      "Epoch 481 | Train Loss: 0.002231 | Validation Loss: 0.002628\n",
      "Epoch 482 | Train Loss: 0.002635 | Validation Loss: 0.002018\n",
      "Epoch 483 | Train Loss: 0.002124 | Validation Loss: 0.002523\n",
      "Epoch 484 | Train Loss: 0.001736 | Validation Loss: 0.002314\n",
      "Epoch 485 | Train Loss: 0.001673 | Validation Loss: 0.002204\n",
      "Epoch 486 | Train Loss: 0.001562 | Validation Loss: 0.002289\n",
      "Epoch 487 | Train Loss: 0.001423 | Validation Loss: 0.001900\n",
      "Epoch 488 | Train Loss: 0.001603 | Validation Loss: 0.002321\n",
      "Epoch 489 | Train Loss: 0.001693 | Validation Loss: 0.001984\n",
      "Epoch 490 | Train Loss: 0.001410 | Validation Loss: 0.001977\n",
      "saved!\n",
      "Epoch 491 | Train Loss: 0.001847 | Validation Loss: 0.001792\n",
      "Epoch 492 | Train Loss: 0.001653 | Validation Loss: 0.002299\n",
      "Epoch 493 | Train Loss: 0.002053 | Validation Loss: 0.002049\n",
      "Epoch 494 | Train Loss: 0.001338 | Validation Loss: 0.002233\n",
      "Epoch 495 | Train Loss: 0.001906 | Validation Loss: 0.002189\n",
      "Epoch 496 | Train Loss: 0.001552 | Validation Loss: 0.002037\n",
      "Epoch 497 | Train Loss: 0.001292 | Validation Loss: 0.001924\n",
      "saved!\n",
      "Epoch 498 | Train Loss: 0.001453 | Validation Loss: 0.001786\n",
      "Epoch 499 | Train Loss: 0.001347 | Validation Loss: 0.001884\n",
      "Epoch 500 | Train Loss: 0.001293 | Validation Loss: 0.001889\n",
      "Epoch 501 | Train Loss: 0.001114 | Validation Loss: 0.001955\n",
      "saved!\n",
      "Epoch 502 | Train Loss: 0.001430 | Validation Loss: 0.001755\n",
      "Epoch 503 | Train Loss: 0.001324 | Validation Loss: 0.001782\n",
      "Epoch 504 | Train Loss: 0.001229 | Validation Loss: 0.001887\n",
      "Epoch 505 | Train Loss: 0.001281 | Validation Loss: 0.001831\n",
      "Epoch 506 | Train Loss: 0.001229 | Validation Loss: 0.001881\n",
      "saved!\n",
      "Epoch 507 | Train Loss: 0.002051 | Validation Loss: 0.001713\n",
      "Epoch 508 | Train Loss: 0.001538 | Validation Loss: 0.001805\n",
      "Epoch 509 | Train Loss: 0.001455 | Validation Loss: 0.001980\n",
      "Epoch 510 | Train Loss: 0.001716 | Validation Loss: 0.001802\n",
      "saved!\n",
      "Epoch 511 | Train Loss: 0.001384 | Validation Loss: 0.001657\n",
      "Epoch 512 | Train Loss: 0.001608 | Validation Loss: 0.001772\n",
      "Epoch 513 | Train Loss: 0.001228 | Validation Loss: 0.001978\n",
      "Epoch 514 | Train Loss: 0.001470 | Validation Loss: 0.001962\n",
      "Epoch 515 | Train Loss: 0.001239 | Validation Loss: 0.001858\n",
      "Epoch 516 | Train Loss: 0.001394 | Validation Loss: 0.001773\n",
      "saved!\n",
      "Epoch 517 | Train Loss: 0.001267 | Validation Loss: 0.001609\n",
      "Epoch 518 | Train Loss: 0.001257 | Validation Loss: 0.001766\n",
      "Epoch 519 | Train Loss: 0.001183 | Validation Loss: 0.001957\n",
      "Epoch 520 | Train Loss: 0.001794 | Validation Loss: 0.001770\n",
      "Epoch 521 | Train Loss: 0.001222 | Validation Loss: 0.001634\n",
      "Epoch 522 | Train Loss: 0.002324 | Validation Loss: 0.001707\n",
      "Epoch 523 | Train Loss: 0.001868 | Validation Loss: 0.001820\n",
      "Epoch 524 | Train Loss: 0.001714 | Validation Loss: 0.001871\n",
      "Epoch 525 | Train Loss: 0.001313 | Validation Loss: 0.001655\n",
      "saved!\n",
      "Epoch 526 | Train Loss: 0.001142 | Validation Loss: 0.001579\n",
      "saved!\n",
      "Epoch 527 | Train Loss: 0.001347 | Validation Loss: 0.001569\n",
      "Epoch 528 | Train Loss: 0.001909 | Validation Loss: 0.001759\n",
      "Epoch 529 | Train Loss: 0.001690 | Validation Loss: 0.001901\n",
      "Epoch 530 | Train Loss: 0.001269 | Validation Loss: 0.001859\n",
      "Epoch 531 | Train Loss: 0.001103 | Validation Loss: 0.001924\n",
      "Epoch 532 | Train Loss: 0.001263 | Validation Loss: 0.001616\n",
      "Epoch 533 | Train Loss: 0.001260 | Validation Loss: 0.001597\n",
      "Epoch 534 | Train Loss: 0.001078 | Validation Loss: 0.001598\n",
      "Epoch 535 | Train Loss: 0.001658 | Validation Loss: 0.001817\n",
      "saved!\n",
      "Epoch 536 | Train Loss: 0.001113 | Validation Loss: 0.001556\n",
      "saved!\n",
      "Epoch 537 | Train Loss: 0.001283 | Validation Loss: 0.001513\n",
      "Epoch 538 | Train Loss: 0.001080 | Validation Loss: 0.001623\n",
      "Epoch 539 | Train Loss: 0.001148 | Validation Loss: 0.001759\n",
      "Epoch 540 | Train Loss: 0.001350 | Validation Loss: 0.001647\n",
      "Epoch 541 | Train Loss: 0.001032 | Validation Loss: 0.001717\n",
      "Epoch 542 | Train Loss: 0.001199 | Validation Loss: 0.001554\n",
      "Epoch 543 | Train Loss: 0.001033 | Validation Loss: 0.001630\n",
      "saved!\n",
      "Epoch 544 | Train Loss: 0.001743 | Validation Loss: 0.001505\n",
      "Epoch 545 | Train Loss: 0.001551 | Validation Loss: 0.001884\n",
      "Epoch 546 | Train Loss: 0.001173 | Validation Loss: 0.001594\n",
      "Epoch 547 | Train Loss: 0.001333 | Validation Loss: 0.001796\n",
      "Epoch 548 | Train Loss: 0.001233 | Validation Loss: 0.001527\n",
      "Epoch 549 | Train Loss: 0.001953 | Validation Loss: 0.001661\n",
      "Epoch 550 | Train Loss: 0.001297 | Validation Loss: 0.001715\n",
      "Epoch 551 | Train Loss: 0.001424 | Validation Loss: 0.002018\n",
      "Epoch 552 | Train Loss: 0.001316 | Validation Loss: 0.001532\n",
      "Epoch 553 | Train Loss: 0.002332 | Validation Loss: 0.001632\n",
      "Epoch 554 | Train Loss: 0.001229 | Validation Loss: 0.001595\n",
      "Epoch 555 | Train Loss: 0.001148 | Validation Loss: 0.001663\n",
      "Epoch 556 | Train Loss: 0.001237 | Validation Loss: 0.001654\n",
      "Epoch 557 | Train Loss: 0.001179 | Validation Loss: 0.001870\n",
      "saved!\n",
      "Epoch 558 | Train Loss: 0.001294 | Validation Loss: 0.001469\n",
      "Epoch 559 | Train Loss: 0.001739 | Validation Loss: 0.001679\n",
      "Epoch 560 | Train Loss: 0.001153 | Validation Loss: 0.001700\n",
      "Epoch 561 | Train Loss: 0.001637 | Validation Loss: 0.001558\n",
      "saved!\n",
      "Epoch 562 | Train Loss: 0.001228 | Validation Loss: 0.001451\n",
      "Epoch 563 | Train Loss: 0.001086 | Validation Loss: 0.001704\n",
      "Epoch 564 | Train Loss: 0.001674 | Validation Loss: 0.001688\n",
      "Epoch 565 | Train Loss: 0.001107 | Validation Loss: 0.001673\n",
      "saved!\n",
      "Epoch 566 | Train Loss: 0.001819 | Validation Loss: 0.001403\n",
      "Epoch 567 | Train Loss: 0.001213 | Validation Loss: 0.001917\n",
      "Epoch 568 | Train Loss: 0.001479 | Validation Loss: 0.001847\n",
      "Epoch 569 | Train Loss: 0.001793 | Validation Loss: 0.002095\n",
      "Epoch 570 | Train Loss: 0.001985 | Validation Loss: 0.001735\n",
      "Epoch 571 | Train Loss: 0.001547 | Validation Loss: 0.001827\n",
      "Epoch 572 | Train Loss: 0.002111 | Validation Loss: 0.002338\n",
      "Epoch 573 | Train Loss: 0.002021 | Validation Loss: 0.002340\n",
      "Epoch 574 | Train Loss: 0.001589 | Validation Loss: 0.002230\n",
      "Epoch 575 | Train Loss: 0.001921 | Validation Loss: 0.001644\n",
      "Epoch 576 | Train Loss: 0.001322 | Validation Loss: 0.001974\n",
      "Epoch 577 | Train Loss: 0.001371 | Validation Loss: 0.002086\n",
      "Epoch 578 | Train Loss: 0.001251 | Validation Loss: 0.002237\n",
      "Epoch 579 | Train Loss: 0.001149 | Validation Loss: 0.002017\n",
      "Epoch 580 | Train Loss: 0.001629 | Validation Loss: 0.001750\n",
      "Epoch 581 | Train Loss: 0.001246 | Validation Loss: 0.001887\n",
      "Epoch 582 | Train Loss: 0.000924 | Validation Loss: 0.001941\n",
      "Epoch 583 | Train Loss: 0.001086 | Validation Loss: 0.001658\n",
      "Epoch 584 | Train Loss: 0.000966 | Validation Loss: 0.001573\n",
      "Epoch 585 | Train Loss: 0.001228 | Validation Loss: 0.001682\n",
      "Epoch 586 | Train Loss: 0.000871 | Validation Loss: 0.001681\n",
      "Epoch 587 | Train Loss: 0.001046 | Validation Loss: 0.001542\n",
      "saved!\n",
      "Epoch 588 | Train Loss: 0.001131 | Validation Loss: 0.001379\n",
      "saved!\n",
      "Epoch 589 | Train Loss: 0.000984 | Validation Loss: 0.001347\n",
      "saved!\n",
      "Epoch 590 | Train Loss: 0.001070 | Validation Loss: 0.001314\n",
      "Epoch 591 | Train Loss: 0.000966 | Validation Loss: 0.001470\n",
      "Epoch 592 | Train Loss: 0.001494 | Validation Loss: 0.001503\n",
      "Epoch 593 | Train Loss: 0.001043 | Validation Loss: 0.001549\n",
      "Epoch 594 | Train Loss: 0.001759 | Validation Loss: 0.001537\n",
      "Epoch 595 | Train Loss: 0.000993 | Validation Loss: 0.001813\n",
      "Epoch 596 | Train Loss: 0.001145 | Validation Loss: 0.001789\n",
      "Epoch 597 | Train Loss: 0.001065 | Validation Loss: 0.001562\n",
      "Epoch 598 | Train Loss: 0.001017 | Validation Loss: 0.001543\n",
      "Epoch 599 | Train Loss: 0.000954 | Validation Loss: 0.001328\n",
      "Epoch 600 | Train Loss: 0.001129 | Validation Loss: 0.001584\n",
      "Epoch 601 | Train Loss: 0.000871 | Validation Loss: 0.001439\n",
      "Epoch 602 | Train Loss: 0.000932 | Validation Loss: 0.001602\n",
      "Epoch 603 | Train Loss: 0.001630 | Validation Loss: 0.001354\n",
      "Epoch 604 | Train Loss: 0.001655 | Validation Loss: 0.001450\n",
      "Epoch 605 | Train Loss: 0.001848 | Validation Loss: 0.001437\n",
      "Epoch 606 | Train Loss: 0.000949 | Validation Loss: 0.001417\n",
      "Epoch 607 | Train Loss: 0.001219 | Validation Loss: 0.001480\n",
      "Epoch 608 | Train Loss: 0.001043 | Validation Loss: 0.001520\n",
      "Epoch 609 | Train Loss: 0.001103 | Validation Loss: 0.001421\n",
      "saved!\n",
      "Epoch 610 | Train Loss: 0.001163 | Validation Loss: 0.001161\n",
      "Epoch 611 | Train Loss: 0.001166 | Validation Loss: 0.001448\n",
      "Epoch 612 | Train Loss: 0.001229 | Validation Loss: 0.001626\n",
      "Epoch 613 | Train Loss: 0.001223 | Validation Loss: 0.001987\n",
      "Epoch 614 | Train Loss: 0.001683 | Validation Loss: 0.001274\n",
      "Epoch 615 | Train Loss: 0.001447 | Validation Loss: 0.002069\n",
      "Epoch 616 | Train Loss: 0.001941 | Validation Loss: 0.001932\n",
      "Epoch 617 | Train Loss: 0.001580 | Validation Loss: 0.002457\n",
      "Epoch 618 | Train Loss: 0.001378 | Validation Loss: 0.001993\n",
      "Epoch 619 | Train Loss: 0.003813 | Validation Loss: 0.001213\n",
      "Epoch 620 | Train Loss: 0.001646 | Validation Loss: 0.002590\n",
      "Epoch 621 | Train Loss: 0.004522 | Validation Loss: 0.002258\n",
      "Epoch 622 | Train Loss: 0.001533 | Validation Loss: 0.003458\n",
      "Epoch 623 | Train Loss: 0.003396 | Validation Loss: 0.002296\n",
      "Epoch 624 | Train Loss: 0.002024 | Validation Loss: 0.001957\n",
      "Epoch 625 | Train Loss: 0.001832 | Validation Loss: 0.003928\n",
      "Epoch 626 | Train Loss: 0.001940 | Validation Loss: 0.002559\n",
      "Epoch 627 | Train Loss: 0.001293 | Validation Loss: 0.003165\n",
      "Epoch 628 | Train Loss: 0.001772 | Validation Loss: 0.001744\n",
      "Epoch 629 | Train Loss: 0.001264 | Validation Loss: 0.001393\n",
      "Epoch 630 | Train Loss: 0.001741 | Validation Loss: 0.001775\n",
      "Epoch 631 | Train Loss: 0.002198 | Validation Loss: 0.002323\n",
      "Epoch 632 | Train Loss: 0.001155 | Validation Loss: 0.002494\n",
      "Epoch 633 | Train Loss: 0.001560 | Validation Loss: 0.002004\n",
      "Epoch 634 | Train Loss: 0.001971 | Validation Loss: 0.001432\n",
      "Epoch 635 | Train Loss: 0.001561 | Validation Loss: 0.001856\n",
      "Epoch 636 | Train Loss: 0.001591 | Validation Loss: 0.001760\n",
      "Epoch 637 | Train Loss: 0.001183 | Validation Loss: 0.002220\n",
      "Epoch 638 | Train Loss: 0.001169 | Validation Loss: 0.001465\n",
      "Epoch 639 | Train Loss: 0.001400 | Validation Loss: 0.001472\n",
      "Epoch 640 | Train Loss: 0.000895 | Validation Loss: 0.001665\n",
      "Epoch 641 | Train Loss: 0.000996 | Validation Loss: 0.001836\n",
      "Epoch 642 | Train Loss: 0.000889 | Validation Loss: 0.001291\n",
      "Epoch 643 | Train Loss: 0.001053 | Validation Loss: 0.001295\n",
      "Epoch 644 | Train Loss: 0.002248 | Validation Loss: 0.001575\n",
      "Epoch 645 | Train Loss: 0.000779 | Validation Loss: 0.001449\n",
      "Epoch 646 | Train Loss: 0.001656 | Validation Loss: 0.001264\n",
      "Epoch 647 | Train Loss: 0.000847 | Validation Loss: 0.001351\n",
      "Epoch 648 | Train Loss: 0.001679 | Validation Loss: 0.001315\n",
      "Epoch 649 | Train Loss: 0.001918 | Validation Loss: 0.001763\n",
      "saved!\n",
      "Epoch 650 | Train Loss: 0.000926 | Validation Loss: 0.001099\n",
      "Epoch 651 | Train Loss: 0.000935 | Validation Loss: 0.001106\n",
      "Epoch 652 | Train Loss: 0.000802 | Validation Loss: 0.001271\n",
      "Epoch 653 | Train Loss: 0.001401 | Validation Loss: 0.001240\n",
      "Epoch 654 | Train Loss: 0.000892 | Validation Loss: 0.001111\n",
      "saved!\n",
      "Epoch 655 | Train Loss: 0.000846 | Validation Loss: 0.001049\n",
      "Epoch 656 | Train Loss: 0.001061 | Validation Loss: 0.001633\n",
      "Epoch 657 | Train Loss: 0.000998 | Validation Loss: 0.001498\n",
      "Epoch 658 | Train Loss: 0.001053 | Validation Loss: 0.001232\n",
      "Epoch 659 | Train Loss: 0.000907 | Validation Loss: 0.001210\n",
      "Epoch 660 | Train Loss: 0.001406 | Validation Loss: 0.001757\n",
      "Epoch 661 | Train Loss: 0.001520 | Validation Loss: 0.001603\n",
      "Epoch 662 | Train Loss: 0.000916 | Validation Loss: 0.001133\n",
      "Epoch 663 | Train Loss: 0.001148 | Validation Loss: 0.001427\n",
      "Epoch 664 | Train Loss: 0.001250 | Validation Loss: 0.001108\n",
      "Epoch 665 | Train Loss: 0.001132 | Validation Loss: 0.001374\n",
      "saved!\n",
      "Epoch 666 | Train Loss: 0.001142 | Validation Loss: 0.001027\n",
      "Epoch 667 | Train Loss: 0.001162 | Validation Loss: 0.001270\n",
      "Epoch 668 | Train Loss: 0.000850 | Validation Loss: 0.001152\n",
      "Epoch 669 | Train Loss: 0.000940 | Validation Loss: 0.001556\n",
      "Epoch 670 | Train Loss: 0.000871 | Validation Loss: 0.001332\n",
      "Epoch 671 | Train Loss: 0.000727 | Validation Loss: 0.001036\n",
      "Epoch 672 | Train Loss: 0.000747 | Validation Loss: 0.001256\n",
      "Epoch 673 | Train Loss: 0.000952 | Validation Loss: 0.001124\n",
      "Epoch 674 | Train Loss: 0.000925 | Validation Loss: 0.001106\n",
      "Epoch 675 | Train Loss: 0.000682 | Validation Loss: 0.001190\n",
      "Epoch 676 | Train Loss: 0.000775 | Validation Loss: 0.001348\n",
      "Epoch 677 | Train Loss: 0.000633 | Validation Loss: 0.001183\n",
      "Epoch 678 | Train Loss: 0.000867 | Validation Loss: 0.001065\n",
      "Epoch 679 | Train Loss: 0.000761 | Validation Loss: 0.001154\n",
      "Epoch 680 | Train Loss: 0.000985 | Validation Loss: 0.001180\n",
      "Epoch 681 | Train Loss: 0.000776 | Validation Loss: 0.001177\n",
      "Epoch 682 | Train Loss: 0.000881 | Validation Loss: 0.001143\n",
      "Epoch 683 | Train Loss: 0.000746 | Validation Loss: 0.001324\n",
      "saved!\n",
      "Epoch 684 | Train Loss: 0.000845 | Validation Loss: 0.001018\n",
      "Epoch 685 | Train Loss: 0.000685 | Validation Loss: 0.001187\n",
      "Epoch 686 | Train Loss: 0.000624 | Validation Loss: 0.001295\n",
      "saved!\n",
      "Epoch 687 | Train Loss: 0.000659 | Validation Loss: 0.000999\n",
      "Epoch 688 | Train Loss: 0.000701 | Validation Loss: 0.001235\n",
      "saved!\n",
      "Epoch 689 | Train Loss: 0.001162 | Validation Loss: 0.000990\n",
      "Epoch 690 | Train Loss: 0.001190 | Validation Loss: 0.001151\n",
      "saved!\n",
      "Epoch 691 | Train Loss: 0.000636 | Validation Loss: 0.000984\n",
      "Epoch 692 | Train Loss: 0.000724 | Validation Loss: 0.001116\n",
      "Epoch 693 | Train Loss: 0.000634 | Validation Loss: 0.001087\n",
      "Epoch 694 | Train Loss: 0.000570 | Validation Loss: 0.001132\n",
      "Epoch 695 | Train Loss: 0.000703 | Validation Loss: 0.001090\n",
      "Epoch 696 | Train Loss: 0.001168 | Validation Loss: 0.001347\n",
      "Epoch 697 | Train Loss: 0.001615 | Validation Loss: 0.001080\n",
      "Epoch 698 | Train Loss: 0.001011 | Validation Loss: 0.001014\n",
      "Epoch 699 | Train Loss: 0.001867 | Validation Loss: 0.001331\n",
      "Epoch 700 | Train Loss: 0.001031 | Validation Loss: 0.001364\n",
      "saved!\n",
      "Epoch 701 | Train Loss: 0.000989 | Validation Loss: 0.000930\n",
      "Epoch 702 | Train Loss: 0.001396 | Validation Loss: 0.001447\n",
      "Epoch 703 | Train Loss: 0.000851 | Validation Loss: 0.001008\n",
      "Epoch 704 | Train Loss: 0.001022 | Validation Loss: 0.001463\n",
      "saved!\n",
      "Epoch 705 | Train Loss: 0.000770 | Validation Loss: 0.000906\n",
      "Epoch 706 | Train Loss: 0.000749 | Validation Loss: 0.001038\n",
      "Epoch 707 | Train Loss: 0.000868 | Validation Loss: 0.000938\n",
      "Epoch 708 | Train Loss: 0.000589 | Validation Loss: 0.001244\n",
      "Epoch 709 | Train Loss: 0.001033 | Validation Loss: 0.000984\n",
      "Epoch 710 | Train Loss: 0.000807 | Validation Loss: 0.000955\n",
      "Epoch 711 | Train Loss: 0.000602 | Validation Loss: 0.001053\n",
      "Epoch 712 | Train Loss: 0.000576 | Validation Loss: 0.001048\n",
      "Epoch 713 | Train Loss: 0.000570 | Validation Loss: 0.001016\n",
      "Epoch 714 | Train Loss: 0.000730 | Validation Loss: 0.000963\n",
      "Epoch 715 | Train Loss: 0.000606 | Validation Loss: 0.000940\n",
      "Epoch 716 | Train Loss: 0.000921 | Validation Loss: 0.001087\n",
      "Epoch 717 | Train Loss: 0.000863 | Validation Loss: 0.001189\n",
      "Epoch 718 | Train Loss: 0.000774 | Validation Loss: 0.001034\n",
      "Epoch 719 | Train Loss: 0.000653 | Validation Loss: 0.000922\n",
      "Epoch 720 | Train Loss: 0.000732 | Validation Loss: 0.001089\n",
      "Epoch 721 | Train Loss: 0.000894 | Validation Loss: 0.001308\n",
      "Epoch 722 | Train Loss: 0.001075 | Validation Loss: 0.001035\n",
      "Epoch 723 | Train Loss: 0.000636 | Validation Loss: 0.001016\n",
      "Epoch 724 | Train Loss: 0.000866 | Validation Loss: 0.001068\n",
      "Epoch 725 | Train Loss: 0.000982 | Validation Loss: 0.001750\n",
      "Epoch 726 | Train Loss: 0.000763 | Validation Loss: 0.001147\n",
      "Epoch 727 | Train Loss: 0.001186 | Validation Loss: 0.001123\n",
      "Epoch 728 | Train Loss: 0.001776 | Validation Loss: 0.000907\n",
      "Epoch 729 | Train Loss: 0.001162 | Validation Loss: 0.001219\n",
      "Epoch 730 | Train Loss: 0.001399 | Validation Loss: 0.001061\n",
      "Epoch 731 | Train Loss: 0.000842 | Validation Loss: 0.001992\n",
      "Epoch 732 | Train Loss: 0.001038 | Validation Loss: 0.000972\n",
      "Epoch 733 | Train Loss: 0.000973 | Validation Loss: 0.001488\n",
      "Epoch 734 | Train Loss: 0.000998 | Validation Loss: 0.001381\n",
      "Epoch 735 | Train Loss: 0.000851 | Validation Loss: 0.001550\n",
      "Epoch 736 | Train Loss: 0.000725 | Validation Loss: 0.001469\n",
      "Epoch 737 | Train Loss: 0.000939 | Validation Loss: 0.000921\n",
      "Epoch 738 | Train Loss: 0.002093 | Validation Loss: 0.001195\n",
      "Epoch 739 | Train Loss: 0.000596 | Validation Loss: 0.001035\n",
      "Epoch 740 | Train Loss: 0.000677 | Validation Loss: 0.001014\n",
      "saved!\n",
      "Epoch 741 | Train Loss: 0.000682 | Validation Loss: 0.000871\n",
      "Epoch 742 | Train Loss: 0.000567 | Validation Loss: 0.001072\n",
      "Epoch 743 | Train Loss: 0.000993 | Validation Loss: 0.000877\n",
      "Epoch 744 | Train Loss: 0.000708 | Validation Loss: 0.001044\n",
      "Epoch 745 | Train Loss: 0.000674 | Validation Loss: 0.000889\n",
      "Epoch 746 | Train Loss: 0.000890 | Validation Loss: 0.000924\n",
      "Epoch 747 | Train Loss: 0.000711 | Validation Loss: 0.001100\n",
      "saved!\n",
      "Epoch 748 | Train Loss: 0.001036 | Validation Loss: 0.000852\n",
      "Epoch 749 | Train Loss: 0.000640 | Validation Loss: 0.001150\n",
      "saved!\n",
      "Epoch 750 | Train Loss: 0.001441 | Validation Loss: 0.000842\n",
      "Epoch 751 | Train Loss: 0.000895 | Validation Loss: 0.000977\n",
      "Epoch 752 | Train Loss: 0.000655 | Validation Loss: 0.001228\n",
      "Epoch 753 | Train Loss: 0.000702 | Validation Loss: 0.001051\n",
      "Epoch 754 | Train Loss: 0.000559 | Validation Loss: 0.000932\n",
      "Epoch 755 | Train Loss: 0.000863 | Validation Loss: 0.000901\n",
      "Epoch 756 | Train Loss: 0.000664 | Validation Loss: 0.001187\n",
      "Epoch 757 | Train Loss: 0.000619 | Validation Loss: 0.001011\n",
      "Epoch 758 | Train Loss: 0.000559 | Validation Loss: 0.000855\n",
      "Epoch 759 | Train Loss: 0.000580 | Validation Loss: 0.001115\n",
      "Epoch 760 | Train Loss: 0.000898 | Validation Loss: 0.001008\n",
      "Epoch 761 | Train Loss: 0.000549 | Validation Loss: 0.001041\n",
      "Epoch 762 | Train Loss: 0.001711 | Validation Loss: 0.001045\n",
      "Epoch 763 | Train Loss: 0.000756 | Validation Loss: 0.001787\n",
      "Epoch 764 | Train Loss: 0.001030 | Validation Loss: 0.001185\n",
      "Epoch 765 | Train Loss: 0.000584 | Validation Loss: 0.001241\n",
      "Epoch 766 | Train Loss: 0.001288 | Validation Loss: 0.000871\n",
      "Epoch 767 | Train Loss: 0.000685 | Validation Loss: 0.001394\n",
      "Epoch 768 | Train Loss: 0.001660 | Validation Loss: 0.000992\n",
      "Epoch 769 | Train Loss: 0.000629 | Validation Loss: 0.001646\n",
      "Epoch 770 | Train Loss: 0.000994 | Validation Loss: 0.001206\n",
      "Epoch 771 | Train Loss: 0.001736 | Validation Loss: 0.000899\n",
      "Epoch 772 | Train Loss: 0.001637 | Validation Loss: 0.001366\n",
      "Epoch 773 | Train Loss: 0.001123 | Validation Loss: 0.001216\n",
      "Epoch 774 | Train Loss: 0.000911 | Validation Loss: 0.001550\n",
      "Epoch 775 | Train Loss: 0.001264 | Validation Loss: 0.001073\n",
      "Epoch 776 | Train Loss: 0.000617 | Validation Loss: 0.002009\n",
      "Epoch 777 | Train Loss: 0.001889 | Validation Loss: 0.001409\n",
      "Epoch 778 | Train Loss: 0.001032 | Validation Loss: 0.001329\n",
      "Epoch 779 | Train Loss: 0.000648 | Validation Loss: 0.001375\n",
      "Epoch 780 | Train Loss: 0.000859 | Validation Loss: 0.000989\n",
      "Epoch 781 | Train Loss: 0.001205 | Validation Loss: 0.001051\n",
      "Epoch 782 | Train Loss: 0.000971 | Validation Loss: 0.001399\n",
      "Epoch 783 | Train Loss: 0.001055 | Validation Loss: 0.001137\n",
      "Epoch 784 | Train Loss: 0.000551 | Validation Loss: 0.001917\n",
      "Epoch 785 | Train Loss: 0.001086 | Validation Loss: 0.001038\n",
      "Epoch 786 | Train Loss: 0.000539 | Validation Loss: 0.001023\n",
      "Epoch 787 | Train Loss: 0.000617 | Validation Loss: 0.001222\n",
      "Epoch 788 | Train Loss: 0.000584 | Validation Loss: 0.000956\n",
      "Epoch 789 | Train Loss: 0.000836 | Validation Loss: 0.000892\n",
      "Epoch 790 | Train Loss: 0.001090 | Validation Loss: 0.001018\n",
      "Epoch 791 | Train Loss: 0.000482 | Validation Loss: 0.001086\n",
      "Epoch 792 | Train Loss: 0.000507 | Validation Loss: 0.000944\n",
      "Epoch 793 | Train Loss: 0.001242 | Validation Loss: 0.000892\n",
      "Epoch 794 | Train Loss: 0.000776 | Validation Loss: 0.001087\n",
      "Epoch 795 | Train Loss: 0.001050 | Validation Loss: 0.000860\n",
      "Epoch 796 | Train Loss: 0.000722 | Validation Loss: 0.001025\n",
      "Epoch 797 | Train Loss: 0.000711 | Validation Loss: 0.000981\n",
      "Epoch 798 | Train Loss: 0.001060 | Validation Loss: 0.000846\n",
      "Epoch 799 | Train Loss: 0.000395 | Validation Loss: 0.000860\n",
      "Epoch 800 | Train Loss: 0.000542 | Validation Loss: 0.000866\n",
      "saved!\n",
      "Epoch 801 | Train Loss: 0.000508 | Validation Loss: 0.000807\n",
      "Epoch 802 | Train Loss: 0.000482 | Validation Loss: 0.000834\n",
      "saved!\n",
      "Epoch 803 | Train Loss: 0.000684 | Validation Loss: 0.000785\n",
      "Epoch 804 | Train Loss: 0.002322 | Validation Loss: 0.000896\n",
      "Epoch 805 | Train Loss: 0.000710 | Validation Loss: 0.001123\n",
      "Epoch 806 | Train Loss: 0.000902 | Validation Loss: 0.000921\n",
      "Epoch 807 | Train Loss: 0.001062 | Validation Loss: 0.000909\n",
      "Epoch 808 | Train Loss: 0.000855 | Validation Loss: 0.000813\n",
      "Epoch 809 | Train Loss: 0.000692 | Validation Loss: 0.001070\n",
      "saved!\n",
      "Epoch 810 | Train Loss: 0.000862 | Validation Loss: 0.000679\n",
      "Epoch 811 | Train Loss: 0.000469 | Validation Loss: 0.000879\n",
      "Epoch 812 | Train Loss: 0.000701 | Validation Loss: 0.000797\n",
      "Epoch 813 | Train Loss: 0.000594 | Validation Loss: 0.000943\n",
      "Epoch 814 | Train Loss: 0.000540 | Validation Loss: 0.000937\n",
      "Epoch 815 | Train Loss: 0.000644 | Validation Loss: 0.000879\n",
      "Epoch 816 | Train Loss: 0.000607 | Validation Loss: 0.000680\n",
      "Epoch 817 | Train Loss: 0.001042 | Validation Loss: 0.000749\n",
      "Epoch 818 | Train Loss: 0.000875 | Validation Loss: 0.001381\n",
      "Epoch 819 | Train Loss: 0.001722 | Validation Loss: 0.001128\n",
      "Epoch 820 | Train Loss: 0.000989 | Validation Loss: 0.000948\n",
      "Epoch 821 | Train Loss: 0.000748 | Validation Loss: 0.001038\n",
      "Epoch 822 | Train Loss: 0.000554 | Validation Loss: 0.000797\n",
      "Epoch 823 | Train Loss: 0.000605 | Validation Loss: 0.000703\n",
      "Epoch 824 | Train Loss: 0.000650 | Validation Loss: 0.000824\n",
      "Epoch 825 | Train Loss: 0.000548 | Validation Loss: 0.000711\n",
      "Epoch 826 | Train Loss: 0.000414 | Validation Loss: 0.000883\n",
      "Epoch 827 | Train Loss: 0.000576 | Validation Loss: 0.000833\n",
      "Epoch 828 | Train Loss: 0.000506 | Validation Loss: 0.000758\n",
      "Epoch 829 | Train Loss: 0.000459 | Validation Loss: 0.000778\n",
      "Epoch 830 | Train Loss: 0.001464 | Validation Loss: 0.000817\n",
      "Epoch 831 | Train Loss: 0.001338 | Validation Loss: 0.000815\n",
      "Epoch 832 | Train Loss: 0.000812 | Validation Loss: 0.000859\n",
      "Epoch 833 | Train Loss: 0.000534 | Validation Loss: 0.000706\n",
      "Epoch 834 | Train Loss: 0.001485 | Validation Loss: 0.000721\n",
      "Epoch 835 | Train Loss: 0.001676 | Validation Loss: 0.000899\n",
      "Epoch 836 | Train Loss: 0.000734 | Validation Loss: 0.000929\n",
      "Epoch 837 | Train Loss: 0.000716 | Validation Loss: 0.001081\n",
      "Epoch 838 | Train Loss: 0.000845 | Validation Loss: 0.000994\n",
      "Epoch 839 | Train Loss: 0.000769 | Validation Loss: 0.001154\n",
      "Epoch 840 | Train Loss: 0.000980 | Validation Loss: 0.001182\n",
      "Epoch 841 | Train Loss: 0.000801 | Validation Loss: 0.001262\n",
      "Epoch 842 | Train Loss: 0.001095 | Validation Loss: 0.000879\n",
      "Epoch 843 | Train Loss: 0.000529 | Validation Loss: 0.001295\n",
      "Epoch 844 | Train Loss: 0.001108 | Validation Loss: 0.000966\n",
      "Epoch 845 | Train Loss: 0.000667 | Validation Loss: 0.001007\n",
      "Epoch 846 | Train Loss: 0.000911 | Validation Loss: 0.001269\n",
      "Epoch 847 | Train Loss: 0.000744 | Validation Loss: 0.000787\n",
      "Epoch 848 | Train Loss: 0.000802 | Validation Loss: 0.000985\n",
      "saved!\n",
      "Epoch 849 | Train Loss: 0.000499 | Validation Loss: 0.000659\n",
      "Epoch 850 | Train Loss: 0.000507 | Validation Loss: 0.000926\n",
      "Epoch 851 | Train Loss: 0.000482 | Validation Loss: 0.000814\n",
      "Epoch 852 | Train Loss: 0.000657 | Validation Loss: 0.000984\n",
      "Epoch 853 | Train Loss: 0.000580 | Validation Loss: 0.000993\n",
      "Epoch 854 | Train Loss: 0.000537 | Validation Loss: 0.000711\n",
      "Epoch 855 | Train Loss: 0.001059 | Validation Loss: 0.000751\n",
      "Epoch 856 | Train Loss: 0.000421 | Validation Loss: 0.000759\n",
      "Epoch 857 | Train Loss: 0.000632 | Validation Loss: 0.000766\n",
      "Epoch 858 | Train Loss: 0.000949 | Validation Loss: 0.000933\n",
      "Epoch 859 | Train Loss: 0.000755 | Validation Loss: 0.001208\n",
      "Epoch 860 | Train Loss: 0.001620 | Validation Loss: 0.000950\n",
      "Epoch 861 | Train Loss: 0.001308 | Validation Loss: 0.001307\n",
      "saved!\n",
      "Epoch 862 | Train Loss: 0.001114 | Validation Loss: 0.000615\n",
      "Epoch 863 | Train Loss: 0.000806 | Validation Loss: 0.001561\n",
      "Epoch 864 | Train Loss: 0.000928 | Validation Loss: 0.001086\n",
      "Epoch 865 | Train Loss: 0.000538 | Validation Loss: 0.001034\n",
      "Epoch 866 | Train Loss: 0.000946 | Validation Loss: 0.000903\n",
      "Epoch 867 | Train Loss: 0.000488 | Validation Loss: 0.001100\n",
      "Epoch 868 | Train Loss: 0.000945 | Validation Loss: 0.001179\n",
      "Epoch 869 | Train Loss: 0.000774 | Validation Loss: 0.000767\n",
      "Epoch 870 | Train Loss: 0.000512 | Validation Loss: 0.000743\n",
      "Epoch 871 | Train Loss: 0.000386 | Validation Loss: 0.000873\n",
      "Epoch 872 | Train Loss: 0.000361 | Validation Loss: 0.001104\n",
      "Epoch 873 | Train Loss: 0.000458 | Validation Loss: 0.000813\n",
      "Epoch 874 | Train Loss: 0.001422 | Validation Loss: 0.000840\n",
      "Epoch 875 | Train Loss: 0.000792 | Validation Loss: 0.000814\n",
      "Epoch 876 | Train Loss: 0.000882 | Validation Loss: 0.000796\n",
      "Epoch 877 | Train Loss: 0.000583 | Validation Loss: 0.001161\n",
      "Epoch 878 | Train Loss: 0.000839 | Validation Loss: 0.000778\n",
      "Epoch 879 | Train Loss: 0.000994 | Validation Loss: 0.001123\n",
      "Epoch 880 | Train Loss: 0.001455 | Validation Loss: 0.001148\n",
      "Epoch 881 | Train Loss: 0.001122 | Validation Loss: 0.000863\n",
      "Epoch 882 | Train Loss: 0.000692 | Validation Loss: 0.001050\n",
      "Epoch 883 | Train Loss: 0.000536 | Validation Loss: 0.000907\n",
      "Epoch 884 | Train Loss: 0.000538 | Validation Loss: 0.000660\n",
      "Epoch 885 | Train Loss: 0.000486 | Validation Loss: 0.000752\n",
      "Epoch 886 | Train Loss: 0.000656 | Validation Loss: 0.000862\n",
      "Epoch 887 | Train Loss: 0.000447 | Validation Loss: 0.000731\n",
      "Epoch 888 | Train Loss: 0.000394 | Validation Loss: 0.000724\n",
      "Epoch 889 | Train Loss: 0.000561 | Validation Loss: 0.000616\n",
      "Epoch 890 | Train Loss: 0.000444 | Validation Loss: 0.000666\n",
      "Epoch 891 | Train Loss: 0.000392 | Validation Loss: 0.000890\n",
      "Epoch 892 | Train Loss: 0.000507 | Validation Loss: 0.000657\n",
      "Epoch 893 | Train Loss: 0.000851 | Validation Loss: 0.000701\n",
      "Epoch 894 | Train Loss: 0.000446 | Validation Loss: 0.000705\n",
      "Epoch 895 | Train Loss: 0.000626 | Validation Loss: 0.000676\n",
      "Epoch 896 | Train Loss: 0.000428 | Validation Loss: 0.000924\n",
      "Epoch 897 | Train Loss: 0.000626 | Validation Loss: 0.000661\n",
      "Epoch 898 | Train Loss: 0.000594 | Validation Loss: 0.000684\n",
      "Epoch 899 | Train Loss: 0.000684 | Validation Loss: 0.000700\n",
      "Epoch 900 | Train Loss: 0.000449 | Validation Loss: 0.000701\n",
      "Epoch 901 | Train Loss: 0.000321 | Validation Loss: 0.000990\n",
      "Epoch 902 | Train Loss: 0.001022 | Validation Loss: 0.000855\n",
      "Epoch 903 | Train Loss: 0.001241 | Validation Loss: 0.000628\n",
      "Epoch 904 | Train Loss: 0.000373 | Validation Loss: 0.000815\n",
      "Epoch 905 | Train Loss: 0.000691 | Validation Loss: 0.000823\n",
      "Epoch 906 | Train Loss: 0.000611 | Validation Loss: 0.000940\n",
      "Epoch 907 | Train Loss: 0.001624 | Validation Loss: 0.000714\n",
      "Epoch 908 | Train Loss: 0.000940 | Validation Loss: 0.000909\n",
      "Epoch 909 | Train Loss: 0.000847 | Validation Loss: 0.001079\n",
      "Epoch 910 | Train Loss: 0.001529 | Validation Loss: 0.001031\n",
      "Epoch 911 | Train Loss: 0.001355 | Validation Loss: 0.001229\n",
      "Epoch 912 | Train Loss: 0.001059 | Validation Loss: 0.000875\n",
      "Epoch 913 | Train Loss: 0.000869 | Validation Loss: 0.001075\n",
      "Epoch 914 | Train Loss: 0.000835 | Validation Loss: 0.000984\n",
      "Epoch 915 | Train Loss: 0.000610 | Validation Loss: 0.001212\n",
      "Epoch 916 | Train Loss: 0.000947 | Validation Loss: 0.000953\n",
      "Epoch 917 | Train Loss: 0.002058 | Validation Loss: 0.001187\n",
      "Epoch 918 | Train Loss: 0.000716 | Validation Loss: 0.001144\n",
      "Epoch 919 | Train Loss: 0.001524 | Validation Loss: 0.000662\n",
      "Epoch 920 | Train Loss: 0.000720 | Validation Loss: 0.001559\n",
      "Epoch 921 | Train Loss: 0.001136 | Validation Loss: 0.000881\n",
      "Epoch 922 | Train Loss: 0.000488 | Validation Loss: 0.001271\n",
      "Epoch 923 | Train Loss: 0.002423 | Validation Loss: 0.001337\n",
      "Epoch 924 | Train Loss: 0.000790 | Validation Loss: 0.000790\n",
      "Epoch 925 | Train Loss: 0.000911 | Validation Loss: 0.001120\n",
      "Epoch 926 | Train Loss: 0.001392 | Validation Loss: 0.000980\n",
      "Epoch 927 | Train Loss: 0.000487 | Validation Loss: 0.001169\n",
      "Epoch 928 | Train Loss: 0.001441 | Validation Loss: 0.000744\n",
      "Epoch 929 | Train Loss: 0.000486 | Validation Loss: 0.000880\n",
      "Epoch 930 | Train Loss: 0.001074 | Validation Loss: 0.001064\n",
      "Epoch 931 | Train Loss: 0.001111 | Validation Loss: 0.000852\n",
      "Epoch 932 | Train Loss: 0.000691 | Validation Loss: 0.000680\n",
      "Epoch 933 | Train Loss: 0.000575 | Validation Loss: 0.000808\n",
      "Epoch 934 | Train Loss: 0.000572 | Validation Loss: 0.000842\n",
      "Epoch 935 | Train Loss: 0.000784 | Validation Loss: 0.000828\n",
      "Epoch 936 | Train Loss: 0.000636 | Validation Loss: 0.000800\n",
      "Epoch 937 | Train Loss: 0.000546 | Validation Loss: 0.000757\n",
      "Epoch 938 | Train Loss: 0.000536 | Validation Loss: 0.001015\n",
      "Epoch 939 | Train Loss: 0.001016 | Validation Loss: 0.001121\n",
      "Epoch 940 | Train Loss: 0.000567 | Validation Loss: 0.000901\n",
      "Epoch 941 | Train Loss: 0.000569 | Validation Loss: 0.000780\n",
      "Epoch 942 | Train Loss: 0.000606 | Validation Loss: 0.000666\n",
      "Epoch 943 | Train Loss: 0.000540 | Validation Loss: 0.001102\n",
      "Epoch 944 | Train Loss: 0.001105 | Validation Loss: 0.000833\n",
      "Epoch 945 | Train Loss: 0.000737 | Validation Loss: 0.000979\n",
      "Epoch 946 | Train Loss: 0.000988 | Validation Loss: 0.000806\n",
      "Epoch 947 | Train Loss: 0.000577 | Validation Loss: 0.000823\n",
      "Epoch 948 | Train Loss: 0.000460 | Validation Loss: 0.001163\n",
      "Epoch 949 | Train Loss: 0.000758 | Validation Loss: 0.000868\n",
      "Epoch 950 | Train Loss: 0.000416 | Validation Loss: 0.000831\n",
      "Epoch 951 | Train Loss: 0.000745 | Validation Loss: 0.000719\n",
      "Epoch 952 | Train Loss: 0.000525 | Validation Loss: 0.000781\n",
      "Epoch 953 | Train Loss: 0.000647 | Validation Loss: 0.000898\n",
      "Epoch 954 | Train Loss: 0.000657 | Validation Loss: 0.000898\n",
      "saved!\n",
      "Epoch 955 | Train Loss: 0.000411 | Validation Loss: 0.000603\n",
      "saved!\n",
      "Epoch 956 | Train Loss: 0.000916 | Validation Loss: 0.000549\n",
      "Epoch 957 | Train Loss: 0.001170 | Validation Loss: 0.000838\n",
      "Epoch 958 | Train Loss: 0.000679 | Validation Loss: 0.000970\n",
      "Epoch 959 | Train Loss: 0.000567 | Validation Loss: 0.000570\n",
      "Epoch 960 | Train Loss: 0.000627 | Validation Loss: 0.000572\n",
      "Epoch 961 | Train Loss: 0.000684 | Validation Loss: 0.000671\n",
      "Epoch 962 | Train Loss: 0.001455 | Validation Loss: 0.000835\n",
      "Epoch 963 | Train Loss: 0.000561 | Validation Loss: 0.000771\n",
      "Epoch 964 | Train Loss: 0.000590 | Validation Loss: 0.000783\n",
      "Epoch 965 | Train Loss: 0.001117 | Validation Loss: 0.000662\n",
      "Epoch 966 | Train Loss: 0.000388 | Validation Loss: 0.001157\n",
      "Epoch 967 | Train Loss: 0.000803 | Validation Loss: 0.000876\n",
      "Epoch 968 | Train Loss: 0.000413 | Validation Loss: 0.000697\n",
      "Epoch 969 | Train Loss: 0.000912 | Validation Loss: 0.001024\n",
      "Epoch 970 | Train Loss: 0.000593 | Validation Loss: 0.000697\n",
      "Epoch 971 | Train Loss: 0.000782 | Validation Loss: 0.000764\n",
      "Epoch 972 | Train Loss: 0.000473 | Validation Loss: 0.000578\n",
      "Epoch 973 | Train Loss: 0.000749 | Validation Loss: 0.000992\n",
      "Epoch 974 | Train Loss: 0.000596 | Validation Loss: 0.001077\n",
      "Epoch 975 | Train Loss: 0.001134 | Validation Loss: 0.000642\n",
      "Epoch 976 | Train Loss: 0.000602 | Validation Loss: 0.001073\n",
      "Epoch 977 | Train Loss: 0.000869 | Validation Loss: 0.000743\n",
      "Epoch 978 | Train Loss: 0.000443 | Validation Loss: 0.001207\n",
      "Epoch 979 | Train Loss: 0.001362 | Validation Loss: 0.000730\n",
      "Epoch 980 | Train Loss: 0.000533 | Validation Loss: 0.001059\n",
      "Epoch 981 | Train Loss: 0.000873 | Validation Loss: 0.000983\n",
      "Epoch 982 | Train Loss: 0.001866 | Validation Loss: 0.001039\n",
      "Epoch 983 | Train Loss: 0.000522 | Validation Loss: 0.001365\n",
      "Epoch 984 | Train Loss: 0.001296 | Validation Loss: 0.000724\n",
      "Epoch 985 | Train Loss: 0.000809 | Validation Loss: 0.000999\n",
      "Epoch 986 | Train Loss: 0.000838 | Validation Loss: 0.001588\n",
      "Epoch 987 | Train Loss: 0.001173 | Validation Loss: 0.000775\n",
      "Epoch 988 | Train Loss: 0.000624 | Validation Loss: 0.001362\n",
      "Epoch 989 | Train Loss: 0.002117 | Validation Loss: 0.001579\n",
      "Epoch 990 | Train Loss: 0.000775 | Validation Loss: 0.000700\n",
      "Epoch 991 | Train Loss: 0.000897 | Validation Loss: 0.001818\n",
      "Epoch 992 | Train Loss: 0.001357 | Validation Loss: 0.000824\n",
      "Epoch 993 | Train Loss: 0.000374 | Validation Loss: 0.001052\n",
      "Epoch 994 | Train Loss: 0.001009 | Validation Loss: 0.001005\n",
      "Epoch 995 | Train Loss: 0.000768 | Validation Loss: 0.000681\n",
      "Epoch 996 | Train Loss: 0.000437 | Validation Loss: 0.001015\n",
      "Epoch 997 | Train Loss: 0.000846 | Validation Loss: 0.000939\n",
      "Epoch 998 | Train Loss: 0.000587 | Validation Loss: 0.000629\n",
      "Epoch 999 | Train Loss: 0.001208 | Validation Loss: 0.000861\n",
      "Epoch 1000 | Train Loss: 0.000528 | Validation Loss: 0.000648\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer = Trainer(model, optimizer, device='cuda' if torch.cuda.is_available() else 'cpu', max_patience=max_patience)\n",
    "trainer.train(train_loader, val_loader, epochs=epochs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0d0dc-79c9-4d8a-989b-d17024cfa179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dosyasına kaydet\n",
    "train_losses = trainer.train_cost  # liste veya numpy array\n",
    "val_losses = trainer.val_cost\n",
    "\n",
    "np.savetxt(\"results/losses.csv\", \n",
    "           np.column_stack((train_losses, val_losses)), \n",
    "           delimiter=\",\", \n",
    "           header=\"train_loss,val_loss\", \n",
    "           comments=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
