{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/arf/home/tunal/ondemand/PhD Thesis Starting/01_SON/Tik-4/Tez/02-SwissRoll/02-UAE_for_SwissRoll-Copy1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd()) # dosya yolunu ver\n",
    "%run ./Model.ipynb\n",
    "%run ../Dataset.ipynb\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f7601ea5-682f-44a9-9b06-0f0efd697f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device='cpu', max_patience=20):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "        self.max_patience = max_patience\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience = 0\n",
    "\n",
    "        self.val_cost = []\n",
    "        self.train_cost = []\n",
    "    \n",
    "    def train(self, train_loader, val_loader=None, epochs=10, print_every=1, name=None):\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for x, x_, y in train_loader:  # label kullanılmıyor\n",
    "                x = x.to(self.device)\n",
    "                x_ = x_.to(self.device)\n",
    "                \n",
    "                z_hat_, x_hat_ = self.model(x_)\n",
    "                z_hat, x_hat = self.model(x)\n",
    "                \n",
    "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,3))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Validation (opsiyonel)\n",
    "            if val_loader:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                \n",
    "                # Early stopping kontrolü\n",
    "                if val_loss < self.best_val_loss:\n",
    "                    print('saved!')\n",
    "                    torch.save(self.model, name + '.model')\n",
    "                    self.best_val_loss = val_loss\n",
    "                    self.patience = 0\n",
    "        \n",
    "                else:\n",
    "                    self.patience = self.patience + 1\n",
    "        \n",
    "                if self.patience > self.max_patience:\n",
    "                    break\n",
    "                    \n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch:3d} | Train Loss: {total_loss / len(train_loader):.6f} | Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "            self.val_cost.append(val_loss)\n",
    "            self.train_cost.append(total_loss / len(train_loader))\n",
    "            \n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, x_, _ in val_loader:\n",
    "                x = x.to(self.device)\n",
    "                x_ = x_.to(self.device)\n",
    "                \n",
    "                z_hat_, x_hat_ = self.model(x_)\n",
    "                z_hat, x_hat = self.model(x)\n",
    "                \n",
    "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,3))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        #print(f\"→ Validation Loss: {avg_loss:.6f}\")\n",
    "        self.model.train()\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6982ede7-a39e-433c-90a9-8850b1f73c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transform\n",
    "class NoiseTransform:\n",
    "    \"\"\"Add some noise.\"\"\"\n",
    "\n",
    "    def __init__(self, split_ratio=0.001, dim=3):\n",
    "\n",
    "        self.normal_dist = split_ratio*np.random.randn(dim,)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "      return x + self.normal_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters & Settings\n",
    "\n",
    "dataset_size = 4000\n",
    "batch_size = 500\n",
    "lr = 0.003\n",
    "\n",
    "epochs = 1000\n",
    "max_patience = 1000\n",
    "\n",
    "split_ratio = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "07121ef5-db8d-4bd5-a327-a8646f696809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = SwissRollDataset(mode='train', n_samples=dataset_size, transform=NoiseTransform(split_ratio))\n",
    "val_dataset = SwissRollDataset(mode='val', n_samples=dataset_size, transform=NoiseTransform(split_ratio))\n",
    "test_dataset = SwissRollDataset(mode='test', n_samples=dataset_size)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c3942a74-4000-4175-b95a-0940fb29ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"results\" klasörünü oluştur (zaten varsa hata vermez)\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "c9abf79c-a874-418b-b46f-aaa2fe649d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "name = 'results/UAE_SwissRoll'\n",
    "model = To_Uniform(\n",
    "                 encoder_layers=[3, 400, 400, 400, 400, 400, 2],\n",
    "                 decoder_layers=[2, 400, 400, 400, 400, 400, 3],\n",
    "                 encoder_act=nn.ReLU,\n",
    "                 decoder_act=nn.ReLU,\n",
    "                 final_encoder_act=nn.Sigmoid,\n",
    "                 final_decoder_act=nn.Sigmoid,\n",
    "                 use_batchnorm=True\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n",
      "Epoch   1 | Train Loss: 0.028706 | Validation Loss: 0.026299\n",
      "saved!\n",
      "Epoch   2 | Train Loss: 0.022536 | Validation Loss: 0.018866\n",
      "saved!\n",
      "Epoch   3 | Train Loss: 0.018717 | Validation Loss: 0.017700\n",
      "saved!\n",
      "Epoch   4 | Train Loss: 0.017379 | Validation Loss: 0.016935\n",
      "saved!\n",
      "Epoch   5 | Train Loss: 0.016285 | Validation Loss: 0.015312\n",
      "saved!\n",
      "Epoch   6 | Train Loss: 0.014948 | Validation Loss: 0.014043\n",
      "saved!\n",
      "Epoch   7 | Train Loss: 0.013615 | Validation Loss: 0.013347\n",
      "saved!\n",
      "Epoch   8 | Train Loss: 0.013341 | Validation Loss: 0.012262\n",
      "saved!\n",
      "Epoch   9 | Train Loss: 0.012493 | Validation Loss: 0.012184\n",
      "saved!\n",
      "Epoch  10 | Train Loss: 0.011893 | Validation Loss: 0.011805\n",
      "saved!\n",
      "Epoch  11 | Train Loss: 0.011067 | Validation Loss: 0.010491\n",
      "saved!\n",
      "Epoch  12 | Train Loss: 0.010323 | Validation Loss: 0.009240\n",
      "saved!\n",
      "Epoch  13 | Train Loss: 0.009458 | Validation Loss: 0.008130\n",
      "saved!\n",
      "Epoch  14 | Train Loss: 0.008311 | Validation Loss: 0.006915\n",
      "saved!\n",
      "Epoch  15 | Train Loss: 0.006625 | Validation Loss: 0.005864\n",
      "Epoch  16 | Train Loss: 0.005722 | Validation Loss: 0.007376\n",
      "Epoch  17 | Train Loss: 0.006203 | Validation Loss: 0.007949\n",
      "Epoch  18 | Train Loss: 0.006322 | Validation Loss: 0.006797\n",
      "saved!\n",
      "Epoch  19 | Train Loss: 0.005486 | Validation Loss: 0.005218\n",
      "saved!\n",
      "Epoch  20 | Train Loss: 0.005044 | Validation Loss: 0.004610\n",
      "Epoch  21 | Train Loss: 0.004651 | Validation Loss: 0.006658\n",
      "saved!\n",
      "Epoch  22 | Train Loss: 0.004629 | Validation Loss: 0.004072\n",
      "saved!\n",
      "Epoch  23 | Train Loss: 0.003301 | Validation Loss: 0.003639\n",
      "saved!\n",
      "Epoch  24 | Train Loss: 0.002901 | Validation Loss: 0.002843\n",
      "Epoch  25 | Train Loss: 0.002971 | Validation Loss: 0.003390\n",
      "Epoch  26 | Train Loss: 0.002971 | Validation Loss: 0.003073\n",
      "Epoch  27 | Train Loss: 0.003096 | Validation Loss: 0.005538\n",
      "Epoch  28 | Train Loss: 0.003314 | Validation Loss: 0.003512\n",
      "saved!\n",
      "Epoch  29 | Train Loss: 0.003186 | Validation Loss: 0.002484\n",
      "Epoch  30 | Train Loss: 0.002576 | Validation Loss: 0.003198\n",
      "saved!\n",
      "Epoch  31 | Train Loss: 0.002671 | Validation Loss: 0.002460\n",
      "saved!\n",
      "Epoch  32 | Train Loss: 0.002822 | Validation Loss: 0.002367\n",
      "Epoch  33 | Train Loss: 0.002693 | Validation Loss: 0.002859\n",
      "Epoch  34 | Train Loss: 0.002775 | Validation Loss: 0.002602\n",
      "Epoch  35 | Train Loss: 0.003036 | Validation Loss: 0.003017\n",
      "Epoch  36 | Train Loss: 0.003117 | Validation Loss: 0.002735\n",
      "saved!\n",
      "Epoch  37 | Train Loss: 0.002859 | Validation Loss: 0.001951\n",
      "Epoch  38 | Train Loss: 0.002283 | Validation Loss: 0.002042\n",
      "saved!\n",
      "Epoch  39 | Train Loss: 0.001965 | Validation Loss: 0.001787\n",
      "saved!\n",
      "Epoch  40 | Train Loss: 0.001748 | Validation Loss: 0.001427\n",
      "Epoch  41 | Train Loss: 0.001666 | Validation Loss: 0.001501\n",
      "saved!\n",
      "Epoch  42 | Train Loss: 0.001587 | Validation Loss: 0.001378\n",
      "Epoch  43 | Train Loss: 0.001516 | Validation Loss: 0.001444\n",
      "Epoch  44 | Train Loss: 0.001373 | Validation Loss: 0.001547\n",
      "saved!\n",
      "Epoch  45 | Train Loss: 0.001591 | Validation Loss: 0.001366\n",
      "saved!\n",
      "Epoch  46 | Train Loss: 0.001444 | Validation Loss: 0.001318\n",
      "Epoch  47 | Train Loss: 0.001405 | Validation Loss: 0.001832\n",
      "Epoch  48 | Train Loss: 0.001943 | Validation Loss: 0.002054\n",
      "Epoch  49 | Train Loss: 0.001715 | Validation Loss: 0.001449\n",
      "Epoch  50 | Train Loss: 0.001910 | Validation Loss: 0.001872\n",
      "Epoch  51 | Train Loss: 0.001696 | Validation Loss: 0.001673\n",
      "Epoch  52 | Train Loss: 0.001548 | Validation Loss: 0.001610\n",
      "Epoch  53 | Train Loss: 0.001810 | Validation Loss: 0.001995\n",
      "Epoch  54 | Train Loss: 0.001752 | Validation Loss: 0.001711\n",
      "Epoch  55 | Train Loss: 0.001446 | Validation Loss: 0.001506\n",
      "Epoch  56 | Train Loss: 0.001512 | Validation Loss: 0.001570\n",
      "Epoch  57 | Train Loss: 0.001573 | Validation Loss: 0.001899\n",
      "saved!\n",
      "Epoch  58 | Train Loss: 0.001914 | Validation Loss: 0.001168\n",
      "Epoch  59 | Train Loss: 0.001785 | Validation Loss: 0.001293\n",
      "Epoch  60 | Train Loss: 0.001962 | Validation Loss: 0.003544\n",
      "Epoch  61 | Train Loss: 0.002839 | Validation Loss: 0.001504\n",
      "Epoch  62 | Train Loss: 0.002310 | Validation Loss: 0.002554\n",
      "Epoch  63 | Train Loss: 0.002240 | Validation Loss: 0.001682\n",
      "Epoch  64 | Train Loss: 0.001837 | Validation Loss: 0.001308\n",
      "Epoch  65 | Train Loss: 0.001423 | Validation Loss: 0.001408\n",
      "Epoch  66 | Train Loss: 0.001319 | Validation Loss: 0.001423\n",
      "Epoch  67 | Train Loss: 0.001264 | Validation Loss: 0.001172\n",
      "Epoch  68 | Train Loss: 0.001202 | Validation Loss: 0.001615\n",
      "Epoch  69 | Train Loss: 0.001519 | Validation Loss: 0.001376\n",
      "saved!\n",
      "Epoch  70 | Train Loss: 0.001233 | Validation Loss: 0.001043\n",
      "Epoch  71 | Train Loss: 0.001216 | Validation Loss: 0.001071\n",
      "Epoch  72 | Train Loss: 0.001397 | Validation Loss: 0.001118\n",
      "Epoch  73 | Train Loss: 0.001714 | Validation Loss: 0.001918\n",
      "Epoch  74 | Train Loss: 0.002128 | Validation Loss: 0.001614\n",
      "Epoch  75 | Train Loss: 0.001441 | Validation Loss: 0.001495\n",
      "Epoch  76 | Train Loss: 0.001649 | Validation Loss: 0.001530\n",
      "Epoch  77 | Train Loss: 0.001283 | Validation Loss: 0.001164\n",
      "Epoch  78 | Train Loss: 0.001313 | Validation Loss: 0.001058\n",
      "Epoch  79 | Train Loss: 0.001218 | Validation Loss: 0.001389\n",
      "Epoch  80 | Train Loss: 0.001411 | Validation Loss: 0.001179\n",
      "saved!\n",
      "Epoch  81 | Train Loss: 0.001242 | Validation Loss: 0.001018\n",
      "Epoch  82 | Train Loss: 0.001239 | Validation Loss: 0.001347\n",
      "saved!\n",
      "Epoch  83 | Train Loss: 0.001343 | Validation Loss: 0.001002\n",
      "Epoch  84 | Train Loss: 0.001315 | Validation Loss: 0.001661\n",
      "Epoch  85 | Train Loss: 0.001518 | Validation Loss: 0.001019\n",
      "Epoch  86 | Train Loss: 0.001029 | Validation Loss: 0.001167\n",
      "Epoch  87 | Train Loss: 0.001123 | Validation Loss: 0.001216\n",
      "Epoch  88 | Train Loss: 0.001077 | Validation Loss: 0.001045\n",
      "Epoch  89 | Train Loss: 0.001246 | Validation Loss: 0.001615\n",
      "Epoch  90 | Train Loss: 0.001660 | Validation Loss: 0.002814\n",
      "Epoch  91 | Train Loss: 0.001761 | Validation Loss: 0.001112\n",
      "Epoch  92 | Train Loss: 0.001478 | Validation Loss: 0.001826\n",
      "Epoch  93 | Train Loss: 0.001535 | Validation Loss: 0.001664\n",
      "Epoch  94 | Train Loss: 0.001291 | Validation Loss: 0.001054\n",
      "Epoch  95 | Train Loss: 0.001231 | Validation Loss: 0.001163\n",
      "Epoch  96 | Train Loss: 0.001343 | Validation Loss: 0.001503\n",
      "Epoch  97 | Train Loss: 0.001451 | Validation Loss: 0.001308\n",
      "Epoch  98 | Train Loss: 0.001385 | Validation Loss: 0.001630\n",
      "Epoch  99 | Train Loss: 0.001402 | Validation Loss: 0.001233\n",
      "Epoch 100 | Train Loss: 0.001388 | Validation Loss: 0.001096\n",
      "Epoch 101 | Train Loss: 0.001251 | Validation Loss: 0.001794\n",
      "Epoch 102 | Train Loss: 0.001516 | Validation Loss: 0.001329\n",
      "Epoch 103 | Train Loss: 0.001062 | Validation Loss: 0.001264\n",
      "saved!\n",
      "Epoch 104 | Train Loss: 0.001066 | Validation Loss: 0.000946\n",
      "Epoch 105 | Train Loss: 0.001358 | Validation Loss: 0.001463\n",
      "Epoch 106 | Train Loss: 0.001438 | Validation Loss: 0.001112\n",
      "Epoch 107 | Train Loss: 0.001213 | Validation Loss: 0.001001\n",
      "Epoch 108 | Train Loss: 0.000946 | Validation Loss: 0.001082\n",
      "Epoch 109 | Train Loss: 0.001143 | Validation Loss: 0.001211\n",
      "saved!\n",
      "Epoch 110 | Train Loss: 0.001043 | Validation Loss: 0.000914\n",
      "Epoch 111 | Train Loss: 0.001162 | Validation Loss: 0.000972\n",
      "Epoch 112 | Train Loss: 0.001276 | Validation Loss: 0.001372\n",
      "Epoch 113 | Train Loss: 0.001227 | Validation Loss: 0.000917\n",
      "Epoch 114 | Train Loss: 0.001245 | Validation Loss: 0.001077\n",
      "Epoch 115 | Train Loss: 0.001256 | Validation Loss: 0.001300\n",
      "Epoch 116 | Train Loss: 0.001520 | Validation Loss: 0.000923\n",
      "Epoch 117 | Train Loss: 0.001245 | Validation Loss: 0.001120\n",
      "Epoch 118 | Train Loss: 0.001185 | Validation Loss: 0.002431\n",
      "Epoch 119 | Train Loss: 0.001693 | Validation Loss: 0.001128\n",
      "Epoch 120 | Train Loss: 0.001198 | Validation Loss: 0.001248\n",
      "Epoch 121 | Train Loss: 0.000961 | Validation Loss: 0.001360\n",
      "Epoch 122 | Train Loss: 0.001042 | Validation Loss: 0.001143\n",
      "Epoch 123 | Train Loss: 0.001027 | Validation Loss: 0.001072\n",
      "saved!\n",
      "Epoch 124 | Train Loss: 0.001153 | Validation Loss: 0.000777\n",
      "Epoch 125 | Train Loss: 0.000819 | Validation Loss: 0.000944\n",
      "Epoch 126 | Train Loss: 0.000985 | Validation Loss: 0.000930\n",
      "Epoch 127 | Train Loss: 0.000843 | Validation Loss: 0.000794\n",
      "Epoch 128 | Train Loss: 0.001188 | Validation Loss: 0.001058\n",
      "Epoch 129 | Train Loss: 0.001080 | Validation Loss: 0.000924\n",
      "saved!\n",
      "Epoch 130 | Train Loss: 0.001002 | Validation Loss: 0.000733\n",
      "Epoch 131 | Train Loss: 0.001052 | Validation Loss: 0.000911\n",
      "Epoch 132 | Train Loss: 0.000898 | Validation Loss: 0.001132\n",
      "Epoch 133 | Train Loss: 0.001066 | Validation Loss: 0.000960\n",
      "Epoch 134 | Train Loss: 0.001234 | Validation Loss: 0.000780\n",
      "Epoch 135 | Train Loss: 0.000913 | Validation Loss: 0.001037\n",
      "Epoch 136 | Train Loss: 0.001136 | Validation Loss: 0.001403\n",
      "Epoch 137 | Train Loss: 0.001068 | Validation Loss: 0.001071\n",
      "saved!\n",
      "Epoch 138 | Train Loss: 0.000975 | Validation Loss: 0.000730\n",
      "Epoch 139 | Train Loss: 0.000833 | Validation Loss: 0.000757\n",
      "Epoch 140 | Train Loss: 0.000820 | Validation Loss: 0.000925\n",
      "Epoch 141 | Train Loss: 0.000971 | Validation Loss: 0.000739\n",
      "Epoch 142 | Train Loss: 0.000826 | Validation Loss: 0.001260\n",
      "Epoch 143 | Train Loss: 0.000946 | Validation Loss: 0.000782\n",
      "saved!\n",
      "Epoch 144 | Train Loss: 0.000688 | Validation Loss: 0.000662\n",
      "saved!\n",
      "Epoch 145 | Train Loss: 0.000634 | Validation Loss: 0.000565\n",
      "Epoch 146 | Train Loss: 0.000566 | Validation Loss: 0.000729\n",
      "Epoch 147 | Train Loss: 0.000630 | Validation Loss: 0.000688\n",
      "Epoch 148 | Train Loss: 0.000965 | Validation Loss: 0.000756\n",
      "Epoch 149 | Train Loss: 0.000886 | Validation Loss: 0.000994\n",
      "Epoch 150 | Train Loss: 0.000892 | Validation Loss: 0.000876\n",
      "Epoch 151 | Train Loss: 0.000874 | Validation Loss: 0.000580\n",
      "Epoch 152 | Train Loss: 0.000693 | Validation Loss: 0.000770\n",
      "Epoch 153 | Train Loss: 0.000799 | Validation Loss: 0.000625\n",
      "Epoch 154 | Train Loss: 0.000692 | Validation Loss: 0.000581\n",
      "Epoch 155 | Train Loss: 0.000813 | Validation Loss: 0.000801\n",
      "Epoch 156 | Train Loss: 0.000809 | Validation Loss: 0.000801\n",
      "Epoch 157 | Train Loss: 0.000746 | Validation Loss: 0.000715\n",
      "Epoch 158 | Train Loss: 0.000572 | Validation Loss: 0.000634\n",
      "Epoch 159 | Train Loss: 0.000751 | Validation Loss: 0.000805\n",
      "saved!\n",
      "Epoch 160 | Train Loss: 0.000833 | Validation Loss: 0.000551\n",
      "Epoch 161 | Train Loss: 0.000738 | Validation Loss: 0.000680\n",
      "Epoch 162 | Train Loss: 0.000557 | Validation Loss: 0.000629\n",
      "Epoch 163 | Train Loss: 0.000554 | Validation Loss: 0.000697\n",
      "Epoch 164 | Train Loss: 0.000831 | Validation Loss: 0.000740\n",
      "Epoch 165 | Train Loss: 0.000679 | Validation Loss: 0.000838\n",
      "Epoch 166 | Train Loss: 0.000581 | Validation Loss: 0.000590\n",
      "Epoch 167 | Train Loss: 0.000833 | Validation Loss: 0.000999\n",
      "Epoch 168 | Train Loss: 0.000744 | Validation Loss: 0.000800\n",
      "Epoch 169 | Train Loss: 0.000947 | Validation Loss: 0.000646\n",
      "Epoch 170 | Train Loss: 0.000735 | Validation Loss: 0.000635\n",
      "Epoch 171 | Train Loss: 0.000618 | Validation Loss: 0.000612\n",
      "Epoch 172 | Train Loss: 0.000763 | Validation Loss: 0.000559\n",
      "Epoch 173 | Train Loss: 0.000565 | Validation Loss: 0.000706\n",
      "saved!\n",
      "Epoch 174 | Train Loss: 0.000642 | Validation Loss: 0.000462\n",
      "Epoch 175 | Train Loss: 0.000568 | Validation Loss: 0.000760\n",
      "Epoch 176 | Train Loss: 0.000727 | Validation Loss: 0.000704\n",
      "Epoch 177 | Train Loss: 0.001073 | Validation Loss: 0.001024\n",
      "Epoch 178 | Train Loss: 0.001051 | Validation Loss: 0.001512\n",
      "Epoch 179 | Train Loss: 0.001267 | Validation Loss: 0.000982\n",
      "Epoch 180 | Train Loss: 0.000837 | Validation Loss: 0.001333\n",
      "Epoch 181 | Train Loss: 0.000996 | Validation Loss: 0.001020\n",
      "Epoch 182 | Train Loss: 0.000821 | Validation Loss: 0.000790\n",
      "Epoch 183 | Train Loss: 0.000772 | Validation Loss: 0.001718\n",
      "Epoch 184 | Train Loss: 0.001167 | Validation Loss: 0.000811\n",
      "Epoch 185 | Train Loss: 0.000732 | Validation Loss: 0.000843\n",
      "Epoch 186 | Train Loss: 0.000788 | Validation Loss: 0.000690\n",
      "Epoch 187 | Train Loss: 0.000810 | Validation Loss: 0.001037\n",
      "Epoch 188 | Train Loss: 0.000919 | Validation Loss: 0.000932\n",
      "Epoch 189 | Train Loss: 0.000652 | Validation Loss: 0.000836\n",
      "Epoch 190 | Train Loss: 0.000910 | Validation Loss: 0.001016\n",
      "Epoch 191 | Train Loss: 0.000864 | Validation Loss: 0.000770\n",
      "Epoch 192 | Train Loss: 0.000998 | Validation Loss: 0.001081\n",
      "Epoch 193 | Train Loss: 0.000977 | Validation Loss: 0.000577\n",
      "Epoch 194 | Train Loss: 0.000832 | Validation Loss: 0.000807\n",
      "Epoch 195 | Train Loss: 0.000819 | Validation Loss: 0.000651\n",
      "Epoch 196 | Train Loss: 0.000816 | Validation Loss: 0.000644\n",
      "Epoch 197 | Train Loss: 0.000703 | Validation Loss: 0.000910\n",
      "Epoch 198 | Train Loss: 0.000634 | Validation Loss: 0.000690\n",
      "Epoch 199 | Train Loss: 0.000699 | Validation Loss: 0.000814\n",
      "Epoch 200 | Train Loss: 0.000799 | Validation Loss: 0.000562\n",
      "saved!\n",
      "Epoch 201 | Train Loss: 0.000629 | Validation Loss: 0.000406\n",
      "Epoch 202 | Train Loss: 0.000645 | Validation Loss: 0.000605\n",
      "Epoch 203 | Train Loss: 0.000679 | Validation Loss: 0.000569\n",
      "Epoch 204 | Train Loss: 0.000786 | Validation Loss: 0.000611\n",
      "Epoch 205 | Train Loss: 0.000630 | Validation Loss: 0.000703\n",
      "Epoch 206 | Train Loss: 0.000741 | Validation Loss: 0.001689\n",
      "Epoch 207 | Train Loss: 0.000889 | Validation Loss: 0.001192\n",
      "Epoch 208 | Train Loss: 0.001151 | Validation Loss: 0.000968\n",
      "Epoch 209 | Train Loss: 0.001257 | Validation Loss: 0.002305\n",
      "Epoch 210 | Train Loss: 0.001715 | Validation Loss: 0.004278\n",
      "Epoch 211 | Train Loss: 0.001413 | Validation Loss: 0.003673\n",
      "Epoch 212 | Train Loss: 0.001646 | Validation Loss: 0.002198\n",
      "Epoch 213 | Train Loss: 0.001899 | Validation Loss: 0.002795\n",
      "Epoch 214 | Train Loss: 0.001740 | Validation Loss: 0.001652\n",
      "Epoch 215 | Train Loss: 0.001605 | Validation Loss: 0.001290\n",
      "Epoch 216 | Train Loss: 0.001485 | Validation Loss: 0.001671\n",
      "Epoch 217 | Train Loss: 0.001459 | Validation Loss: 0.001721\n",
      "Epoch 218 | Train Loss: 0.001550 | Validation Loss: 0.001302\n",
      "Epoch 219 | Train Loss: 0.001227 | Validation Loss: 0.000920\n",
      "Epoch 220 | Train Loss: 0.001612 | Validation Loss: 0.001007\n",
      "Epoch 221 | Train Loss: 0.001347 | Validation Loss: 0.001321\n",
      "Epoch 222 | Train Loss: 0.001071 | Validation Loss: 0.001068\n",
      "Epoch 223 | Train Loss: 0.000938 | Validation Loss: 0.000941\n",
      "Epoch 224 | Train Loss: 0.001262 | Validation Loss: 0.001178\n",
      "Epoch 225 | Train Loss: 0.000915 | Validation Loss: 0.001293\n",
      "Epoch 226 | Train Loss: 0.000937 | Validation Loss: 0.000726\n",
      "Epoch 227 | Train Loss: 0.001570 | Validation Loss: 0.001071\n",
      "Epoch 228 | Train Loss: 0.001579 | Validation Loss: 0.002661\n",
      "Epoch 229 | Train Loss: 0.001545 | Validation Loss: 0.001511\n",
      "Epoch 230 | Train Loss: 0.001235 | Validation Loss: 0.001157\n",
      "Epoch 231 | Train Loss: 0.001233 | Validation Loss: 0.001653\n",
      "Epoch 232 | Train Loss: 0.001294 | Validation Loss: 0.001462\n",
      "Epoch 233 | Train Loss: 0.001115 | Validation Loss: 0.001380\n",
      "Epoch 234 | Train Loss: 0.001025 | Validation Loss: 0.001188\n",
      "Epoch 235 | Train Loss: 0.001041 | Validation Loss: 0.000707\n",
      "Epoch 236 | Train Loss: 0.000839 | Validation Loss: 0.000858\n",
      "Epoch 237 | Train Loss: 0.000805 | Validation Loss: 0.000761\n",
      "Epoch 238 | Train Loss: 0.000877 | Validation Loss: 0.000982\n",
      "Epoch 239 | Train Loss: 0.001118 | Validation Loss: 0.000767\n",
      "Epoch 240 | Train Loss: 0.000916 | Validation Loss: 0.000799\n",
      "Epoch 241 | Train Loss: 0.000753 | Validation Loss: 0.000807\n",
      "Epoch 242 | Train Loss: 0.000843 | Validation Loss: 0.001272\n",
      "Epoch 243 | Train Loss: 0.000926 | Validation Loss: 0.000809\n",
      "Epoch 244 | Train Loss: 0.000646 | Validation Loss: 0.000702\n",
      "Epoch 245 | Train Loss: 0.000726 | Validation Loss: 0.000663\n",
      "Epoch 246 | Train Loss: 0.000926 | Validation Loss: 0.000594\n",
      "Epoch 247 | Train Loss: 0.001029 | Validation Loss: 0.000896\n",
      "Epoch 248 | Train Loss: 0.001075 | Validation Loss: 0.002086\n",
      "Epoch 249 | Train Loss: 0.000861 | Validation Loss: 0.001090\n",
      "Epoch 250 | Train Loss: 0.000994 | Validation Loss: 0.001153\n",
      "Epoch 251 | Train Loss: 0.000955 | Validation Loss: 0.000761\n",
      "Epoch 252 | Train Loss: 0.000679 | Validation Loss: 0.000772\n",
      "Epoch 253 | Train Loss: 0.000733 | Validation Loss: 0.000803\n",
      "Epoch 254 | Train Loss: 0.000685 | Validation Loss: 0.000474\n",
      "Epoch 255 | Train Loss: 0.000693 | Validation Loss: 0.000529\n",
      "Epoch 256 | Train Loss: 0.000662 | Validation Loss: 0.000574\n",
      "Epoch 257 | Train Loss: 0.000653 | Validation Loss: 0.000475\n",
      "Epoch 258 | Train Loss: 0.000667 | Validation Loss: 0.000616\n",
      "Epoch 259 | Train Loss: 0.000857 | Validation Loss: 0.000597\n",
      "Epoch 260 | Train Loss: 0.000800 | Validation Loss: 0.000713\n",
      "Epoch 261 | Train Loss: 0.000690 | Validation Loss: 0.000698\n",
      "Epoch 262 | Train Loss: 0.000573 | Validation Loss: 0.000543\n",
      "Epoch 263 | Train Loss: 0.000669 | Validation Loss: 0.000541\n",
      "Epoch 264 | Train Loss: 0.000739 | Validation Loss: 0.000653\n",
      "Epoch 265 | Train Loss: 0.000628 | Validation Loss: 0.000510\n",
      "Epoch 266 | Train Loss: 0.000599 | Validation Loss: 0.000732\n",
      "Epoch 267 | Train Loss: 0.000582 | Validation Loss: 0.000624\n",
      "Epoch 268 | Train Loss: 0.001198 | Validation Loss: 0.000958\n",
      "Epoch 269 | Train Loss: 0.000848 | Validation Loss: 0.000840\n",
      "Epoch 270 | Train Loss: 0.000684 | Validation Loss: 0.000441\n",
      "Epoch 271 | Train Loss: 0.000707 | Validation Loss: 0.000688\n",
      "Epoch 272 | Train Loss: 0.001047 | Validation Loss: 0.000957\n",
      "Epoch 273 | Train Loss: 0.000959 | Validation Loss: 0.000909\n",
      "Epoch 274 | Train Loss: 0.000763 | Validation Loss: 0.000663\n",
      "Epoch 275 | Train Loss: 0.000822 | Validation Loss: 0.000522\n",
      "Epoch 276 | Train Loss: 0.000583 | Validation Loss: 0.000703\n",
      "saved!\n",
      "Epoch 277 | Train Loss: 0.000634 | Validation Loss: 0.000403\n",
      "Epoch 278 | Train Loss: 0.000714 | Validation Loss: 0.000678\n",
      "Epoch 279 | Train Loss: 0.000828 | Validation Loss: 0.000525\n",
      "Epoch 280 | Train Loss: 0.000663 | Validation Loss: 0.000880\n",
      "Epoch 281 | Train Loss: 0.000622 | Validation Loss: 0.000439\n",
      "saved!\n",
      "Epoch 282 | Train Loss: 0.000605 | Validation Loss: 0.000347\n",
      "Epoch 283 | Train Loss: 0.000567 | Validation Loss: 0.000431\n",
      "Epoch 284 | Train Loss: 0.000589 | Validation Loss: 0.000613\n",
      "Epoch 285 | Train Loss: 0.001045 | Validation Loss: 0.000956\n",
      "Epoch 286 | Train Loss: 0.000914 | Validation Loss: 0.000691\n",
      "Epoch 287 | Train Loss: 0.000844 | Validation Loss: 0.000496\n",
      "Epoch 288 | Train Loss: 0.000719 | Validation Loss: 0.000804\n",
      "Epoch 289 | Train Loss: 0.000937 | Validation Loss: 0.001416\n",
      "Epoch 290 | Train Loss: 0.000766 | Validation Loss: 0.000692\n",
      "Epoch 291 | Train Loss: 0.000616 | Validation Loss: 0.000553\n",
      "Epoch 292 | Train Loss: 0.001071 | Validation Loss: 0.000535\n",
      "Epoch 293 | Train Loss: 0.000847 | Validation Loss: 0.001035\n",
      "Epoch 294 | Train Loss: 0.000835 | Validation Loss: 0.000839\n",
      "Epoch 295 | Train Loss: 0.000699 | Validation Loss: 0.000889\n",
      "Epoch 296 | Train Loss: 0.000560 | Validation Loss: 0.000389\n",
      "Epoch 297 | Train Loss: 0.000716 | Validation Loss: 0.000515\n",
      "Epoch 298 | Train Loss: 0.000760 | Validation Loss: 0.000615\n",
      "Epoch 299 | Train Loss: 0.000683 | Validation Loss: 0.000901\n",
      "Epoch 300 | Train Loss: 0.000638 | Validation Loss: 0.000422\n",
      "Epoch 301 | Train Loss: 0.000555 | Validation Loss: 0.000463\n",
      "Epoch 302 | Train Loss: 0.000604 | Validation Loss: 0.000957\n",
      "Epoch 303 | Train Loss: 0.000649 | Validation Loss: 0.000785\n",
      "Epoch 304 | Train Loss: 0.000737 | Validation Loss: 0.000496\n",
      "Epoch 305 | Train Loss: 0.000587 | Validation Loss: 0.000463\n",
      "saved!\n",
      "Epoch 306 | Train Loss: 0.000477 | Validation Loss: 0.000335\n",
      "Epoch 307 | Train Loss: 0.000656 | Validation Loss: 0.000609\n",
      "Epoch 308 | Train Loss: 0.000635 | Validation Loss: 0.000549\n",
      "Epoch 309 | Train Loss: 0.000455 | Validation Loss: 0.000495\n",
      "Epoch 310 | Train Loss: 0.000649 | Validation Loss: 0.000344\n",
      "saved!\n",
      "Epoch 311 | Train Loss: 0.000538 | Validation Loss: 0.000320\n",
      "Epoch 312 | Train Loss: 0.000659 | Validation Loss: 0.000793\n",
      "Epoch 313 | Train Loss: 0.000747 | Validation Loss: 0.000613\n",
      "Epoch 314 | Train Loss: 0.000591 | Validation Loss: 0.000593\n",
      "Epoch 315 | Train Loss: 0.000761 | Validation Loss: 0.000470\n",
      "Epoch 316 | Train Loss: 0.000552 | Validation Loss: 0.000416\n",
      "Epoch 317 | Train Loss: 0.000495 | Validation Loss: 0.000473\n",
      "Epoch 318 | Train Loss: 0.000572 | Validation Loss: 0.000603\n",
      "Epoch 319 | Train Loss: 0.000468 | Validation Loss: 0.000415\n",
      "Epoch 320 | Train Loss: 0.000517 | Validation Loss: 0.000493\n",
      "Epoch 321 | Train Loss: 0.000669 | Validation Loss: 0.001008\n",
      "Epoch 322 | Train Loss: 0.000609 | Validation Loss: 0.000693\n",
      "Epoch 323 | Train Loss: 0.000832 | Validation Loss: 0.000544\n",
      "Epoch 324 | Train Loss: 0.000763 | Validation Loss: 0.000738\n",
      "Epoch 325 | Train Loss: 0.000838 | Validation Loss: 0.000836\n",
      "Epoch 326 | Train Loss: 0.000944 | Validation Loss: 0.000692\n",
      "Epoch 327 | Train Loss: 0.000618 | Validation Loss: 0.000722\n",
      "Epoch 328 | Train Loss: 0.000639 | Validation Loss: 0.000453\n",
      "Epoch 329 | Train Loss: 0.000631 | Validation Loss: 0.000554\n",
      "Epoch 330 | Train Loss: 0.000606 | Validation Loss: 0.001032\n",
      "Epoch 331 | Train Loss: 0.000403 | Validation Loss: 0.000419\n",
      "Epoch 332 | Train Loss: 0.000472 | Validation Loss: 0.000432\n",
      "Epoch 333 | Train Loss: 0.000506 | Validation Loss: 0.000422\n",
      "saved!\n",
      "Epoch 334 | Train Loss: 0.000626 | Validation Loss: 0.000294\n",
      "Epoch 335 | Train Loss: 0.000724 | Validation Loss: 0.000367\n",
      "Epoch 336 | Train Loss: 0.000618 | Validation Loss: 0.000733\n",
      "Epoch 337 | Train Loss: 0.000560 | Validation Loss: 0.000718\n",
      "Epoch 338 | Train Loss: 0.000818 | Validation Loss: 0.000935\n",
      "Epoch 339 | Train Loss: 0.000624 | Validation Loss: 0.000733\n",
      "Epoch 340 | Train Loss: 0.000970 | Validation Loss: 0.000770\n",
      "Epoch 341 | Train Loss: 0.000727 | Validation Loss: 0.000677\n",
      "Epoch 342 | Train Loss: 0.000796 | Validation Loss: 0.001046\n",
      "Epoch 343 | Train Loss: 0.000765 | Validation Loss: 0.000556\n",
      "Epoch 344 | Train Loss: 0.000509 | Validation Loss: 0.000331\n",
      "Epoch 345 | Train Loss: 0.000577 | Validation Loss: 0.000474\n",
      "Epoch 346 | Train Loss: 0.000591 | Validation Loss: 0.000472\n",
      "Epoch 347 | Train Loss: 0.000626 | Validation Loss: 0.000479\n",
      "Epoch 348 | Train Loss: 0.000806 | Validation Loss: 0.000572\n",
      "Epoch 349 | Train Loss: 0.000891 | Validation Loss: 0.000714\n",
      "Epoch 350 | Train Loss: 0.000672 | Validation Loss: 0.000555\n",
      "Epoch 351 | Train Loss: 0.000521 | Validation Loss: 0.000344\n",
      "Epoch 352 | Train Loss: 0.000543 | Validation Loss: 0.000489\n",
      "Epoch 353 | Train Loss: 0.000650 | Validation Loss: 0.000620\n",
      "Epoch 354 | Train Loss: 0.000701 | Validation Loss: 0.000647\n",
      "Epoch 355 | Train Loss: 0.000700 | Validation Loss: 0.000668\n",
      "Epoch 356 | Train Loss: 0.000655 | Validation Loss: 0.000763\n",
      "Epoch 357 | Train Loss: 0.000729 | Validation Loss: 0.000591\n",
      "Epoch 358 | Train Loss: 0.000615 | Validation Loss: 0.000647\n",
      "Epoch 359 | Train Loss: 0.000609 | Validation Loss: 0.000499\n",
      "Epoch 360 | Train Loss: 0.000643 | Validation Loss: 0.000619\n",
      "Epoch 361 | Train Loss: 0.000450 | Validation Loss: 0.000316\n",
      "Epoch 362 | Train Loss: 0.000489 | Validation Loss: 0.000637\n",
      "Epoch 363 | Train Loss: 0.000697 | Validation Loss: 0.000747\n",
      "Epoch 364 | Train Loss: 0.001082 | Validation Loss: 0.000569\n",
      "Epoch 365 | Train Loss: 0.000591 | Validation Loss: 0.000465\n",
      "Epoch 366 | Train Loss: 0.000700 | Validation Loss: 0.000705\n",
      "Epoch 367 | Train Loss: 0.000724 | Validation Loss: 0.000568\n",
      "Epoch 368 | Train Loss: 0.000696 | Validation Loss: 0.000778\n",
      "Epoch 369 | Train Loss: 0.000717 | Validation Loss: 0.000478\n",
      "Epoch 370 | Train Loss: 0.000578 | Validation Loss: 0.000506\n",
      "Epoch 371 | Train Loss: 0.000634 | Validation Loss: 0.000699\n",
      "Epoch 372 | Train Loss: 0.000708 | Validation Loss: 0.000421\n",
      "Epoch 373 | Train Loss: 0.000460 | Validation Loss: 0.000403\n",
      "Epoch 374 | Train Loss: 0.000472 | Validation Loss: 0.000603\n",
      "Epoch 375 | Train Loss: 0.000502 | Validation Loss: 0.000638\n",
      "Epoch 376 | Train Loss: 0.000625 | Validation Loss: 0.000605\n",
      "Epoch 377 | Train Loss: 0.000581 | Validation Loss: 0.000467\n",
      "Epoch 378 | Train Loss: 0.000435 | Validation Loss: 0.000461\n",
      "Epoch 379 | Train Loss: 0.000409 | Validation Loss: 0.000377\n",
      "Epoch 380 | Train Loss: 0.000360 | Validation Loss: 0.000447\n",
      "Epoch 381 | Train Loss: 0.000398 | Validation Loss: 0.000317\n",
      "Epoch 382 | Train Loss: 0.000487 | Validation Loss: 0.000496\n",
      "Epoch 383 | Train Loss: 0.000478 | Validation Loss: 0.000383\n",
      "saved!\n",
      "Epoch 384 | Train Loss: 0.000539 | Validation Loss: 0.000261\n",
      "Epoch 385 | Train Loss: 0.000479 | Validation Loss: 0.000545\n",
      "Epoch 386 | Train Loss: 0.000470 | Validation Loss: 0.000507\n",
      "Epoch 387 | Train Loss: 0.000577 | Validation Loss: 0.000498\n",
      "Epoch 388 | Train Loss: 0.000567 | Validation Loss: 0.000749\n",
      "Epoch 389 | Train Loss: 0.000702 | Validation Loss: 0.000682\n",
      "Epoch 390 | Train Loss: 0.000573 | Validation Loss: 0.000681\n",
      "Epoch 391 | Train Loss: 0.000768 | Validation Loss: 0.000987\n",
      "Epoch 392 | Train Loss: 0.000550 | Validation Loss: 0.000652\n",
      "Epoch 393 | Train Loss: 0.000729 | Validation Loss: 0.000668\n",
      "Epoch 394 | Train Loss: 0.000829 | Validation Loss: 0.000915\n",
      "Epoch 395 | Train Loss: 0.000791 | Validation Loss: 0.000498\n",
      "Epoch 396 | Train Loss: 0.000724 | Validation Loss: 0.000394\n",
      "Epoch 397 | Train Loss: 0.000617 | Validation Loss: 0.000696\n",
      "Epoch 398 | Train Loss: 0.000970 | Validation Loss: 0.000603\n",
      "Epoch 399 | Train Loss: 0.001269 | Validation Loss: 0.001481\n",
      "Epoch 400 | Train Loss: 0.001217 | Validation Loss: 0.000761\n",
      "Epoch 401 | Train Loss: 0.000959 | Validation Loss: 0.000906\n",
      "Epoch 402 | Train Loss: 0.000912 | Validation Loss: 0.000631\n",
      "Epoch 403 | Train Loss: 0.000678 | Validation Loss: 0.000483\n",
      "Epoch 404 | Train Loss: 0.000488 | Validation Loss: 0.000372\n",
      "Epoch 405 | Train Loss: 0.000584 | Validation Loss: 0.000460\n",
      "Epoch 406 | Train Loss: 0.000509 | Validation Loss: 0.000330\n",
      "Epoch 407 | Train Loss: 0.000480 | Validation Loss: 0.000466\n",
      "Epoch 408 | Train Loss: 0.000528 | Validation Loss: 0.000392\n",
      "Epoch 409 | Train Loss: 0.000716 | Validation Loss: 0.000423\n",
      "Epoch 410 | Train Loss: 0.000513 | Validation Loss: 0.000572\n",
      "Epoch 411 | Train Loss: 0.000551 | Validation Loss: 0.000652\n",
      "Epoch 412 | Train Loss: 0.000815 | Validation Loss: 0.000546\n",
      "Epoch 413 | Train Loss: 0.000675 | Validation Loss: 0.000655\n",
      "Epoch 414 | Train Loss: 0.000691 | Validation Loss: 0.000662\n",
      "Epoch 415 | Train Loss: 0.000675 | Validation Loss: 0.000546\n",
      "Epoch 416 | Train Loss: 0.000635 | Validation Loss: 0.000692\n",
      "Epoch 417 | Train Loss: 0.000619 | Validation Loss: 0.000424\n",
      "Epoch 418 | Train Loss: 0.000593 | Validation Loss: 0.000742\n",
      "Epoch 419 | Train Loss: 0.000573 | Validation Loss: 0.000482\n",
      "Epoch 420 | Train Loss: 0.000539 | Validation Loss: 0.000559\n",
      "Epoch 421 | Train Loss: 0.000548 | Validation Loss: 0.000560\n",
      "Epoch 422 | Train Loss: 0.000745 | Validation Loss: 0.000533\n",
      "Epoch 423 | Train Loss: 0.000717 | Validation Loss: 0.000605\n",
      "Epoch 424 | Train Loss: 0.000660 | Validation Loss: 0.000390\n",
      "Epoch 425 | Train Loss: 0.000495 | Validation Loss: 0.000402\n",
      "Epoch 426 | Train Loss: 0.000568 | Validation Loss: 0.000400\n",
      "Epoch 427 | Train Loss: 0.000682 | Validation Loss: 0.000307\n",
      "Epoch 428 | Train Loss: 0.000474 | Validation Loss: 0.000412\n",
      "Epoch 429 | Train Loss: 0.000474 | Validation Loss: 0.000340\n",
      "Epoch 430 | Train Loss: 0.000466 | Validation Loss: 0.000559\n",
      "Epoch 431 | Train Loss: 0.000663 | Validation Loss: 0.001811\n",
      "Epoch 432 | Train Loss: 0.000580 | Validation Loss: 0.000778\n",
      "Epoch 433 | Train Loss: 0.000490 | Validation Loss: 0.000647\n",
      "Epoch 434 | Train Loss: 0.000540 | Validation Loss: 0.000336\n",
      "Epoch 435 | Train Loss: 0.000553 | Validation Loss: 0.000341\n",
      "Epoch 436 | Train Loss: 0.000557 | Validation Loss: 0.000363\n",
      "Epoch 437 | Train Loss: 0.000487 | Validation Loss: 0.000708\n",
      "Epoch 438 | Train Loss: 0.000584 | Validation Loss: 0.000824\n",
      "Epoch 439 | Train Loss: 0.000692 | Validation Loss: 0.000788\n",
      "Epoch 440 | Train Loss: 0.000658 | Validation Loss: 0.000389\n",
      "Epoch 441 | Train Loss: 0.000898 | Validation Loss: 0.001036\n",
      "Epoch 442 | Train Loss: 0.000844 | Validation Loss: 0.000628\n",
      "Epoch 443 | Train Loss: 0.000660 | Validation Loss: 0.000522\n",
      "Epoch 444 | Train Loss: 0.000544 | Validation Loss: 0.000455\n",
      "Epoch 445 | Train Loss: 0.000742 | Validation Loss: 0.000594\n",
      "Epoch 446 | Train Loss: 0.000527 | Validation Loss: 0.000552\n",
      "Epoch 447 | Train Loss: 0.000531 | Validation Loss: 0.000454\n",
      "Epoch 448 | Train Loss: 0.000518 | Validation Loss: 0.000313\n",
      "Epoch 449 | Train Loss: 0.000735 | Validation Loss: 0.000387\n",
      "Epoch 450 | Train Loss: 0.000500 | Validation Loss: 0.000323\n",
      "Epoch 451 | Train Loss: 0.000405 | Validation Loss: 0.000265\n",
      "saved!\n",
      "Epoch 452 | Train Loss: 0.000352 | Validation Loss: 0.000225\n",
      "Epoch 453 | Train Loss: 0.000433 | Validation Loss: 0.000298\n",
      "Epoch 454 | Train Loss: 0.000396 | Validation Loss: 0.000226\n",
      "Epoch 455 | Train Loss: 0.000417 | Validation Loss: 0.000325\n",
      "Epoch 456 | Train Loss: 0.000299 | Validation Loss: 0.000274\n",
      "Epoch 457 | Train Loss: 0.000400 | Validation Loss: 0.000270\n",
      "Epoch 458 | Train Loss: 0.000512 | Validation Loss: 0.000463\n",
      "Epoch 459 | Train Loss: 0.000504 | Validation Loss: 0.000466\n",
      "Epoch 460 | Train Loss: 0.000490 | Validation Loss: 0.000321\n",
      "Epoch 461 | Train Loss: 0.000422 | Validation Loss: 0.000434\n",
      "Epoch 462 | Train Loss: 0.000509 | Validation Loss: 0.000652\n",
      "Epoch 463 | Train Loss: 0.000393 | Validation Loss: 0.000342\n",
      "Epoch 464 | Train Loss: 0.000440 | Validation Loss: 0.000321\n",
      "Epoch 465 | Train Loss: 0.000290 | Validation Loss: 0.000233\n",
      "Epoch 466 | Train Loss: 0.000325 | Validation Loss: 0.000276\n",
      "Epoch 467 | Train Loss: 0.000310 | Validation Loss: 0.000287\n",
      "saved!\n",
      "Epoch 468 | Train Loss: 0.000455 | Validation Loss: 0.000158\n",
      "saved!\n",
      "Epoch 469 | Train Loss: 0.000456 | Validation Loss: 0.000147\n",
      "Epoch 470 | Train Loss: 0.000351 | Validation Loss: 0.000226\n",
      "Epoch 471 | Train Loss: 0.000478 | Validation Loss: 0.000273\n",
      "Epoch 472 | Train Loss: 0.000314 | Validation Loss: 0.000280\n",
      "Epoch 473 | Train Loss: 0.000442 | Validation Loss: 0.000155\n",
      "Epoch 474 | Train Loss: 0.000261 | Validation Loss: 0.000188\n",
      "Epoch 475 | Train Loss: 0.000279 | Validation Loss: 0.000165\n",
      "Epoch 476 | Train Loss: 0.000311 | Validation Loss: 0.000252\n",
      "Epoch 477 | Train Loss: 0.000472 | Validation Loss: 0.000389\n",
      "Epoch 478 | Train Loss: 0.000625 | Validation Loss: 0.000869\n",
      "Epoch 479 | Train Loss: 0.001090 | Validation Loss: 0.000999\n",
      "Epoch 480 | Train Loss: 0.000678 | Validation Loss: 0.000415\n",
      "Epoch 481 | Train Loss: 0.000559 | Validation Loss: 0.000354\n",
      "Epoch 482 | Train Loss: 0.000503 | Validation Loss: 0.000404\n",
      "Epoch 483 | Train Loss: 0.000484 | Validation Loss: 0.000392\n",
      "Epoch 484 | Train Loss: 0.000324 | Validation Loss: 0.000279\n",
      "Epoch 485 | Train Loss: 0.000277 | Validation Loss: 0.000304\n",
      "Epoch 486 | Train Loss: 0.000438 | Validation Loss: 0.000208\n",
      "Epoch 487 | Train Loss: 0.000555 | Validation Loss: 0.000480\n",
      "Epoch 488 | Train Loss: 0.000652 | Validation Loss: 0.000851\n",
      "Epoch 489 | Train Loss: 0.000533 | Validation Loss: 0.000356\n",
      "Epoch 490 | Train Loss: 0.000669 | Validation Loss: 0.000315\n",
      "Epoch 491 | Train Loss: 0.000583 | Validation Loss: 0.000474\n",
      "Epoch 492 | Train Loss: 0.000568 | Validation Loss: 0.000631\n",
      "Epoch 493 | Train Loss: 0.000552 | Validation Loss: 0.000320\n",
      "Epoch 494 | Train Loss: 0.000632 | Validation Loss: 0.000548\n",
      "Epoch 495 | Train Loss: 0.000611 | Validation Loss: 0.000723\n",
      "Epoch 496 | Train Loss: 0.000991 | Validation Loss: 0.000848\n",
      "Epoch 497 | Train Loss: 0.000950 | Validation Loss: 0.000905\n",
      "Epoch 498 | Train Loss: 0.000999 | Validation Loss: 0.001031\n",
      "Epoch 499 | Train Loss: 0.000977 | Validation Loss: 0.000854\n",
      "Epoch 500 | Train Loss: 0.000807 | Validation Loss: 0.000868\n",
      "Epoch 501 | Train Loss: 0.000770 | Validation Loss: 0.000714\n",
      "Epoch 502 | Train Loss: 0.000674 | Validation Loss: 0.000533\n",
      "Epoch 503 | Train Loss: 0.000497 | Validation Loss: 0.000445\n",
      "Epoch 504 | Train Loss: 0.000496 | Validation Loss: 0.000612\n",
      "Epoch 505 | Train Loss: 0.000459 | Validation Loss: 0.000375\n",
      "Epoch 506 | Train Loss: 0.000501 | Validation Loss: 0.000423\n",
      "Epoch 507 | Train Loss: 0.000473 | Validation Loss: 0.000508\n",
      "Epoch 508 | Train Loss: 0.000549 | Validation Loss: 0.000504\n",
      "Epoch 509 | Train Loss: 0.000443 | Validation Loss: 0.000296\n",
      "Epoch 510 | Train Loss: 0.000412 | Validation Loss: 0.000327\n",
      "Epoch 511 | Train Loss: 0.000557 | Validation Loss: 0.000437\n",
      "Epoch 512 | Train Loss: 0.000345 | Validation Loss: 0.000373\n",
      "Epoch 513 | Train Loss: 0.000461 | Validation Loss: 0.000396\n",
      "Epoch 514 | Train Loss: 0.000340 | Validation Loss: 0.000251\n",
      "Epoch 515 | Train Loss: 0.000477 | Validation Loss: 0.000557\n",
      "Epoch 516 | Train Loss: 0.000516 | Validation Loss: 0.000627\n",
      "Epoch 517 | Train Loss: 0.000904 | Validation Loss: 0.001084\n",
      "Epoch 518 | Train Loss: 0.000792 | Validation Loss: 0.000850\n",
      "Epoch 519 | Train Loss: 0.000827 | Validation Loss: 0.000646\n",
      "Epoch 520 | Train Loss: 0.000589 | Validation Loss: 0.000434\n",
      "Epoch 521 | Train Loss: 0.000634 | Validation Loss: 0.000422\n",
      "Epoch 522 | Train Loss: 0.000701 | Validation Loss: 0.000565\n",
      "Epoch 523 | Train Loss: 0.000853 | Validation Loss: 0.000659\n",
      "Epoch 524 | Train Loss: 0.001062 | Validation Loss: 0.001233\n",
      "Epoch 525 | Train Loss: 0.000833 | Validation Loss: 0.000550\n",
      "Epoch 526 | Train Loss: 0.000674 | Validation Loss: 0.000741\n",
      "Epoch 527 | Train Loss: 0.000643 | Validation Loss: 0.000571\n",
      "Epoch 528 | Train Loss: 0.000824 | Validation Loss: 0.000368\n",
      "Epoch 529 | Train Loss: 0.000576 | Validation Loss: 0.000341\n",
      "Epoch 530 | Train Loss: 0.000483 | Validation Loss: 0.000320\n",
      "Epoch 531 | Train Loss: 0.000521 | Validation Loss: 0.000393\n",
      "Epoch 532 | Train Loss: 0.000576 | Validation Loss: 0.000544\n",
      "Epoch 533 | Train Loss: 0.000494 | Validation Loss: 0.000528\n",
      "Epoch 534 | Train Loss: 0.000760 | Validation Loss: 0.000577\n",
      "Epoch 535 | Train Loss: 0.000646 | Validation Loss: 0.000906\n",
      "Epoch 536 | Train Loss: 0.000729 | Validation Loss: 0.000551\n",
      "Epoch 537 | Train Loss: 0.000602 | Validation Loss: 0.001081\n",
      "Epoch 538 | Train Loss: 0.000627 | Validation Loss: 0.000306\n",
      "Epoch 539 | Train Loss: 0.000651 | Validation Loss: 0.000420\n",
      "Epoch 540 | Train Loss: 0.000628 | Validation Loss: 0.000556\n",
      "Epoch 541 | Train Loss: 0.000734 | Validation Loss: 0.000384\n",
      "Epoch 542 | Train Loss: 0.000623 | Validation Loss: 0.000293\n",
      "Epoch 543 | Train Loss: 0.000480 | Validation Loss: 0.000369\n",
      "Epoch 544 | Train Loss: 0.000526 | Validation Loss: 0.000354\n",
      "Epoch 545 | Train Loss: 0.000570 | Validation Loss: 0.000489\n",
      "Epoch 546 | Train Loss: 0.000542 | Validation Loss: 0.000479\n",
      "Epoch 547 | Train Loss: 0.000453 | Validation Loss: 0.000362\n",
      "Epoch 548 | Train Loss: 0.000495 | Validation Loss: 0.000418\n",
      "Epoch 549 | Train Loss: 0.000347 | Validation Loss: 0.000286\n",
      "Epoch 550 | Train Loss: 0.000347 | Validation Loss: 0.000334\n",
      "Epoch 551 | Train Loss: 0.000369 | Validation Loss: 0.000417\n",
      "Epoch 552 | Train Loss: 0.000360 | Validation Loss: 0.000462\n",
      "Epoch 553 | Train Loss: 0.000546 | Validation Loss: 0.000596\n",
      "Epoch 554 | Train Loss: 0.000715 | Validation Loss: 0.001256\n",
      "Epoch 555 | Train Loss: 0.001262 | Validation Loss: 0.000721\n",
      "Epoch 556 | Train Loss: 0.000831 | Validation Loss: 0.000876\n",
      "Epoch 557 | Train Loss: 0.000903 | Validation Loss: 0.000391\n",
      "Epoch 558 | Train Loss: 0.000774 | Validation Loss: 0.000499\n",
      "Epoch 559 | Train Loss: 0.000924 | Validation Loss: 0.000431\n",
      "Epoch 560 | Train Loss: 0.000806 | Validation Loss: 0.000440\n",
      "Epoch 561 | Train Loss: 0.000528 | Validation Loss: 0.000614\n",
      "Epoch 562 | Train Loss: 0.000668 | Validation Loss: 0.000537\n",
      "Epoch 563 | Train Loss: 0.000708 | Validation Loss: 0.000908\n",
      "Epoch 564 | Train Loss: 0.000739 | Validation Loss: 0.000628\n",
      "Epoch 565 | Train Loss: 0.000865 | Validation Loss: 0.000785\n",
      "Epoch 566 | Train Loss: 0.000693 | Validation Loss: 0.000491\n",
      "Epoch 567 | Train Loss: 0.000628 | Validation Loss: 0.000279\n",
      "Epoch 568 | Train Loss: 0.000519 | Validation Loss: 0.000326\n",
      "Epoch 569 | Train Loss: 0.000577 | Validation Loss: 0.000473\n",
      "Epoch 570 | Train Loss: 0.000617 | Validation Loss: 0.000634\n",
      "Epoch 571 | Train Loss: 0.000654 | Validation Loss: 0.000514\n",
      "Epoch 572 | Train Loss: 0.000852 | Validation Loss: 0.000829\n",
      "Epoch 573 | Train Loss: 0.000816 | Validation Loss: 0.000358\n",
      "Epoch 574 | Train Loss: 0.000766 | Validation Loss: 0.000414\n",
      "Epoch 575 | Train Loss: 0.000619 | Validation Loss: 0.000870\n",
      "Epoch 576 | Train Loss: 0.000655 | Validation Loss: 0.001253\n",
      "Epoch 577 | Train Loss: 0.000922 | Validation Loss: 0.000647\n",
      "Epoch 578 | Train Loss: 0.000824 | Validation Loss: 0.000994\n",
      "Epoch 579 | Train Loss: 0.000766 | Validation Loss: 0.000420\n",
      "Epoch 580 | Train Loss: 0.000710 | Validation Loss: 0.000918\n",
      "Epoch 581 | Train Loss: 0.001119 | Validation Loss: 0.001503\n",
      "Epoch 582 | Train Loss: 0.000942 | Validation Loss: 0.002522\n",
      "Epoch 583 | Train Loss: 0.000884 | Validation Loss: 0.000658\n",
      "Epoch 584 | Train Loss: 0.000769 | Validation Loss: 0.001007\n",
      "Epoch 585 | Train Loss: 0.000640 | Validation Loss: 0.000837\n",
      "Epoch 586 | Train Loss: 0.000791 | Validation Loss: 0.000332\n",
      "Epoch 587 | Train Loss: 0.000761 | Validation Loss: 0.001000\n",
      "Epoch 588 | Train Loss: 0.000724 | Validation Loss: 0.000749\n",
      "Epoch 589 | Train Loss: 0.000629 | Validation Loss: 0.000559\n",
      "Epoch 590 | Train Loss: 0.000690 | Validation Loss: 0.000321\n",
      "Epoch 591 | Train Loss: 0.000658 | Validation Loss: 0.000292\n",
      "Epoch 592 | Train Loss: 0.000541 | Validation Loss: 0.000373\n",
      "Epoch 593 | Train Loss: 0.000731 | Validation Loss: 0.000474\n",
      "Epoch 594 | Train Loss: 0.000806 | Validation Loss: 0.001188\n",
      "Epoch 595 | Train Loss: 0.000960 | Validation Loss: 0.000518\n",
      "Epoch 596 | Train Loss: 0.000703 | Validation Loss: 0.000354\n",
      "Epoch 597 | Train Loss: 0.000553 | Validation Loss: 0.000246\n",
      "Epoch 598 | Train Loss: 0.000383 | Validation Loss: 0.000254\n",
      "Epoch 599 | Train Loss: 0.000402 | Validation Loss: 0.000278\n",
      "Epoch 600 | Train Loss: 0.000374 | Validation Loss: 0.000222\n",
      "Epoch 601 | Train Loss: 0.000458 | Validation Loss: 0.000208\n",
      "Epoch 602 | Train Loss: 0.000467 | Validation Loss: 0.000200\n",
      "Epoch 603 | Train Loss: 0.000414 | Validation Loss: 0.000266\n",
      "Epoch 604 | Train Loss: 0.000433 | Validation Loss: 0.000355\n",
      "Epoch 605 | Train Loss: 0.000439 | Validation Loss: 0.000261\n",
      "Epoch 606 | Train Loss: 0.000321 | Validation Loss: 0.000288\n",
      "Epoch 607 | Train Loss: 0.000467 | Validation Loss: 0.000239\n",
      "Epoch 608 | Train Loss: 0.000507 | Validation Loss: 0.000324\n",
      "Epoch 609 | Train Loss: 0.000507 | Validation Loss: 0.000577\n",
      "Epoch 610 | Train Loss: 0.000663 | Validation Loss: 0.000279\n",
      "Epoch 611 | Train Loss: 0.000480 | Validation Loss: 0.000215\n",
      "Epoch 612 | Train Loss: 0.000379 | Validation Loss: 0.000242\n",
      "Epoch 613 | Train Loss: 0.000304 | Validation Loss: 0.000193\n",
      "Epoch 614 | Train Loss: 0.000245 | Validation Loss: 0.000214\n",
      "Epoch 615 | Train Loss: 0.000257 | Validation Loss: 0.000276\n",
      "Epoch 616 | Train Loss: 0.000360 | Validation Loss: 0.000160\n",
      "Epoch 617 | Train Loss: 0.000321 | Validation Loss: 0.000255\n",
      "Epoch 618 | Train Loss: 0.000244 | Validation Loss: 0.000231\n",
      "Epoch 619 | Train Loss: 0.000315 | Validation Loss: 0.000198\n",
      "saved!\n",
      "Epoch 620 | Train Loss: 0.000285 | Validation Loss: 0.000122\n",
      "saved!\n",
      "Epoch 621 | Train Loss: 0.000264 | Validation Loss: 0.000073\n",
      "Epoch 622 | Train Loss: 0.000266 | Validation Loss: 0.000106\n",
      "Epoch 623 | Train Loss: 0.000513 | Validation Loss: 0.000226\n",
      "Epoch 624 | Train Loss: 0.000510 | Validation Loss: 0.000343\n",
      "Epoch 625 | Train Loss: 0.000520 | Validation Loss: 0.000359\n",
      "Epoch 626 | Train Loss: 0.000350 | Validation Loss: 0.000247\n",
      "Epoch 627 | Train Loss: 0.000332 | Validation Loss: 0.000215\n",
      "Epoch 628 | Train Loss: 0.000302 | Validation Loss: 0.000236\n",
      "Epoch 629 | Train Loss: 0.000526 | Validation Loss: 0.000281\n",
      "Epoch 630 | Train Loss: 0.000486 | Validation Loss: 0.000133\n",
      "Epoch 631 | Train Loss: 0.000296 | Validation Loss: 0.000150\n",
      "Epoch 632 | Train Loss: 0.000300 | Validation Loss: 0.000144\n",
      "Epoch 633 | Train Loss: 0.000419 | Validation Loss: 0.000153\n",
      "Epoch 634 | Train Loss: 0.000329 | Validation Loss: 0.000209\n",
      "Epoch 635 | Train Loss: 0.000457 | Validation Loss: 0.000179\n",
      "Epoch 636 | Train Loss: 0.000390 | Validation Loss: 0.000158\n",
      "Epoch 637 | Train Loss: 0.000288 | Validation Loss: 0.000136\n",
      "Epoch 638 | Train Loss: 0.000291 | Validation Loss: 0.000128\n",
      "Epoch 639 | Train Loss: 0.000298 | Validation Loss: 0.000106\n",
      "Epoch 640 | Train Loss: 0.000226 | Validation Loss: 0.000141\n",
      "Epoch 641 | Train Loss: 0.000196 | Validation Loss: 0.000139\n",
      "Epoch 642 | Train Loss: 0.000224 | Validation Loss: 0.000094\n",
      "saved!\n",
      "Epoch 643 | Train Loss: 0.000219 | Validation Loss: 0.000070\n",
      "Epoch 644 | Train Loss: 0.000202 | Validation Loss: 0.000082\n",
      "Epoch 645 | Train Loss: 0.000183 | Validation Loss: 0.000082\n",
      "Epoch 646 | Train Loss: 0.000284 | Validation Loss: 0.000097\n",
      "Epoch 647 | Train Loss: 0.000195 | Validation Loss: 0.000086\n",
      "Epoch 648 | Train Loss: 0.000144 | Validation Loss: 0.000154\n",
      "Epoch 649 | Train Loss: 0.000270 | Validation Loss: 0.000107\n",
      "Epoch 650 | Train Loss: 0.000342 | Validation Loss: 0.000095\n",
      "Epoch 651 | Train Loss: 0.000391 | Validation Loss: 0.000269\n",
      "Epoch 652 | Train Loss: 0.000501 | Validation Loss: 0.000405\n",
      "Epoch 653 | Train Loss: 0.000372 | Validation Loss: 0.000227\n",
      "Epoch 654 | Train Loss: 0.000296 | Validation Loss: 0.000153\n",
      "Epoch 655 | Train Loss: 0.000377 | Validation Loss: 0.000207\n",
      "Epoch 656 | Train Loss: 0.000250 | Validation Loss: 0.000216\n",
      "Epoch 657 | Train Loss: 0.000311 | Validation Loss: 0.000239\n",
      "Epoch 658 | Train Loss: 0.000376 | Validation Loss: 0.000138\n",
      "Epoch 659 | Train Loss: 0.000522 | Validation Loss: 0.000158\n",
      "Epoch 660 | Train Loss: 0.000387 | Validation Loss: 0.000273\n",
      "Epoch 661 | Train Loss: 0.000383 | Validation Loss: 0.000109\n",
      "Epoch 662 | Train Loss: 0.000250 | Validation Loss: 0.000221\n",
      "Epoch 663 | Train Loss: 0.000307 | Validation Loss: 0.000280\n",
      "Epoch 664 | Train Loss: 0.000266 | Validation Loss: 0.000203\n",
      "Epoch 665 | Train Loss: 0.000301 | Validation Loss: 0.000123\n",
      "Epoch 666 | Train Loss: 0.000317 | Validation Loss: 0.000114\n",
      "Epoch 667 | Train Loss: 0.000259 | Validation Loss: 0.000124\n",
      "Epoch 668 | Train Loss: 0.000268 | Validation Loss: 0.000086\n",
      "Epoch 669 | Train Loss: 0.000251 | Validation Loss: 0.000073\n",
      "Epoch 670 | Train Loss: 0.000167 | Validation Loss: 0.000133\n",
      "Epoch 671 | Train Loss: 0.000334 | Validation Loss: 0.000184\n",
      "Epoch 672 | Train Loss: 0.000269 | Validation Loss: 0.000173\n",
      "Epoch 673 | Train Loss: 0.000354 | Validation Loss: 0.000135\n",
      "Epoch 674 | Train Loss: 0.000276 | Validation Loss: 0.000193\n",
      "Epoch 675 | Train Loss: 0.000419 | Validation Loss: 0.000308\n",
      "Epoch 676 | Train Loss: 0.000471 | Validation Loss: 0.000303\n",
      "Epoch 677 | Train Loss: 0.000504 | Validation Loss: 0.000357\n",
      "Epoch 678 | Train Loss: 0.000421 | Validation Loss: 0.000331\n",
      "Epoch 679 | Train Loss: 0.000446 | Validation Loss: 0.000260\n",
      "Epoch 680 | Train Loss: 0.000301 | Validation Loss: 0.000289\n",
      "Epoch 681 | Train Loss: 0.000385 | Validation Loss: 0.000198\n",
      "Epoch 682 | Train Loss: 0.000274 | Validation Loss: 0.000161\n",
      "Epoch 683 | Train Loss: 0.000307 | Validation Loss: 0.000154\n",
      "Epoch 684 | Train Loss: 0.000388 | Validation Loss: 0.000336\n",
      "Epoch 685 | Train Loss: 0.000579 | Validation Loss: 0.000101\n",
      "Epoch 686 | Train Loss: 0.000247 | Validation Loss: 0.000125\n",
      "Epoch 687 | Train Loss: 0.000295 | Validation Loss: 0.000076\n",
      "Epoch 688 | Train Loss: 0.000318 | Validation Loss: 0.000125\n",
      "Epoch 689 | Train Loss: 0.000427 | Validation Loss: 0.000235\n",
      "Epoch 690 | Train Loss: 0.000405 | Validation Loss: 0.000261\n",
      "Epoch 691 | Train Loss: 0.000388 | Validation Loss: 0.000220\n",
      "Epoch 692 | Train Loss: 0.000305 | Validation Loss: 0.000244\n",
      "Epoch 693 | Train Loss: 0.000365 | Validation Loss: 0.000271\n",
      "Epoch 694 | Train Loss: 0.000465 | Validation Loss: 0.000428\n",
      "Epoch 695 | Train Loss: 0.000423 | Validation Loss: 0.000235\n",
      "Epoch 696 | Train Loss: 0.000333 | Validation Loss: 0.000212\n",
      "Epoch 697 | Train Loss: 0.000282 | Validation Loss: 0.000171\n",
      "Epoch 698 | Train Loss: 0.000246 | Validation Loss: 0.000127\n",
      "Epoch 699 | Train Loss: 0.000271 | Validation Loss: 0.000112\n",
      "Epoch 700 | Train Loss: 0.000260 | Validation Loss: 0.000196\n",
      "Epoch 701 | Train Loss: 0.000223 | Validation Loss: 0.000158\n",
      "Epoch 702 | Train Loss: 0.000174 | Validation Loss: 0.000160\n",
      "Epoch 703 | Train Loss: 0.000197 | Validation Loss: 0.000197\n",
      "Epoch 704 | Train Loss: 0.000571 | Validation Loss: 0.002581\n",
      "Epoch 705 | Train Loss: 0.000463 | Validation Loss: 0.000354\n",
      "Epoch 706 | Train Loss: 0.000393 | Validation Loss: 0.000357\n",
      "Epoch 707 | Train Loss: 0.000457 | Validation Loss: 0.000186\n",
      "Epoch 708 | Train Loss: 0.000449 | Validation Loss: 0.000187\n",
      "Epoch 709 | Train Loss: 0.000243 | Validation Loss: 0.000158\n",
      "Epoch 710 | Train Loss: 0.000251 | Validation Loss: 0.000140\n",
      "Epoch 711 | Train Loss: 0.000321 | Validation Loss: 0.000154\n",
      "Epoch 712 | Train Loss: 0.000289 | Validation Loss: 0.000197\n",
      "Epoch 713 | Train Loss: 0.000228 | Validation Loss: 0.000208\n",
      "Epoch 714 | Train Loss: 0.000337 | Validation Loss: 0.000315\n",
      "Epoch 715 | Train Loss: 0.000408 | Validation Loss: 0.000228\n",
      "Epoch 716 | Train Loss: 0.000312 | Validation Loss: 0.000127\n",
      "Epoch 717 | Train Loss: 0.000220 | Validation Loss: 0.000123\n",
      "Epoch 718 | Train Loss: 0.000241 | Validation Loss: 0.000071\n",
      "Epoch 719 | Train Loss: 0.000200 | Validation Loss: 0.000097\n",
      "Epoch 720 | Train Loss: 0.000364 | Validation Loss: 0.000230\n",
      "Epoch 721 | Train Loss: 0.000365 | Validation Loss: 0.000234\n",
      "Epoch 722 | Train Loss: 0.000254 | Validation Loss: 0.000322\n",
      "Epoch 723 | Train Loss: 0.000404 | Validation Loss: 0.000375\n",
      "Epoch 724 | Train Loss: 0.000381 | Validation Loss: 0.000253\n",
      "Epoch 725 | Train Loss: 0.000549 | Validation Loss: 0.000236\n",
      "Epoch 726 | Train Loss: 0.000393 | Validation Loss: 0.000285\n",
      "Epoch 727 | Train Loss: 0.000367 | Validation Loss: 0.000239\n",
      "Epoch 728 | Train Loss: 0.000414 | Validation Loss: 0.000439\n",
      "Epoch 729 | Train Loss: 0.000424 | Validation Loss: 0.000233\n",
      "Epoch 730 | Train Loss: 0.000333 | Validation Loss: 0.000190\n",
      "Epoch 731 | Train Loss: 0.000347 | Validation Loss: 0.000226\n",
      "Epoch 732 | Train Loss: 0.000352 | Validation Loss: 0.000234\n",
      "Epoch 733 | Train Loss: 0.000286 | Validation Loss: 0.000181\n",
      "Epoch 734 | Train Loss: 0.000236 | Validation Loss: 0.000145\n",
      "Epoch 735 | Train Loss: 0.000401 | Validation Loss: 0.000134\n",
      "Epoch 736 | Train Loss: 0.000389 | Validation Loss: 0.000157\n",
      "Epoch 737 | Train Loss: 0.000353 | Validation Loss: 0.000178\n",
      "Epoch 738 | Train Loss: 0.000216 | Validation Loss: 0.000113\n",
      "Epoch 739 | Train Loss: 0.000251 | Validation Loss: 0.000138\n",
      "Epoch 740 | Train Loss: 0.000270 | Validation Loss: 0.000275\n",
      "Epoch 741 | Train Loss: 0.000488 | Validation Loss: 0.000332\n",
      "Epoch 742 | Train Loss: 0.000366 | Validation Loss: 0.000148\n",
      "Epoch 743 | Train Loss: 0.000673 | Validation Loss: 0.000403\n",
      "Epoch 744 | Train Loss: 0.000525 | Validation Loss: 0.000440\n",
      "Epoch 745 | Train Loss: 0.000506 | Validation Loss: 0.000359\n",
      "Epoch 746 | Train Loss: 0.000417 | Validation Loss: 0.000226\n",
      "Epoch 747 | Train Loss: 0.000347 | Validation Loss: 0.000198\n",
      "Epoch 748 | Train Loss: 0.000221 | Validation Loss: 0.000142\n",
      "Epoch 749 | Train Loss: 0.000237 | Validation Loss: 0.000081\n",
      "Epoch 750 | Train Loss: 0.000297 | Validation Loss: 0.000092\n",
      "Epoch 751 | Train Loss: 0.000297 | Validation Loss: 0.000129\n",
      "Epoch 752 | Train Loss: 0.000232 | Validation Loss: 0.000112\n",
      "Epoch 753 | Train Loss: 0.000299 | Validation Loss: 0.000167\n",
      "Epoch 754 | Train Loss: 0.000314 | Validation Loss: 0.000178\n",
      "Epoch 755 | Train Loss: 0.000328 | Validation Loss: 0.000373\n",
      "Epoch 756 | Train Loss: 0.000342 | Validation Loss: 0.000263\n",
      "Epoch 757 | Train Loss: 0.000327 | Validation Loss: 0.000148\n",
      "Epoch 758 | Train Loss: 0.000283 | Validation Loss: 0.000346\n",
      "Epoch 759 | Train Loss: 0.000236 | Validation Loss: 0.000144\n",
      "Epoch 760 | Train Loss: 0.000260 | Validation Loss: 0.000175\n",
      "Epoch 761 | Train Loss: 0.000203 | Validation Loss: 0.000214\n",
      "Epoch 762 | Train Loss: 0.000550 | Validation Loss: 0.000208\n",
      "Epoch 763 | Train Loss: 0.000495 | Validation Loss: 0.000331\n",
      "Epoch 764 | Train Loss: 0.000541 | Validation Loss: 0.000277\n",
      "Epoch 765 | Train Loss: 0.000339 | Validation Loss: 0.000320\n",
      "Epoch 766 | Train Loss: 0.000361 | Validation Loss: 0.000152\n",
      "Epoch 767 | Train Loss: 0.000278 | Validation Loss: 0.000187\n",
      "Epoch 768 | Train Loss: 0.000352 | Validation Loss: 0.000194\n",
      "Epoch 769 | Train Loss: 0.000275 | Validation Loss: 0.000187\n",
      "Epoch 770 | Train Loss: 0.000326 | Validation Loss: 0.000204\n",
      "Epoch 771 | Train Loss: 0.000460 | Validation Loss: 0.000144\n",
      "Epoch 772 | Train Loss: 0.000263 | Validation Loss: 0.000133\n",
      "Epoch 773 | Train Loss: 0.000194 | Validation Loss: 0.000094\n",
      "Epoch 774 | Train Loss: 0.000215 | Validation Loss: 0.000094\n",
      "saved!\n",
      "Epoch 775 | Train Loss: 0.000142 | Validation Loss: 0.000063\n",
      "Epoch 776 | Train Loss: 0.000227 | Validation Loss: 0.000137\n",
      "Epoch 777 | Train Loss: 0.000367 | Validation Loss: 0.000209\n",
      "Epoch 778 | Train Loss: 0.000284 | Validation Loss: 0.000181\n",
      "Epoch 779 | Train Loss: 0.000271 | Validation Loss: 0.000093\n",
      "Epoch 780 | Train Loss: 0.000215 | Validation Loss: 0.000084\n",
      "saved!\n",
      "Epoch 781 | Train Loss: 0.000199 | Validation Loss: 0.000055\n",
      "Epoch 782 | Train Loss: 0.000294 | Validation Loss: 0.000101\n",
      "Epoch 783 | Train Loss: 0.000249 | Validation Loss: 0.000121\n",
      "Epoch 784 | Train Loss: 0.000350 | Validation Loss: 0.000182\n",
      "Epoch 785 | Train Loss: 0.000405 | Validation Loss: 0.000215\n",
      "Epoch 786 | Train Loss: 0.000455 | Validation Loss: 0.000287\n",
      "Epoch 787 | Train Loss: 0.000440 | Validation Loss: 0.000203\n",
      "Epoch 788 | Train Loss: 0.000300 | Validation Loss: 0.000170\n",
      "Epoch 789 | Train Loss: 0.000261 | Validation Loss: 0.000182\n",
      "Epoch 790 | Train Loss: 0.000243 | Validation Loss: 0.000210\n",
      "Epoch 791 | Train Loss: 0.000407 | Validation Loss: 0.000164\n",
      "Epoch 792 | Train Loss: 0.000336 | Validation Loss: 0.000219\n",
      "Epoch 793 | Train Loss: 0.000320 | Validation Loss: 0.000205\n",
      "Epoch 794 | Train Loss: 0.000431 | Validation Loss: 0.000191\n",
      "Epoch 795 | Train Loss: 0.000390 | Validation Loss: 0.000192\n",
      "Epoch 796 | Train Loss: 0.000233 | Validation Loss: 0.000165\n",
      "Epoch 797 | Train Loss: 0.000351 | Validation Loss: 0.000124\n",
      "Epoch 798 | Train Loss: 0.000363 | Validation Loss: 0.000147\n",
      "Epoch 799 | Train Loss: 0.000435 | Validation Loss: 0.000278\n",
      "Epoch 800 | Train Loss: 0.000352 | Validation Loss: 0.000319\n",
      "Epoch 801 | Train Loss: 0.000367 | Validation Loss: 0.000189\n",
      "Epoch 802 | Train Loss: 0.000339 | Validation Loss: 0.000163\n",
      "Epoch 803 | Train Loss: 0.000322 | Validation Loss: 0.000257\n",
      "Epoch 804 | Train Loss: 0.000494 | Validation Loss: 0.000371\n",
      "Epoch 805 | Train Loss: 0.000562 | Validation Loss: 0.000291\n",
      "Epoch 806 | Train Loss: 0.000401 | Validation Loss: 0.000252\n",
      "Epoch 807 | Train Loss: 0.000560 | Validation Loss: 0.000288\n",
      "Epoch 808 | Train Loss: 0.000607 | Validation Loss: 0.000418\n",
      "Epoch 809 | Train Loss: 0.000531 | Validation Loss: 0.000395\n",
      "Epoch 810 | Train Loss: 0.000417 | Validation Loss: 0.000279\n",
      "Epoch 811 | Train Loss: 0.000461 | Validation Loss: 0.000291\n",
      "Epoch 812 | Train Loss: 0.000257 | Validation Loss: 0.000162\n",
      "Epoch 813 | Train Loss: 0.000278 | Validation Loss: 0.000215\n",
      "Epoch 814 | Train Loss: 0.000292 | Validation Loss: 0.000170\n",
      "Epoch 815 | Train Loss: 0.000244 | Validation Loss: 0.000142\n",
      "Epoch 816 | Train Loss: 0.000307 | Validation Loss: 0.000207\n",
      "Epoch 817 | Train Loss: 0.000263 | Validation Loss: 0.000077\n",
      "Epoch 818 | Train Loss: 0.000255 | Validation Loss: 0.000126\n",
      "Epoch 819 | Train Loss: 0.000338 | Validation Loss: 0.000154\n",
      "Epoch 820 | Train Loss: 0.000246 | Validation Loss: 0.000180\n",
      "Epoch 821 | Train Loss: 0.000220 | Validation Loss: 0.000143\n",
      "Epoch 822 | Train Loss: 0.000257 | Validation Loss: 0.000132\n",
      "Epoch 823 | Train Loss: 0.000162 | Validation Loss: 0.000163\n",
      "Epoch 824 | Train Loss: 0.000161 | Validation Loss: 0.000094\n",
      "Epoch 825 | Train Loss: 0.000225 | Validation Loss: 0.000128\n",
      "Epoch 826 | Train Loss: 0.000202 | Validation Loss: 0.000092\n",
      "Epoch 827 | Train Loss: 0.000174 | Validation Loss: 0.000133\n",
      "Epoch 828 | Train Loss: 0.000204 | Validation Loss: 0.000131\n",
      "Epoch 829 | Train Loss: 0.000236 | Validation Loss: 0.000145\n",
      "Epoch 830 | Train Loss: 0.000189 | Validation Loss: 0.000131\n",
      "Epoch 831 | Train Loss: 0.000236 | Validation Loss: 0.000148\n",
      "Epoch 832 | Train Loss: 0.000240 | Validation Loss: 0.000161\n",
      "Epoch 833 | Train Loss: 0.000311 | Validation Loss: 0.000186\n",
      "Epoch 834 | Train Loss: 0.000461 | Validation Loss: 0.000437\n",
      "Epoch 835 | Train Loss: 0.000550 | Validation Loss: 0.000310\n",
      "Epoch 836 | Train Loss: 0.000308 | Validation Loss: 0.000257\n",
      "Epoch 837 | Train Loss: 0.000328 | Validation Loss: 0.000142\n",
      "Epoch 838 | Train Loss: 0.000271 | Validation Loss: 0.000118\n",
      "Epoch 839 | Train Loss: 0.000386 | Validation Loss: 0.000122\n",
      "Epoch 840 | Train Loss: 0.000309 | Validation Loss: 0.000118\n",
      "Epoch 841 | Train Loss: 0.000276 | Validation Loss: 0.000189\n",
      "Epoch 842 | Train Loss: 0.000261 | Validation Loss: 0.000164\n",
      "Epoch 843 | Train Loss: 0.000185 | Validation Loss: 0.000127\n",
      "Epoch 844 | Train Loss: 0.000220 | Validation Loss: 0.000097\n",
      "Epoch 845 | Train Loss: 0.000269 | Validation Loss: 0.000084\n",
      "Epoch 846 | Train Loss: 0.000217 | Validation Loss: 0.000120\n",
      "Epoch 847 | Train Loss: 0.000244 | Validation Loss: 0.000134\n",
      "Epoch 848 | Train Loss: 0.000233 | Validation Loss: 0.000108\n",
      "Epoch 849 | Train Loss: 0.000310 | Validation Loss: 0.000099\n",
      "Epoch 850 | Train Loss: 0.000229 | Validation Loss: 0.000127\n",
      "Epoch 851 | Train Loss: 0.000169 | Validation Loss: 0.000091\n",
      "Epoch 852 | Train Loss: 0.000218 | Validation Loss: 0.000106\n",
      "Epoch 853 | Train Loss: 0.000207 | Validation Loss: 0.000063\n",
      "Epoch 854 | Train Loss: 0.000195 | Validation Loss: 0.000078\n",
      "Epoch 855 | Train Loss: 0.000211 | Validation Loss: 0.000087\n",
      "Epoch 856 | Train Loss: 0.000209 | Validation Loss: 0.000141\n",
      "Epoch 857 | Train Loss: 0.000199 | Validation Loss: 0.000116\n",
      "Epoch 858 | Train Loss: 0.000268 | Validation Loss: 0.000089\n",
      "Epoch 859 | Train Loss: 0.000250 | Validation Loss: 0.000081\n",
      "Epoch 860 | Train Loss: 0.000202 | Validation Loss: 0.000092\n",
      "Epoch 861 | Train Loss: 0.000158 | Validation Loss: 0.000085\n",
      "Epoch 862 | Train Loss: 0.000314 | Validation Loss: 0.000350\n",
      "Epoch 863 | Train Loss: 0.000401 | Validation Loss: 0.000282\n",
      "Epoch 864 | Train Loss: 0.000381 | Validation Loss: 0.000383\n",
      "Epoch 865 | Train Loss: 0.000401 | Validation Loss: 0.000248\n",
      "Epoch 866 | Train Loss: 0.000346 | Validation Loss: 0.000237\n",
      "Epoch 867 | Train Loss: 0.000270 | Validation Loss: 0.000206\n",
      "Epoch 868 | Train Loss: 0.000468 | Validation Loss: 0.000141\n",
      "Epoch 869 | Train Loss: 0.000233 | Validation Loss: 0.000125\n",
      "Epoch 870 | Train Loss: 0.000236 | Validation Loss: 0.000101\n",
      "Epoch 871 | Train Loss: 0.000245 | Validation Loss: 0.000101\n",
      "Epoch 872 | Train Loss: 0.000149 | Validation Loss: 0.000085\n",
      "Epoch 873 | Train Loss: 0.000207 | Validation Loss: 0.000104\n",
      "Epoch 874 | Train Loss: 0.000247 | Validation Loss: 0.000096\n",
      "Epoch 875 | Train Loss: 0.000262 | Validation Loss: 0.000077\n",
      "Epoch 876 | Train Loss: 0.000168 | Validation Loss: 0.000074\n",
      "Epoch 877 | Train Loss: 0.000129 | Validation Loss: 0.000083\n",
      "Epoch 878 | Train Loss: 0.000200 | Validation Loss: 0.000114\n",
      "Epoch 879 | Train Loss: 0.000177 | Validation Loss: 0.000094\n",
      "saved!\n",
      "Epoch 880 | Train Loss: 0.000180 | Validation Loss: 0.000054\n",
      "Epoch 881 | Train Loss: 0.000187 | Validation Loss: 0.000075\n",
      "Epoch 882 | Train Loss: 0.000198 | Validation Loss: 0.000142\n",
      "Epoch 883 | Train Loss: 0.000185 | Validation Loss: 0.000128\n",
      "Epoch 884 | Train Loss: 0.000168 | Validation Loss: 0.000130\n",
      "Epoch 885 | Train Loss: 0.000189 | Validation Loss: 0.000152\n",
      "Epoch 886 | Train Loss: 0.000288 | Validation Loss: 0.000086\n",
      "Epoch 887 | Train Loss: 0.000234 | Validation Loss: 0.000163\n",
      "Epoch 888 | Train Loss: 0.000215 | Validation Loss: 0.000124\n",
      "Epoch 889 | Train Loss: 0.000191 | Validation Loss: 0.000178\n",
      "Epoch 890 | Train Loss: 0.000361 | Validation Loss: 0.000161\n",
      "Epoch 891 | Train Loss: 0.000206 | Validation Loss: 0.000108\n",
      "Epoch 892 | Train Loss: 0.000204 | Validation Loss: 0.000127\n",
      "Epoch 893 | Train Loss: 0.000232 | Validation Loss: 0.000073\n",
      "Epoch 894 | Train Loss: 0.000295 | Validation Loss: 0.000110\n",
      "Epoch 895 | Train Loss: 0.000254 | Validation Loss: 0.000167\n",
      "Epoch 896 | Train Loss: 0.000234 | Validation Loss: 0.000149\n",
      "Epoch 897 | Train Loss: 0.000219 | Validation Loss: 0.000155\n",
      "Epoch 898 | Train Loss: 0.000236 | Validation Loss: 0.000202\n",
      "Epoch 899 | Train Loss: 0.000278 | Validation Loss: 0.000191\n",
      "Epoch 900 | Train Loss: 0.000392 | Validation Loss: 0.000332\n",
      "Epoch 901 | Train Loss: 0.000421 | Validation Loss: 0.000208\n",
      "Epoch 902 | Train Loss: 0.000309 | Validation Loss: 0.000162\n",
      "Epoch 903 | Train Loss: 0.000278 | Validation Loss: 0.000190\n",
      "Epoch 904 | Train Loss: 0.000285 | Validation Loss: 0.000135\n",
      "Epoch 905 | Train Loss: 0.000188 | Validation Loss: 0.000079\n",
      "Epoch 906 | Train Loss: 0.000196 | Validation Loss: 0.000114\n",
      "Epoch 907 | Train Loss: 0.000241 | Validation Loss: 0.000190\n",
      "Epoch 908 | Train Loss: 0.000353 | Validation Loss: 0.000446\n",
      "Epoch 909 | Train Loss: 0.000436 | Validation Loss: 0.000288\n",
      "Epoch 910 | Train Loss: 0.000431 | Validation Loss: 0.000350\n",
      "Epoch 911 | Train Loss: 0.000592 | Validation Loss: 0.000255\n",
      "Epoch 912 | Train Loss: 0.000668 | Validation Loss: 0.000256\n",
      "Epoch 913 | Train Loss: 0.000406 | Validation Loss: 0.000249\n",
      "Epoch 914 | Train Loss: 0.000351 | Validation Loss: 0.000264\n",
      "Epoch 915 | Train Loss: 0.000403 | Validation Loss: 0.000273\n",
      "Epoch 916 | Train Loss: 0.000469 | Validation Loss: 0.000230\n",
      "Epoch 917 | Train Loss: 0.000475 | Validation Loss: 0.000230\n",
      "Epoch 918 | Train Loss: 0.000287 | Validation Loss: 0.000113\n",
      "Epoch 919 | Train Loss: 0.000274 | Validation Loss: 0.000143\n",
      "Epoch 920 | Train Loss: 0.000300 | Validation Loss: 0.000123\n",
      "Epoch 921 | Train Loss: 0.000178 | Validation Loss: 0.000088\n",
      "Epoch 922 | Train Loss: 0.000238 | Validation Loss: 0.000116\n",
      "Epoch 923 | Train Loss: 0.000182 | Validation Loss: 0.000114\n",
      "Epoch 924 | Train Loss: 0.000223 | Validation Loss: 0.000169\n",
      "Epoch 925 | Train Loss: 0.000282 | Validation Loss: 0.000158\n",
      "Epoch 926 | Train Loss: 0.000224 | Validation Loss: 0.000110\n",
      "Epoch 927 | Train Loss: 0.000192 | Validation Loss: 0.000097\n",
      "Epoch 928 | Train Loss: 0.000144 | Validation Loss: 0.000085\n",
      "Epoch 929 | Train Loss: 0.000198 | Validation Loss: 0.000084\n",
      "Epoch 930 | Train Loss: 0.000265 | Validation Loss: 0.000159\n",
      "Epoch 931 | Train Loss: 0.000155 | Validation Loss: 0.000124\n",
      "Epoch 932 | Train Loss: 0.000132 | Validation Loss: 0.000096\n",
      "Epoch 933 | Train Loss: 0.000126 | Validation Loss: 0.000089\n",
      "Epoch 934 | Train Loss: 0.000178 | Validation Loss: 0.000073\n",
      "Epoch 935 | Train Loss: 0.000188 | Validation Loss: 0.000109\n",
      "Epoch 936 | Train Loss: 0.000239 | Validation Loss: 0.000140\n",
      "Epoch 937 | Train Loss: 0.000294 | Validation Loss: 0.000295\n",
      "Epoch 938 | Train Loss: 0.000307 | Validation Loss: 0.000133\n",
      "Epoch 939 | Train Loss: 0.000231 | Validation Loss: 0.000108\n",
      "Epoch 940 | Train Loss: 0.000206 | Validation Loss: 0.000080\n",
      "Epoch 941 | Train Loss: 0.000162 | Validation Loss: 0.000109\n",
      "Epoch 942 | Train Loss: 0.000250 | Validation Loss: 0.000102\n",
      "Epoch 943 | Train Loss: 0.000227 | Validation Loss: 0.000133\n",
      "Epoch 944 | Train Loss: 0.000268 | Validation Loss: 0.000204\n",
      "Epoch 945 | Train Loss: 0.000257 | Validation Loss: 0.000207\n",
      "Epoch 946 | Train Loss: 0.000251 | Validation Loss: 0.000194\n",
      "Epoch 947 | Train Loss: 0.000377 | Validation Loss: 0.000076\n",
      "Epoch 948 | Train Loss: 0.000145 | Validation Loss: 0.000075\n",
      "Epoch 949 | Train Loss: 0.000188 | Validation Loss: 0.000111\n",
      "Epoch 950 | Train Loss: 0.000263 | Validation Loss: 0.000100\n",
      "Epoch 951 | Train Loss: 0.000307 | Validation Loss: 0.000111\n",
      "Epoch 952 | Train Loss: 0.000375 | Validation Loss: 0.000173\n",
      "Epoch 953 | Train Loss: 0.000632 | Validation Loss: 0.000393\n",
      "Epoch 954 | Train Loss: 0.000484 | Validation Loss: 0.000341\n",
      "Epoch 955 | Train Loss: 0.000407 | Validation Loss: 0.000260\n",
      "Epoch 956 | Train Loss: 0.000308 | Validation Loss: 0.000175\n",
      "Epoch 957 | Train Loss: 0.000266 | Validation Loss: 0.000185\n",
      "Epoch 958 | Train Loss: 0.000326 | Validation Loss: 0.000267\n",
      "Epoch 959 | Train Loss: 0.000303 | Validation Loss: 0.000288\n",
      "Epoch 960 | Train Loss: 0.000258 | Validation Loss: 0.000187\n",
      "Epoch 961 | Train Loss: 0.000203 | Validation Loss: 0.000070\n",
      "Epoch 962 | Train Loss: 0.000280 | Validation Loss: 0.000150\n",
      "Epoch 963 | Train Loss: 0.000261 | Validation Loss: 0.000082\n",
      "Epoch 964 | Train Loss: 0.000247 | Validation Loss: 0.000099\n",
      "Epoch 965 | Train Loss: 0.000305 | Validation Loss: 0.000118\n",
      "Epoch 966 | Train Loss: 0.000298 | Validation Loss: 0.000144\n",
      "Epoch 967 | Train Loss: 0.000292 | Validation Loss: 0.000173\n",
      "Epoch 968 | Train Loss: 0.000270 | Validation Loss: 0.000150\n",
      "Epoch 969 | Train Loss: 0.000225 | Validation Loss: 0.000230\n",
      "Epoch 970 | Train Loss: 0.000249 | Validation Loss: 0.000313\n",
      "Epoch 971 | Train Loss: 0.000273 | Validation Loss: 0.000203\n",
      "Epoch 972 | Train Loss: 0.000281 | Validation Loss: 0.000170\n",
      "Epoch 973 | Train Loss: 0.000219 | Validation Loss: 0.000172\n",
      "Epoch 974 | Train Loss: 0.000222 | Validation Loss: 0.000114\n",
      "Epoch 975 | Train Loss: 0.000268 | Validation Loss: 0.000086\n",
      "Epoch 976 | Train Loss: 0.000234 | Validation Loss: 0.000081\n",
      "Epoch 977 | Train Loss: 0.000187 | Validation Loss: 0.000115\n",
      "Epoch 978 | Train Loss: 0.000242 | Validation Loss: 0.000132\n",
      "Epoch 979 | Train Loss: 0.000269 | Validation Loss: 0.000133\n",
      "Epoch 980 | Train Loss: 0.000212 | Validation Loss: 0.000108\n",
      "Epoch 981 | Train Loss: 0.000230 | Validation Loss: 0.000115\n",
      "Epoch 982 | Train Loss: 0.000158 | Validation Loss: 0.000080\n",
      "Epoch 983 | Train Loss: 0.000150 | Validation Loss: 0.000073\n",
      "Epoch 984 | Train Loss: 0.000167 | Validation Loss: 0.000088\n",
      "Epoch 985 | Train Loss: 0.000175 | Validation Loss: 0.000121\n",
      "Epoch 986 | Train Loss: 0.000169 | Validation Loss: 0.000096\n",
      "Epoch 987 | Train Loss: 0.000157 | Validation Loss: 0.000117\n",
      "Epoch 988 | Train Loss: 0.000171 | Validation Loss: 0.000116\n",
      "Epoch 989 | Train Loss: 0.000132 | Validation Loss: 0.000096\n",
      "Epoch 990 | Train Loss: 0.000162 | Validation Loss: 0.000150\n",
      "Epoch 991 | Train Loss: 0.000322 | Validation Loss: 0.000269\n",
      "Epoch 992 | Train Loss: 0.000272 | Validation Loss: 0.000099\n",
      "Epoch 993 | Train Loss: 0.000175 | Validation Loss: 0.000085\n",
      "Epoch 994 | Train Loss: 0.000187 | Validation Loss: 0.000110\n",
      "Epoch 995 | Train Loss: 0.000140 | Validation Loss: 0.000083\n",
      "Epoch 996 | Train Loss: 0.000202 | Validation Loss: 0.000065\n",
      "Epoch 997 | Train Loss: 0.000225 | Validation Loss: 0.000091\n",
      "Epoch 998 | Train Loss: 0.000205 | Validation Loss: 0.000106\n",
      "Epoch 999 | Train Loss: 0.000239 | Validation Loss: 0.000193\n",
      "Epoch 1000 | Train Loss: 0.000303 | Validation Loss: 0.000134\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer = Trainer(model, optimizer, device='cuda' if torch.cuda.is_available() else 'cpu', max_patience=max_patience)\n",
    "trainer.train(train_loader, val_loader, epochs=epochs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a9a9a4f9-f227-46b9-be50-4d9c646e9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dosyasına kaydet\n",
    "train_losses = trainer.train_cost  # liste veya numpy array\n",
    "val_losses = trainer.val_cost\n",
    "\n",
    "np.savetxt(\"results/losses.csv\", \n",
    "           np.column_stack((train_losses, val_losses)), \n",
    "           delimiter=\",\", \n",
    "           header=\"train_loss,val_loss\", \n",
    "           comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7168da8-a305-4c9f-ad14-5976ae968339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
