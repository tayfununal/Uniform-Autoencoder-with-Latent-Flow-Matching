{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HUlH8uSjhEHu","outputId":"f880528a-20ef-47bd-8977-ae9a43e1312f","executionInfo":{"status":"ok","timestamp":1769374681870,"user_tz":-180,"elapsed":1308,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"id":"HUlH8uSjhEHu","execution_count":151,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":152,"id":"c9773b26-6bef-4aed-b8f7-14ba52992e30","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c9773b26-6bef-4aed-b8f7-14ba52992e30","outputId":"3df9f9db-82d6-4c7b-a626-135acf5e8385","executionInfo":{"status":"ok","timestamp":1769374681879,"user_tz":-180,"elapsed":18,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import os\n","\n","print(os.getcwd()) # dosya yolunu ver\n","%run /content/drive/MyDrive/UAE/Model.ipynb\n","%run /content/drive/MyDrive/UAE/Dataset.ipynb\n","\n","plt.rcParams['font.size'] = 16\n","plt.rcParams['font.family'] = 'DeJavu Serif'\n","plt.rcParams['font.serif'] = ['Times New Roman']"]},{"cell_type":"code","execution_count":153,"id":"f7601ea5-682f-44a9-9b06-0f0efd697f46","metadata":{"id":"f7601ea5-682f-44a9-9b06-0f0efd697f46","executionInfo":{"status":"ok","timestamp":1769374681885,"user_tz":-180,"elapsed":4,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[],"source":["class Trainer:\n","    def __init__(self, model, optimizer, device='cpu', max_patience=20):\n","        self.model = model.to(device)\n","        self.optimizer = optimizer\n","        self.device = device\n","\n","        self.max_patience = max_patience\n","        self.best_val_loss = float('inf')\n","        self.patience = 0\n","\n","        self.val_cost = []\n","        self.train_cost = []\n","\n","    def train(self, train_loader, val_loader=None, epochs=10, print_every=1, name=None):\n","        self.model.train()\n","\n","        for epoch in range(1, epochs + 1):\n","            total_loss = 0.0\n","            for x, x_, _ in train_loader:  # label kullanılmıyor\n","                x = x.to(self.device)\n","                x_ = x_.to(self.device)\n","\n","                with torch.no_grad():\n","                  z_hat_, _ = self.model(x_)\n","                z_hat, x_hat = self.model(x)\n","\n","                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,64))\n","\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                total_loss += loss.item()\n","\n","            # Validation (opsiyonel)\n","            if val_loader:\n","                val_loss = self.validate(val_loader)\n","\n","                # Early stopping kontrolü\n","                if val_loss < self.best_val_loss:\n","                    print('saved!')\n","                    torch.save(self.model, name + '.model')\n","                    self.best_val_loss = val_loss\n","                    self.patience = 0\n","\n","                else:\n","                    self.patience = self.patience + 1\n","\n","                if self.patience > self.max_patience:\n","                    print(\"Early stopping triggered.\")\n","                    break\n","\n","            if epoch % print_every == 0:\n","                print(f\"Epoch {epoch:3d} | Train Loss: {total_loss / len(train_loader):.6f} | Validation Loss: {val_loss:.6f}\")\n","\n","            self.val_cost.append(val_loss)\n","            self.train_cost.append(total_loss / len(train_loader))\n","\n","    def validate(self, val_loader):\n","        self.model.eval()\n","        total_loss = 0.0\n","\n","        with torch.no_grad():\n","            for x, x_, _ in val_loader:\n","                x = x.to(self.device)\n","                x_ = x_.to(self.device)\n","\n","\n","                z_hat_, _ = self.model(x_)\n","                z_hat, x_hat = self.model(x)\n","\n","                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x.reshape(-1,64))\n","\n","                total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(val_loader)\n","        #print(f\"→ Validation Loss: {avg_loss:.6f}\")\n","        self.model.train()\n","        return avg_loss"]},{"cell_type":"code","execution_count":154,"id":"6982ede7-a39e-433c-90a9-8850b1f73c85","metadata":{"id":"6982ede7-a39e-433c-90a9-8850b1f73c85","executionInfo":{"status":"ok","timestamp":1769374681900,"user_tz":-180,"elapsed":2,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[],"source":["# Custom Transform\n","class NoiseTransform:\n","    \"\"\"Add some noise.\"\"\"\n","\n","    def __init__(self, split_ratio=0.001, dim=64):\n","\n","        self.split_ratio = split_ratio\n","        self.dim = dim\n","\n","    def __call__(self, x):\n","      return x + self.split_ratio * np.random.randn(self.dim,)"]},{"cell_type":"code","execution_count":155,"id":"54c32132-3b3a-40ef-8c48-eba7f2aae31f","metadata":{"id":"54c32132-3b3a-40ef-8c48-eba7f2aae31f","executionInfo":{"status":"ok","timestamp":1769374681905,"user_tz":-180,"elapsed":4,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[],"source":["# Hyper-Parameters & Settings\n","\n","batch_size = 256\n","lr = 0.0003\n","\n","epochs = 1000\n","max_patience = 1000\n","\n","split_ratio = 0.0001"]},{"cell_type":"code","execution_count":156,"id":"07121ef5-db8d-4bd5-a327-a8646f696809","metadata":{"id":"07121ef5-db8d-4bd5-a327-a8646f696809","executionInfo":{"status":"ok","timestamp":1769374681912,"user_tz":-180,"elapsed":1,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[],"source":["# Dataset\n","train_dataset = DigitsDataset(mode='train', transform=NoiseTransform(split_ratio))\n","val_dataset = DigitsDataset(mode='val', transform=NoiseTransform(split_ratio))\n","test_dataset = DigitsDataset(mode='test')\n","\n","# DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"]},{"cell_type":"code","execution_count":157,"id":"c9abf79c-a874-418b-b46f-aaa2fe649d6e","metadata":{"id":"c9abf79c-a874-418b-b46f-aaa2fe649d6e","executionInfo":{"status":"ok","timestamp":1769374681931,"user_tz":-180,"elapsed":17,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[],"source":["# \"results\" klasörünü oluştur (zaten varsa hata vermez)\n","os.makedirs(\"/content/drive/MyDrive/UAE/results\", exist_ok=True)\n","\n","# Model\n","name = '/content/drive/MyDrive/UAE/results/UAE_Digits'\n","model = To_Uniform(\n","                 encoder_layers=[64, 512, 512, 512, 512, 3],\n","                 decoder_layers=[3, 512, 512, 512, 512, 64],\n","                 encoder_act=nn.SiLU,\n","                 decoder_act=nn.SiLU,\n","                 final_encoder_act=nn.Sigmoid,\n","                 final_decoder_act=nn.Sigmoid,\n","                 use_batchnorm=True\n",")\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":158,"id":"8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48","outputId":"3e103182-3428-4dfe-84b4-bf36b5be6f4e","executionInfo":{"status":"ok","timestamp":1769374762988,"user_tz":-180,"elapsed":81054,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["saved!\n","Epoch   1 | Train Loss: 0.176447 | Validation Loss: 0.170860\n","saved!\n","Epoch   2 | Train Loss: 0.162244 | Validation Loss: 0.144444\n","saved!\n","Epoch   3 | Train Loss: 0.121787 | Validation Loss: 0.092316\n","saved!\n","Epoch   4 | Train Loss: 0.083728 | Validation Loss: 0.082168\n","saved!\n","Epoch   5 | Train Loss: 0.077141 | Validation Loss: 0.078364\n","saved!\n","Epoch   6 | Train Loss: 0.075168 | Validation Loss: 0.076392\n","saved!\n","Epoch   7 | Train Loss: 0.073085 | Validation Loss: 0.074640\n","saved!\n","Epoch   8 | Train Loss: 0.071745 | Validation Loss: 0.073702\n","saved!\n","Epoch   9 | Train Loss: 0.070244 | Validation Loss: 0.072271\n","saved!\n","Epoch  10 | Train Loss: 0.068987 | Validation Loss: 0.071232\n","saved!\n","Epoch  11 | Train Loss: 0.067829 | Validation Loss: 0.069999\n","saved!\n","Epoch  12 | Train Loss: 0.066571 | Validation Loss: 0.068384\n","saved!\n","Epoch  13 | Train Loss: 0.065124 | Validation Loss: 0.066637\n","saved!\n","Epoch  14 | Train Loss: 0.063226 | Validation Loss: 0.064581\n","saved!\n","Epoch  15 | Train Loss: 0.061079 | Validation Loss: 0.062506\n","saved!\n","Epoch  16 | Train Loss: 0.059492 | Validation Loss: 0.061244\n","saved!\n","Epoch  17 | Train Loss: 0.058418 | Validation Loss: 0.060155\n","saved!\n","Epoch  18 | Train Loss: 0.057232 | Validation Loss: 0.058782\n","saved!\n","Epoch  19 | Train Loss: 0.056231 | Validation Loss: 0.057770\n","saved!\n","Epoch  20 | Train Loss: 0.055279 | Validation Loss: 0.056748\n","saved!\n","Epoch  21 | Train Loss: 0.054536 | Validation Loss: 0.056187\n","saved!\n","Epoch  22 | Train Loss: 0.053889 | Validation Loss: 0.055576\n","saved!\n","Epoch  23 | Train Loss: 0.053267 | Validation Loss: 0.054905\n","saved!\n","Epoch  24 | Train Loss: 0.052505 | Validation Loss: 0.053988\n","saved!\n","Epoch  25 | Train Loss: 0.051747 | Validation Loss: 0.053061\n","saved!\n","Epoch  26 | Train Loss: 0.050600 | Validation Loss: 0.051649\n","saved!\n","Epoch  27 | Train Loss: 0.049121 | Validation Loss: 0.050531\n","saved!\n","Epoch  28 | Train Loss: 0.047924 | Validation Loss: 0.049303\n","saved!\n","Epoch  29 | Train Loss: 0.046863 | Validation Loss: 0.048171\n","saved!\n","Epoch  30 | Train Loss: 0.045792 | Validation Loss: 0.047664\n","saved!\n","Epoch  31 | Train Loss: 0.045298 | Validation Loss: 0.046737\n","saved!\n","Epoch  32 | Train Loss: 0.044659 | Validation Loss: 0.046603\n","saved!\n","Epoch  33 | Train Loss: 0.044121 | Validation Loss: 0.046193\n","saved!\n","Epoch  34 | Train Loss: 0.043755 | Validation Loss: 0.045362\n","saved!\n","Epoch  35 | Train Loss: 0.043144 | Validation Loss: 0.044961\n","saved!\n","Epoch  36 | Train Loss: 0.042840 | Validation Loss: 0.044303\n","saved!\n","Epoch  37 | Train Loss: 0.042363 | Validation Loss: 0.044013\n","saved!\n","Epoch  38 | Train Loss: 0.041943 | Validation Loss: 0.043458\n","Epoch  39 | Train Loss: 0.041787 | Validation Loss: 0.043486\n","saved!\n","Epoch  40 | Train Loss: 0.041730 | Validation Loss: 0.042809\n","saved!\n","Epoch  41 | Train Loss: 0.041217 | Validation Loss: 0.042327\n","saved!\n","Epoch  42 | Train Loss: 0.040953 | Validation Loss: 0.042007\n","saved!\n","Epoch  43 | Train Loss: 0.040332 | Validation Loss: 0.041484\n","saved!\n","Epoch  44 | Train Loss: 0.039871 | Validation Loss: 0.041033\n","saved!\n","Epoch  45 | Train Loss: 0.039620 | Validation Loss: 0.040737\n","saved!\n","Epoch  46 | Train Loss: 0.039457 | Validation Loss: 0.040204\n","saved!\n","Epoch  47 | Train Loss: 0.038940 | Validation Loss: 0.039631\n","saved!\n","Epoch  48 | Train Loss: 0.038103 | Validation Loss: 0.039092\n","saved!\n","Epoch  49 | Train Loss: 0.037527 | Validation Loss: 0.038309\n","saved!\n","Epoch  50 | Train Loss: 0.037023 | Validation Loss: 0.038019\n","saved!\n","Epoch  51 | Train Loss: 0.036579 | Validation Loss: 0.037663\n","Epoch  52 | Train Loss: 0.036557 | Validation Loss: 0.037758\n","saved!\n","Epoch  53 | Train Loss: 0.036549 | Validation Loss: 0.037151\n","saved!\n","Epoch  54 | Train Loss: 0.035874 | Validation Loss: 0.036860\n","saved!\n","Epoch  55 | Train Loss: 0.035622 | Validation Loss: 0.036435\n","saved!\n","Epoch  56 | Train Loss: 0.035323 | Validation Loss: 0.036214\n","saved!\n","Epoch  57 | Train Loss: 0.034981 | Validation Loss: 0.036024\n","Epoch  58 | Train Loss: 0.034909 | Validation Loss: 0.036291\n","saved!\n","Epoch  59 | Train Loss: 0.034280 | Validation Loss: 0.035246\n","saved!\n","Epoch  60 | Train Loss: 0.034159 | Validation Loss: 0.034757\n","saved!\n","Epoch  61 | Train Loss: 0.033956 | Validation Loss: 0.034387\n","saved!\n","Epoch  62 | Train Loss: 0.033347 | Validation Loss: 0.034374\n","Epoch  63 | Train Loss: 0.033168 | Validation Loss: 0.034617\n","saved!\n","Epoch  64 | Train Loss: 0.033209 | Validation Loss: 0.033894\n","saved!\n","Epoch  65 | Train Loss: 0.033207 | Validation Loss: 0.033839\n","Epoch  66 | Train Loss: 0.033053 | Validation Loss: 0.033928\n","saved!\n","Epoch  67 | Train Loss: 0.032447 | Validation Loss: 0.033368\n","Epoch  68 | Train Loss: 0.032240 | Validation Loss: 0.033457\n","Epoch  69 | Train Loss: 0.032255 | Validation Loss: 0.033474\n","saved!\n","Epoch  70 | Train Loss: 0.032684 | Validation Loss: 0.032810\n","saved!\n","Epoch  71 | Train Loss: 0.031550 | Validation Loss: 0.032770\n","saved!\n","Epoch  72 | Train Loss: 0.031916 | Validation Loss: 0.032399\n","Epoch  73 | Train Loss: 0.031820 | Validation Loss: 0.032409\n","saved!\n","Epoch  74 | Train Loss: 0.031323 | Validation Loss: 0.032215\n","saved!\n","Epoch  75 | Train Loss: 0.031061 | Validation Loss: 0.032205\n","saved!\n","Epoch  76 | Train Loss: 0.030691 | Validation Loss: 0.031645\n","saved!\n","Epoch  77 | Train Loss: 0.030713 | Validation Loss: 0.031622\n","saved!\n","Epoch  78 | Train Loss: 0.030603 | Validation Loss: 0.031582\n","saved!\n","Epoch  79 | Train Loss: 0.030449 | Validation Loss: 0.031405\n","Epoch  80 | Train Loss: 0.030533 | Validation Loss: 0.031816\n","saved!\n","Epoch  81 | Train Loss: 0.030463 | Validation Loss: 0.031309\n","saved!\n","Epoch  82 | Train Loss: 0.030077 | Validation Loss: 0.031026\n","Epoch  83 | Train Loss: 0.029784 | Validation Loss: 0.031166\n","saved!\n","Epoch  84 | Train Loss: 0.029706 | Validation Loss: 0.030817\n","Epoch  85 | Train Loss: 0.029794 | Validation Loss: 0.030884\n","saved!\n","Epoch  86 | Train Loss: 0.029596 | Validation Loss: 0.030371\n","saved!\n","Epoch  87 | Train Loss: 0.029221 | Validation Loss: 0.030195\n","Epoch  88 | Train Loss: 0.029655 | Validation Loss: 0.030523\n","saved!\n","Epoch  89 | Train Loss: 0.029141 | Validation Loss: 0.030125\n","saved!\n","Epoch  90 | Train Loss: 0.028754 | Validation Loss: 0.030029\n","saved!\n","Epoch  91 | Train Loss: 0.028726 | Validation Loss: 0.029824\n","saved!\n","Epoch  92 | Train Loss: 0.028291 | Validation Loss: 0.029816\n","saved!\n","Epoch  93 | Train Loss: 0.028366 | Validation Loss: 0.029658\n","saved!\n","Epoch  94 | Train Loss: 0.028246 | Validation Loss: 0.029563\n","Epoch  95 | Train Loss: 0.028268 | Validation Loss: 0.029701\n","saved!\n","Epoch  96 | Train Loss: 0.027950 | Validation Loss: 0.029107\n","Epoch  97 | Train Loss: 0.027747 | Validation Loss: 0.029147\n","Epoch  98 | Train Loss: 0.027597 | Validation Loss: 0.029213\n","saved!\n","Epoch  99 | Train Loss: 0.027429 | Validation Loss: 0.029021\n","saved!\n","Epoch 100 | Train Loss: 0.027365 | Validation Loss: 0.028935\n","Epoch 101 | Train Loss: 0.027505 | Validation Loss: 0.029007\n","saved!\n","Epoch 102 | Train Loss: 0.027312 | Validation Loss: 0.028524\n","Epoch 103 | Train Loss: 0.027362 | Validation Loss: 0.028534\n","Epoch 104 | Train Loss: 0.026821 | Validation Loss: 0.028713\n","Epoch 105 | Train Loss: 0.026831 | Validation Loss: 0.028723\n","Epoch 106 | Train Loss: 0.026854 | Validation Loss: 0.028649\n","Epoch 107 | Train Loss: 0.027433 | Validation Loss: 0.028635\n","saved!\n","Epoch 108 | Train Loss: 0.026763 | Validation Loss: 0.028252\n","Epoch 109 | Train Loss: 0.026853 | Validation Loss: 0.028436\n","Epoch 110 | Train Loss: 0.027264 | Validation Loss: 0.028433\n","Epoch 111 | Train Loss: 0.026915 | Validation Loss: 0.028491\n","Epoch 112 | Train Loss: 0.026376 | Validation Loss: 0.028281\n","saved!\n","Epoch 113 | Train Loss: 0.026116 | Validation Loss: 0.028138\n","saved!\n","Epoch 114 | Train Loss: 0.025891 | Validation Loss: 0.027739\n","saved!\n","Epoch 115 | Train Loss: 0.025579 | Validation Loss: 0.027570\n","Epoch 116 | Train Loss: 0.025991 | Validation Loss: 0.027773\n","saved!\n","Epoch 117 | Train Loss: 0.026008 | Validation Loss: 0.027521\n","Epoch 118 | Train Loss: 0.026062 | Validation Loss: 0.027625\n","Epoch 119 | Train Loss: 0.026289 | Validation Loss: 0.027703\n","Epoch 120 | Train Loss: 0.026539 | Validation Loss: 0.027553\n","Epoch 121 | Train Loss: 0.025748 | Validation Loss: 0.027543\n","saved!\n","Epoch 122 | Train Loss: 0.025565 | Validation Loss: 0.027141\n","Epoch 123 | Train Loss: 0.025190 | Validation Loss: 0.027166\n","saved!\n","Epoch 124 | Train Loss: 0.025396 | Validation Loss: 0.026959\n","saved!\n","Epoch 125 | Train Loss: 0.024763 | Validation Loss: 0.026921\n","Epoch 126 | Train Loss: 0.024760 | Validation Loss: 0.026948\n","saved!\n","Epoch 127 | Train Loss: 0.025078 | Validation Loss: 0.026670\n","Epoch 128 | Train Loss: 0.024407 | Validation Loss: 0.026685\n","Epoch 129 | Train Loss: 0.025177 | Validation Loss: 0.027002\n","Epoch 130 | Train Loss: 0.025113 | Validation Loss: 0.027018\n","Epoch 131 | Train Loss: 0.024632 | Validation Loss: 0.027034\n","saved!\n","Epoch 132 | Train Loss: 0.024351 | Validation Loss: 0.026641\n","Epoch 133 | Train Loss: 0.024969 | Validation Loss: 0.026711\n","Epoch 134 | Train Loss: 0.024160 | Validation Loss: 0.026829\n","saved!\n","Epoch 135 | Train Loss: 0.023917 | Validation Loss: 0.026491\n","saved!\n","Epoch 136 | Train Loss: 0.023916 | Validation Loss: 0.026469\n","saved!\n","Epoch 137 | Train Loss: 0.023855 | Validation Loss: 0.026350\n","saved!\n","Epoch 138 | Train Loss: 0.023728 | Validation Loss: 0.026108\n","saved!\n","Epoch 139 | Train Loss: 0.023621 | Validation Loss: 0.026010\n","saved!\n","Epoch 140 | Train Loss: 0.023436 | Validation Loss: 0.025893\n","Epoch 141 | Train Loss: 0.023045 | Validation Loss: 0.026017\n","Epoch 142 | Train Loss: 0.023178 | Validation Loss: 0.025976\n","Epoch 143 | Train Loss: 0.023158 | Validation Loss: 0.025906\n","Epoch 144 | Train Loss: 0.023877 | Validation Loss: 0.026387\n","Epoch 145 | Train Loss: 0.023252 | Validation Loss: 0.026260\n","Epoch 146 | Train Loss: 0.023247 | Validation Loss: 0.026201\n","Epoch 147 | Train Loss: 0.023417 | Validation Loss: 0.026388\n","saved!\n","Epoch 148 | Train Loss: 0.023351 | Validation Loss: 0.025733\n","Epoch 149 | Train Loss: 0.022955 | Validation Loss: 0.025754\n","saved!\n","Epoch 150 | Train Loss: 0.022813 | Validation Loss: 0.025453\n","Epoch 151 | Train Loss: 0.022740 | Validation Loss: 0.025711\n","Epoch 152 | Train Loss: 0.022672 | Validation Loss: 0.025543\n","Epoch 153 | Train Loss: 0.022683 | Validation Loss: 0.025777\n","saved!\n","Epoch 154 | Train Loss: 0.022259 | Validation Loss: 0.025452\n","Epoch 155 | Train Loss: 0.022388 | Validation Loss: 0.025489\n","Epoch 156 | Train Loss: 0.022860 | Validation Loss: 0.025567\n","saved!\n","Epoch 157 | Train Loss: 0.022396 | Validation Loss: 0.025181\n","Epoch 158 | Train Loss: 0.022610 | Validation Loss: 0.025505\n","Epoch 159 | Train Loss: 0.022416 | Validation Loss: 0.025279\n","Epoch 160 | Train Loss: 0.021951 | Validation Loss: 0.025686\n","Epoch 161 | Train Loss: 0.022189 | Validation Loss: 0.025305\n","saved!\n","Epoch 162 | Train Loss: 0.021742 | Validation Loss: 0.025176\n","saved!\n","Epoch 163 | Train Loss: 0.022420 | Validation Loss: 0.025112\n","Epoch 164 | Train Loss: 0.022786 | Validation Loss: 0.025589\n","Epoch 165 | Train Loss: 0.022712 | Validation Loss: 0.025237\n","Epoch 166 | Train Loss: 0.021891 | Validation Loss: 0.025140\n","Epoch 167 | Train Loss: 0.022060 | Validation Loss: 0.025115\n","Epoch 168 | Train Loss: 0.022332 | Validation Loss: 0.025541\n","Epoch 169 | Train Loss: 0.022104 | Validation Loss: 0.025565\n","Epoch 170 | Train Loss: 0.021932 | Validation Loss: 0.025237\n","Epoch 171 | Train Loss: 0.021934 | Validation Loss: 0.025367\n","saved!\n","Epoch 172 | Train Loss: 0.021777 | Validation Loss: 0.024892\n","saved!\n","Epoch 173 | Train Loss: 0.021378 | Validation Loss: 0.024835\n","saved!\n","Epoch 174 | Train Loss: 0.021496 | Validation Loss: 0.024763\n","saved!\n","Epoch 175 | Train Loss: 0.021313 | Validation Loss: 0.024531\n","Epoch 176 | Train Loss: 0.021036 | Validation Loss: 0.024770\n","Epoch 177 | Train Loss: 0.021121 | Validation Loss: 0.024681\n","Epoch 178 | Train Loss: 0.021312 | Validation Loss: 0.024678\n","Epoch 179 | Train Loss: 0.020978 | Validation Loss: 0.024577\n","Epoch 180 | Train Loss: 0.020624 | Validation Loss: 0.024777\n","Epoch 181 | Train Loss: 0.020841 | Validation Loss: 0.024689\n","saved!\n","Epoch 182 | Train Loss: 0.020785 | Validation Loss: 0.024503\n","Epoch 183 | Train Loss: 0.020465 | Validation Loss: 0.024624\n","saved!\n","Epoch 184 | Train Loss: 0.020520 | Validation Loss: 0.024401\n","Epoch 185 | Train Loss: 0.020650 | Validation Loss: 0.024539\n","Epoch 186 | Train Loss: 0.020587 | Validation Loss: 0.024721\n","Epoch 187 | Train Loss: 0.020851 | Validation Loss: 0.024788\n","Epoch 188 | Train Loss: 0.021287 | Validation Loss: 0.024517\n","Epoch 189 | Train Loss: 0.021167 | Validation Loss: 0.024513\n","Epoch 190 | Train Loss: 0.020693 | Validation Loss: 0.024634\n","Epoch 191 | Train Loss: 0.020954 | Validation Loss: 0.024657\n","Epoch 192 | Train Loss: 0.020540 | Validation Loss: 0.024599\n","Epoch 193 | Train Loss: 0.020442 | Validation Loss: 0.024627\n","Epoch 194 | Train Loss: 0.020538 | Validation Loss: 0.024412\n","saved!\n","Epoch 195 | Train Loss: 0.019903 | Validation Loss: 0.024336\n","Epoch 196 | Train Loss: 0.020134 | Validation Loss: 0.024533\n","Epoch 197 | Train Loss: 0.020648 | Validation Loss: 0.024468\n","Epoch 198 | Train Loss: 0.020039 | Validation Loss: 0.024573\n","Epoch 199 | Train Loss: 0.020084 | Validation Loss: 0.024350\n","saved!\n","Epoch 200 | Train Loss: 0.020044 | Validation Loss: 0.024201\n","saved!\n","Epoch 201 | Train Loss: 0.020301 | Validation Loss: 0.024148\n","Epoch 202 | Train Loss: 0.020124 | Validation Loss: 0.024406\n","Epoch 203 | Train Loss: 0.019918 | Validation Loss: 0.024277\n","saved!\n","Epoch 204 | Train Loss: 0.019877 | Validation Loss: 0.024148\n","Epoch 205 | Train Loss: 0.019862 | Validation Loss: 0.024526\n","saved!\n","Epoch 206 | Train Loss: 0.019877 | Validation Loss: 0.024001\n","Epoch 207 | Train Loss: 0.019819 | Validation Loss: 0.024168\n","Epoch 208 | Train Loss: 0.019759 | Validation Loss: 0.024229\n","Epoch 209 | Train Loss: 0.019730 | Validation Loss: 0.024344\n","Epoch 210 | Train Loss: 0.020721 | Validation Loss: 0.025085\n","Epoch 211 | Train Loss: 0.019897 | Validation Loss: 0.024013\n","Epoch 212 | Train Loss: 0.019748 | Validation Loss: 0.024074\n","Epoch 213 | Train Loss: 0.019966 | Validation Loss: 0.024125\n","saved!\n","Epoch 214 | Train Loss: 0.019229 | Validation Loss: 0.023954\n","Epoch 215 | Train Loss: 0.019665 | Validation Loss: 0.024188\n","Epoch 216 | Train Loss: 0.019719 | Validation Loss: 0.024345\n","Epoch 217 | Train Loss: 0.019847 | Validation Loss: 0.024251\n","Epoch 218 | Train Loss: 0.019450 | Validation Loss: 0.024516\n","Epoch 219 | Train Loss: 0.019552 | Validation Loss: 0.024108\n","Epoch 220 | Train Loss: 0.019032 | Validation Loss: 0.024157\n","saved!\n","Epoch 221 | Train Loss: 0.019376 | Validation Loss: 0.023912\n","Epoch 222 | Train Loss: 0.019070 | Validation Loss: 0.024023\n","Epoch 223 | Train Loss: 0.019187 | Validation Loss: 0.023947\n","Epoch 224 | Train Loss: 0.019043 | Validation Loss: 0.024080\n","Epoch 225 | Train Loss: 0.019492 | Validation Loss: 0.024291\n","saved!\n","Epoch 226 | Train Loss: 0.019234 | Validation Loss: 0.023862\n","saved!\n","Epoch 227 | Train Loss: 0.019744 | Validation Loss: 0.023678\n","Epoch 228 | Train Loss: 0.019414 | Validation Loss: 0.023955\n","Epoch 229 | Train Loss: 0.019046 | Validation Loss: 0.024112\n","Epoch 230 | Train Loss: 0.019091 | Validation Loss: 0.023763\n","Epoch 231 | Train Loss: 0.018660 | Validation Loss: 0.024009\n","Epoch 232 | Train Loss: 0.018758 | Validation Loss: 0.023769\n","Epoch 233 | Train Loss: 0.018485 | Validation Loss: 0.023831\n","Epoch 234 | Train Loss: 0.018504 | Validation Loss: 0.024196\n","Epoch 235 | Train Loss: 0.019011 | Validation Loss: 0.023883\n","Epoch 236 | Train Loss: 0.019435 | Validation Loss: 0.023858\n","saved!\n","Epoch 237 | Train Loss: 0.018818 | Validation Loss: 0.023446\n","Epoch 238 | Train Loss: 0.018991 | Validation Loss: 0.023739\n","Epoch 239 | Train Loss: 0.019166 | Validation Loss: 0.023781\n","Epoch 240 | Train Loss: 0.018676 | Validation Loss: 0.023673\n","Epoch 241 | Train Loss: 0.018670 | Validation Loss: 0.023743\n","Epoch 242 | Train Loss: 0.018512 | Validation Loss: 0.024010\n","Epoch 243 | Train Loss: 0.018453 | Validation Loss: 0.023844\n","Epoch 244 | Train Loss: 0.018077 | Validation Loss: 0.023539\n","Epoch 245 | Train Loss: 0.018456 | Validation Loss: 0.023526\n","Epoch 246 | Train Loss: 0.018876 | Validation Loss: 0.023799\n","Epoch 247 | Train Loss: 0.018416 | Validation Loss: 0.023716\n","Epoch 248 | Train Loss: 0.018491 | Validation Loss: 0.023743\n","Epoch 249 | Train Loss: 0.018612 | Validation Loss: 0.024074\n","Epoch 250 | Train Loss: 0.018817 | Validation Loss: 0.024023\n","Epoch 251 | Train Loss: 0.018900 | Validation Loss: 0.024354\n","Epoch 252 | Train Loss: 0.018771 | Validation Loss: 0.024105\n","Epoch 253 | Train Loss: 0.018874 | Validation Loss: 0.024198\n","Epoch 254 | Train Loss: 0.018657 | Validation Loss: 0.023765\n","Epoch 255 | Train Loss: 0.018572 | Validation Loss: 0.023868\n","Epoch 256 | Train Loss: 0.018658 | Validation Loss: 0.024096\n","Epoch 257 | Train Loss: 0.018773 | Validation Loss: 0.023747\n","Epoch 258 | Train Loss: 0.018275 | Validation Loss: 0.023997\n","Epoch 259 | Train Loss: 0.018108 | Validation Loss: 0.023825\n","Epoch 260 | Train Loss: 0.018204 | Validation Loss: 0.023480\n","Epoch 261 | Train Loss: 0.017992 | Validation Loss: 0.023617\n","Epoch 262 | Train Loss: 0.017836 | Validation Loss: 0.023500\n","Epoch 263 | Train Loss: 0.018010 | Validation Loss: 0.023481\n","saved!\n","Epoch 264 | Train Loss: 0.017812 | Validation Loss: 0.023388\n","Epoch 265 | Train Loss: 0.017741 | Validation Loss: 0.023623\n","Epoch 266 | Train Loss: 0.017716 | Validation Loss: 0.023466\n","Epoch 267 | Train Loss: 0.017880 | Validation Loss: 0.023624\n","Epoch 268 | Train Loss: 0.017726 | Validation Loss: 0.023731\n","Epoch 269 | Train Loss: 0.018564 | Validation Loss: 0.023836\n","Epoch 270 | Train Loss: 0.018315 | Validation Loss: 0.023559\n","Epoch 271 | Train Loss: 0.017922 | Validation Loss: 0.023626\n","Epoch 272 | Train Loss: 0.017839 | Validation Loss: 0.023718\n","Epoch 273 | Train Loss: 0.017873 | Validation Loss: 0.023475\n","saved!\n","Epoch 274 | Train Loss: 0.017461 | Validation Loss: 0.023373\n","saved!\n","Epoch 275 | Train Loss: 0.017848 | Validation Loss: 0.023261\n","Epoch 276 | Train Loss: 0.017324 | Validation Loss: 0.023537\n","Epoch 277 | Train Loss: 0.017658 | Validation Loss: 0.023459\n","Epoch 278 | Train Loss: 0.017431 | Validation Loss: 0.023879\n","Epoch 279 | Train Loss: 0.017236 | Validation Loss: 0.024036\n","Epoch 280 | Train Loss: 0.017228 | Validation Loss: 0.023829\n","Epoch 281 | Train Loss: 0.017552 | Validation Loss: 0.023454\n","saved!\n","Epoch 282 | Train Loss: 0.017489 | Validation Loss: 0.023212\n","Epoch 283 | Train Loss: 0.017526 | Validation Loss: 0.023337\n","Epoch 284 | Train Loss: 0.018155 | Validation Loss: 0.023769\n","Epoch 285 | Train Loss: 0.017794 | Validation Loss: 0.023387\n","Epoch 286 | Train Loss: 0.017844 | Validation Loss: 0.023650\n","saved!\n","Epoch 287 | Train Loss: 0.017580 | Validation Loss: 0.023058\n","Epoch 288 | Train Loss: 0.017907 | Validation Loss: 0.023525\n","Epoch 289 | Train Loss: 0.016996 | Validation Loss: 0.023192\n","Epoch 290 | Train Loss: 0.017499 | Validation Loss: 0.023190\n","Epoch 291 | Train Loss: 0.017172 | Validation Loss: 0.023362\n","Epoch 292 | Train Loss: 0.017132 | Validation Loss: 0.023672\n","Epoch 293 | Train Loss: 0.017336 | Validation Loss: 0.023308\n","Epoch 294 | Train Loss: 0.017631 | Validation Loss: 0.023648\n","Epoch 295 | Train Loss: 0.017391 | Validation Loss: 0.023216\n","saved!\n","Epoch 296 | Train Loss: 0.017079 | Validation Loss: 0.023014\n","Epoch 297 | Train Loss: 0.017081 | Validation Loss: 0.023170\n","saved!\n","Epoch 298 | Train Loss: 0.017774 | Validation Loss: 0.023003\n","Epoch 299 | Train Loss: 0.017169 | Validation Loss: 0.023138\n","Epoch 300 | Train Loss: 0.017436 | Validation Loss: 0.023053\n","Epoch 301 | Train Loss: 0.017255 | Validation Loss: 0.023176\n","Epoch 302 | Train Loss: 0.017610 | Validation Loss: 0.023081\n","Epoch 303 | Train Loss: 0.017071 | Validation Loss: 0.023338\n","Epoch 304 | Train Loss: 0.016860 | Validation Loss: 0.023131\n","Epoch 305 | Train Loss: 0.016770 | Validation Loss: 0.023238\n","Epoch 306 | Train Loss: 0.016718 | Validation Loss: 0.023039\n","saved!\n","Epoch 307 | Train Loss: 0.016571 | Validation Loss: 0.022932\n","Epoch 308 | Train Loss: 0.017028 | Validation Loss: 0.023098\n","Epoch 309 | Train Loss: 0.017671 | Validation Loss: 0.023439\n","Epoch 310 | Train Loss: 0.017736 | Validation Loss: 0.023266\n","Epoch 311 | Train Loss: 0.018127 | Validation Loss: 0.023510\n","Epoch 312 | Train Loss: 0.017959 | Validation Loss: 0.023511\n","Epoch 313 | Train Loss: 0.017587 | Validation Loss: 0.023235\n","Epoch 314 | Train Loss: 0.016792 | Validation Loss: 0.022982\n","Epoch 315 | Train Loss: 0.016874 | Validation Loss: 0.023040\n","Epoch 316 | Train Loss: 0.016878 | Validation Loss: 0.023220\n","Epoch 317 | Train Loss: 0.016832 | Validation Loss: 0.023084\n","Epoch 318 | Train Loss: 0.017279 | Validation Loss: 0.023011\n","Epoch 319 | Train Loss: 0.017266 | Validation Loss: 0.022997\n","Epoch 320 | Train Loss: 0.016851 | Validation Loss: 0.023118\n","Epoch 321 | Train Loss: 0.016558 | Validation Loss: 0.023074\n","Epoch 322 | Train Loss: 0.016804 | Validation Loss: 0.023271\n","Epoch 323 | Train Loss: 0.016670 | Validation Loss: 0.023059\n","Epoch 324 | Train Loss: 0.017140 | Validation Loss: 0.023059\n","Epoch 325 | Train Loss: 0.016544 | Validation Loss: 0.022952\n","Epoch 326 | Train Loss: 0.016518 | Validation Loss: 0.023072\n","saved!\n","Epoch 327 | Train Loss: 0.016361 | Validation Loss: 0.022688\n","Epoch 328 | Train Loss: 0.016263 | Validation Loss: 0.022865\n","Epoch 329 | Train Loss: 0.016324 | Validation Loss: 0.023085\n","Epoch 330 | Train Loss: 0.016379 | Validation Loss: 0.022973\n","Epoch 331 | Train Loss: 0.016719 | Validation Loss: 0.022972\n","Epoch 332 | Train Loss: 0.016933 | Validation Loss: 0.023107\n","Epoch 333 | Train Loss: 0.016615 | Validation Loss: 0.022835\n","Epoch 334 | Train Loss: 0.016680 | Validation Loss: 0.023238\n","Epoch 335 | Train Loss: 0.016362 | Validation Loss: 0.022980\n","Epoch 336 | Train Loss: 0.016296 | Validation Loss: 0.023251\n","Epoch 337 | Train Loss: 0.016593 | Validation Loss: 0.022885\n","Epoch 338 | Train Loss: 0.016493 | Validation Loss: 0.023007\n","Epoch 339 | Train Loss: 0.016705 | Validation Loss: 0.022887\n","Epoch 340 | Train Loss: 0.016000 | Validation Loss: 0.023204\n","Epoch 341 | Train Loss: 0.016289 | Validation Loss: 0.022814\n","Epoch 342 | Train Loss: 0.016602 | Validation Loss: 0.023096\n","Epoch 343 | Train Loss: 0.015902 | Validation Loss: 0.023051\n","Epoch 344 | Train Loss: 0.016351 | Validation Loss: 0.023055\n","saved!\n","Epoch 345 | Train Loss: 0.016168 | Validation Loss: 0.022672\n","Epoch 346 | Train Loss: 0.015791 | Validation Loss: 0.023089\n","Epoch 347 | Train Loss: 0.016241 | Validation Loss: 0.022803\n","Epoch 348 | Train Loss: 0.015601 | Validation Loss: 0.023365\n","Epoch 349 | Train Loss: 0.015609 | Validation Loss: 0.022963\n","Epoch 350 | Train Loss: 0.015920 | Validation Loss: 0.023127\n","Epoch 351 | Train Loss: 0.016308 | Validation Loss: 0.023114\n","Epoch 352 | Train Loss: 0.016022 | Validation Loss: 0.023106\n","Epoch 353 | Train Loss: 0.016047 | Validation Loss: 0.022801\n","Epoch 354 | Train Loss: 0.015807 | Validation Loss: 0.022950\n","Epoch 355 | Train Loss: 0.015981 | Validation Loss: 0.023512\n","Epoch 356 | Train Loss: 0.016044 | Validation Loss: 0.023110\n","Epoch 357 | Train Loss: 0.015984 | Validation Loss: 0.022972\n","Epoch 358 | Train Loss: 0.015994 | Validation Loss: 0.023104\n","Epoch 359 | Train Loss: 0.016061 | Validation Loss: 0.022994\n","Epoch 360 | Train Loss: 0.015806 | Validation Loss: 0.022928\n","Epoch 361 | Train Loss: 0.015621 | Validation Loss: 0.022680\n","Epoch 362 | Train Loss: 0.015405 | Validation Loss: 0.022935\n","Epoch 363 | Train Loss: 0.015802 | Validation Loss: 0.022940\n","Epoch 364 | Train Loss: 0.015770 | Validation Loss: 0.023213\n","Epoch 365 | Train Loss: 0.015831 | Validation Loss: 0.022924\n","Epoch 366 | Train Loss: 0.015697 | Validation Loss: 0.023304\n","Epoch 367 | Train Loss: 0.015672 | Validation Loss: 0.022885\n","Epoch 368 | Train Loss: 0.015425 | Validation Loss: 0.023110\n","saved!\n","Epoch 369 | Train Loss: 0.015579 | Validation Loss: 0.022650\n","Epoch 370 | Train Loss: 0.015644 | Validation Loss: 0.022690\n","Epoch 371 | Train Loss: 0.015427 | Validation Loss: 0.022991\n","Epoch 372 | Train Loss: 0.015997 | Validation Loss: 0.022853\n","Epoch 373 | Train Loss: 0.016519 | Validation Loss: 0.023069\n","Epoch 374 | Train Loss: 0.015944 | Validation Loss: 0.023542\n","Epoch 375 | Train Loss: 0.015746 | Validation Loss: 0.023102\n","Epoch 376 | Train Loss: 0.015685 | Validation Loss: 0.023233\n","saved!\n","Epoch 377 | Train Loss: 0.015780 | Validation Loss: 0.022611\n","Epoch 378 | Train Loss: 0.015449 | Validation Loss: 0.022937\n","saved!\n","Epoch 379 | Train Loss: 0.015690 | Validation Loss: 0.022598\n","Epoch 380 | Train Loss: 0.015563 | Validation Loss: 0.022644\n","Epoch 381 | Train Loss: 0.016050 | Validation Loss: 0.023056\n","Epoch 382 | Train Loss: 0.016932 | Validation Loss: 0.023375\n","Epoch 383 | Train Loss: 0.016316 | Validation Loss: 0.023378\n","Epoch 384 | Train Loss: 0.016128 | Validation Loss: 0.022785\n","Epoch 385 | Train Loss: 0.016103 | Validation Loss: 0.023157\n","saved!\n","Epoch 386 | Train Loss: 0.015838 | Validation Loss: 0.022591\n","Epoch 387 | Train Loss: 0.015587 | Validation Loss: 0.022895\n","Epoch 388 | Train Loss: 0.015648 | Validation Loss: 0.022881\n","Epoch 389 | Train Loss: 0.015751 | Validation Loss: 0.022627\n","Epoch 390 | Train Loss: 0.015176 | Validation Loss: 0.023099\n","Epoch 391 | Train Loss: 0.015963 | Validation Loss: 0.022900\n","Epoch 392 | Train Loss: 0.019012 | Validation Loss: 0.023380\n","Epoch 393 | Train Loss: 0.017488 | Validation Loss: 0.022784\n","Epoch 394 | Train Loss: 0.016973 | Validation Loss: 0.022622\n","Epoch 395 | Train Loss: 0.016337 | Validation Loss: 0.022890\n","Epoch 396 | Train Loss: 0.016113 | Validation Loss: 0.023282\n","Epoch 397 | Train Loss: 0.016344 | Validation Loss: 0.022750\n","Epoch 398 | Train Loss: 0.015724 | Validation Loss: 0.023033\n","Epoch 399 | Train Loss: 0.016140 | Validation Loss: 0.022720\n","Epoch 400 | Train Loss: 0.015824 | Validation Loss: 0.023262\n","saved!\n","Epoch 401 | Train Loss: 0.015781 | Validation Loss: 0.022370\n","Epoch 402 | Train Loss: 0.015683 | Validation Loss: 0.022572\n","Epoch 403 | Train Loss: 0.014886 | Validation Loss: 0.022571\n","Epoch 404 | Train Loss: 0.014970 | Validation Loss: 0.022653\n","Epoch 405 | Train Loss: 0.015150 | Validation Loss: 0.022468\n","Epoch 406 | Train Loss: 0.015092 | Validation Loss: 0.022810\n","Epoch 407 | Train Loss: 0.015420 | Validation Loss: 0.022667\n","Epoch 408 | Train Loss: 0.014977 | Validation Loss: 0.022832\n","Epoch 409 | Train Loss: 0.015494 | Validation Loss: 0.022440\n","Epoch 410 | Train Loss: 0.015137 | Validation Loss: 0.022623\n","saved!\n","Epoch 411 | Train Loss: 0.015860 | Validation Loss: 0.022320\n","Epoch 412 | Train Loss: 0.014851 | Validation Loss: 0.022597\n","Epoch 413 | Train Loss: 0.015662 | Validation Loss: 0.022595\n","Epoch 414 | Train Loss: 0.016213 | Validation Loss: 0.022671\n","Epoch 415 | Train Loss: 0.015459 | Validation Loss: 0.022575\n","Epoch 416 | Train Loss: 0.014914 | Validation Loss: 0.022595\n","Epoch 417 | Train Loss: 0.016545 | Validation Loss: 0.023363\n","Epoch 418 | Train Loss: 0.015531 | Validation Loss: 0.022868\n","Epoch 419 | Train Loss: 0.015172 | Validation Loss: 0.023014\n","Epoch 420 | Train Loss: 0.015494 | Validation Loss: 0.022766\n","Epoch 421 | Train Loss: 0.015007 | Validation Loss: 0.022834\n","Epoch 422 | Train Loss: 0.014924 | Validation Loss: 0.022614\n","Epoch 423 | Train Loss: 0.014725 | Validation Loss: 0.022550\n","Epoch 424 | Train Loss: 0.014366 | Validation Loss: 0.022769\n","Epoch 425 | Train Loss: 0.014722 | Validation Loss: 0.022326\n","Epoch 426 | Train Loss: 0.015065 | Validation Loss: 0.022747\n","saved!\n","Epoch 427 | Train Loss: 0.014883 | Validation Loss: 0.022320\n","Epoch 428 | Train Loss: 0.015141 | Validation Loss: 0.022611\n","Epoch 429 | Train Loss: 0.016129 | Validation Loss: 0.022675\n","Epoch 430 | Train Loss: 0.015537 | Validation Loss: 0.023328\n","Epoch 431 | Train Loss: 0.015176 | Validation Loss: 0.022369\n","Epoch 432 | Train Loss: 0.014902 | Validation Loss: 0.022895\n","Epoch 433 | Train Loss: 0.015331 | Validation Loss: 0.022409\n","Epoch 434 | Train Loss: 0.015362 | Validation Loss: 0.022414\n","Epoch 435 | Train Loss: 0.014738 | Validation Loss: 0.022455\n","Epoch 436 | Train Loss: 0.015212 | Validation Loss: 0.022534\n","Epoch 437 | Train Loss: 0.014756 | Validation Loss: 0.022653\n","Epoch 438 | Train Loss: 0.014513 | Validation Loss: 0.022816\n","Epoch 439 | Train Loss: 0.014747 | Validation Loss: 0.022606\n","Epoch 440 | Train Loss: 0.014800 | Validation Loss: 0.022804\n","saved!\n","Epoch 441 | Train Loss: 0.015071 | Validation Loss: 0.022300\n","Epoch 442 | Train Loss: 0.014452 | Validation Loss: 0.022547\n","Epoch 443 | Train Loss: 0.014293 | Validation Loss: 0.022495\n","Epoch 444 | Train Loss: 0.014835 | Validation Loss: 0.022468\n","Epoch 445 | Train Loss: 0.014587 | Validation Loss: 0.022496\n","Epoch 446 | Train Loss: 0.014565 | Validation Loss: 0.022488\n","Epoch 447 | Train Loss: 0.014307 | Validation Loss: 0.022654\n","Epoch 448 | Train Loss: 0.014556 | Validation Loss: 0.022959\n","Epoch 449 | Train Loss: 0.014639 | Validation Loss: 0.022528\n","Epoch 450 | Train Loss: 0.014394 | Validation Loss: 0.022565\n","Epoch 451 | Train Loss: 0.014251 | Validation Loss: 0.022661\n","Epoch 452 | Train Loss: 0.014432 | Validation Loss: 0.022843\n","Epoch 453 | Train Loss: 0.014446 | Validation Loss: 0.022428\n","Epoch 454 | Train Loss: 0.014338 | Validation Loss: 0.022773\n","Epoch 455 | Train Loss: 0.014684 | Validation Loss: 0.022659\n","Epoch 456 | Train Loss: 0.014865 | Validation Loss: 0.022977\n","Epoch 457 | Train Loss: 0.014717 | Validation Loss: 0.022558\n","Epoch 458 | Train Loss: 0.014973 | Validation Loss: 0.022802\n","Epoch 459 | Train Loss: 0.014631 | Validation Loss: 0.022766\n","Epoch 460 | Train Loss: 0.014981 | Validation Loss: 0.022643\n","Epoch 461 | Train Loss: 0.014266 | Validation Loss: 0.022854\n","Epoch 462 | Train Loss: 0.014390 | Validation Loss: 0.022483\n","Epoch 463 | Train Loss: 0.014098 | Validation Loss: 0.022756\n","Epoch 464 | Train Loss: 0.013999 | Validation Loss: 0.022543\n","Epoch 465 | Train Loss: 0.014614 | Validation Loss: 0.022966\n","Epoch 466 | Train Loss: 0.014275 | Validation Loss: 0.022606\n","Epoch 467 | Train Loss: 0.013995 | Validation Loss: 0.022818\n","Epoch 468 | Train Loss: 0.014437 | Validation Loss: 0.022457\n","Epoch 469 | Train Loss: 0.014240 | Validation Loss: 0.022862\n","Epoch 470 | Train Loss: 0.014764 | Validation Loss: 0.022849\n","Epoch 471 | Train Loss: 0.014291 | Validation Loss: 0.022753\n","Epoch 472 | Train Loss: 0.015160 | Validation Loss: 0.022640\n","Epoch 473 | Train Loss: 0.014959 | Validation Loss: 0.022698\n","Epoch 474 | Train Loss: 0.014517 | Validation Loss: 0.022312\n","Epoch 475 | Train Loss: 0.014289 | Validation Loss: 0.023007\n","Epoch 476 | Train Loss: 0.014138 | Validation Loss: 0.022756\n","Epoch 477 | Train Loss: 0.014308 | Validation Loss: 0.022850\n","Epoch 478 | Train Loss: 0.014250 | Validation Loss: 0.022882\n","Epoch 479 | Train Loss: 0.014128 | Validation Loss: 0.022626\n","Epoch 480 | Train Loss: 0.013892 | Validation Loss: 0.022834\n","Epoch 481 | Train Loss: 0.014630 | Validation Loss: 0.022716\n","Epoch 482 | Train Loss: 0.014464 | Validation Loss: 0.022760\n","Epoch 483 | Train Loss: 0.014747 | Validation Loss: 0.022700\n","Epoch 484 | Train Loss: 0.014174 | Validation Loss: 0.022616\n","Epoch 485 | Train Loss: 0.013914 | Validation Loss: 0.022636\n","Epoch 486 | Train Loss: 0.014051 | Validation Loss: 0.022863\n","Epoch 487 | Train Loss: 0.013903 | Validation Loss: 0.022632\n","Epoch 488 | Train Loss: 0.013829 | Validation Loss: 0.023075\n","Epoch 489 | Train Loss: 0.013853 | Validation Loss: 0.022436\n","Epoch 490 | Train Loss: 0.013869 | Validation Loss: 0.023055\n","Epoch 491 | Train Loss: 0.014058 | Validation Loss: 0.022421\n","Epoch 492 | Train Loss: 0.013985 | Validation Loss: 0.022924\n","Epoch 493 | Train Loss: 0.014123 | Validation Loss: 0.022598\n","Epoch 494 | Train Loss: 0.014266 | Validation Loss: 0.023003\n","Epoch 495 | Train Loss: 0.013926 | Validation Loss: 0.022791\n","Epoch 496 | Train Loss: 0.014636 | Validation Loss: 0.022432\n","Epoch 497 | Train Loss: 0.014604 | Validation Loss: 0.022814\n","Epoch 498 | Train Loss: 0.014229 | Validation Loss: 0.022746\n","Epoch 499 | Train Loss: 0.014641 | Validation Loss: 0.022884\n","Epoch 500 | Train Loss: 0.014239 | Validation Loss: 0.023144\n","Epoch 501 | Train Loss: 0.014259 | Validation Loss: 0.022554\n","Epoch 502 | Train Loss: 0.015679 | Validation Loss: 0.022309\n","Epoch 503 | Train Loss: 0.014450 | Validation Loss: 0.022307\n","Epoch 504 | Train Loss: 0.014937 | Validation Loss: 0.022315\n","Epoch 505 | Train Loss: 0.013965 | Validation Loss: 0.022753\n","Epoch 506 | Train Loss: 0.013566 | Validation Loss: 0.022575\n","Epoch 507 | Train Loss: 0.014222 | Validation Loss: 0.022420\n","Epoch 508 | Train Loss: 0.013604 | Validation Loss: 0.022751\n","Epoch 509 | Train Loss: 0.013979 | Validation Loss: 0.022921\n","Epoch 510 | Train Loss: 0.014362 | Validation Loss: 0.023208\n","Epoch 511 | Train Loss: 0.013583 | Validation Loss: 0.022603\n","Epoch 512 | Train Loss: 0.014292 | Validation Loss: 0.022516\n","Epoch 513 | Train Loss: 0.014627 | Validation Loss: 0.022352\n","Epoch 514 | Train Loss: 0.013890 | Validation Loss: 0.022552\n","Epoch 515 | Train Loss: 0.014034 | Validation Loss: 0.022722\n","Epoch 516 | Train Loss: 0.013675 | Validation Loss: 0.022433\n","Epoch 517 | Train Loss: 0.014054 | Validation Loss: 0.022379\n","Epoch 518 | Train Loss: 0.014571 | Validation Loss: 0.022524\n","Epoch 519 | Train Loss: 0.014613 | Validation Loss: 0.022761\n","Epoch 520 | Train Loss: 0.014473 | Validation Loss: 0.022661\n","Epoch 521 | Train Loss: 0.013665 | Validation Loss: 0.022596\n","Epoch 522 | Train Loss: 0.013710 | Validation Loss: 0.022439\n","Epoch 523 | Train Loss: 0.013881 | Validation Loss: 0.022397\n","Epoch 524 | Train Loss: 0.013312 | Validation Loss: 0.022447\n","saved!\n","Epoch 525 | Train Loss: 0.013673 | Validation Loss: 0.022286\n","Epoch 526 | Train Loss: 0.013877 | Validation Loss: 0.022615\n","Epoch 527 | Train Loss: 0.014256 | Validation Loss: 0.022596\n","Epoch 528 | Train Loss: 0.014077 | Validation Loss: 0.022701\n","Epoch 529 | Train Loss: 0.013761 | Validation Loss: 0.022374\n","Epoch 530 | Train Loss: 0.013490 | Validation Loss: 0.022489\n","saved!\n","Epoch 531 | Train Loss: 0.013328 | Validation Loss: 0.022106\n","Epoch 532 | Train Loss: 0.013742 | Validation Loss: 0.022557\n","Epoch 533 | Train Loss: 0.013949 | Validation Loss: 0.022213\n","Epoch 534 | Train Loss: 0.013142 | Validation Loss: 0.022201\n","Epoch 535 | Train Loss: 0.013986 | Validation Loss: 0.022353\n","Epoch 536 | Train Loss: 0.013809 | Validation Loss: 0.022723\n","Epoch 537 | Train Loss: 0.013879 | Validation Loss: 0.022319\n","Epoch 538 | Train Loss: 0.014147 | Validation Loss: 0.022366\n","Epoch 539 | Train Loss: 0.014041 | Validation Loss: 0.022159\n","Epoch 540 | Train Loss: 0.013923 | Validation Loss: 0.022301\n","Epoch 541 | Train Loss: 0.013529 | Validation Loss: 0.022616\n","Epoch 542 | Train Loss: 0.013394 | Validation Loss: 0.022563\n","Epoch 543 | Train Loss: 0.013476 | Validation Loss: 0.022884\n","Epoch 544 | Train Loss: 0.013304 | Validation Loss: 0.022435\n","Epoch 545 | Train Loss: 0.013926 | Validation Loss: 0.022177\n","Epoch 546 | Train Loss: 0.013134 | Validation Loss: 0.022267\n","Epoch 547 | Train Loss: 0.012709 | Validation Loss: 0.022309\n","Epoch 548 | Train Loss: 0.013389 | Validation Loss: 0.022283\n","Epoch 549 | Train Loss: 0.013277 | Validation Loss: 0.022224\n","Epoch 550 | Train Loss: 0.013051 | Validation Loss: 0.022385\n","Epoch 551 | Train Loss: 0.013187 | Validation Loss: 0.022328\n","Epoch 552 | Train Loss: 0.014011 | Validation Loss: 0.022628\n","Epoch 553 | Train Loss: 0.013990 | Validation Loss: 0.022303\n","Epoch 554 | Train Loss: 0.014019 | Validation Loss: 0.022628\n","Epoch 555 | Train Loss: 0.013812 | Validation Loss: 0.022576\n","Epoch 556 | Train Loss: 0.013965 | Validation Loss: 0.022468\n","Epoch 557 | Train Loss: 0.013741 | Validation Loss: 0.022628\n","Epoch 558 | Train Loss: 0.013398 | Validation Loss: 0.022895\n","Epoch 559 | Train Loss: 0.013738 | Validation Loss: 0.022346\n","Epoch 560 | Train Loss: 0.013122 | Validation Loss: 0.022401\n","Epoch 561 | Train Loss: 0.013384 | Validation Loss: 0.022222\n","Epoch 562 | Train Loss: 0.013277 | Validation Loss: 0.022438\n","Epoch 563 | Train Loss: 0.012853 | Validation Loss: 0.022172\n","Epoch 564 | Train Loss: 0.013077 | Validation Loss: 0.022205\n","Epoch 565 | Train Loss: 0.012883 | Validation Loss: 0.022450\n","Epoch 566 | Train Loss: 0.013667 | Validation Loss: 0.022690\n","Epoch 567 | Train Loss: 0.013910 | Validation Loss: 0.022690\n","Epoch 568 | Train Loss: 0.013060 | Validation Loss: 0.022622\n","Epoch 569 | Train Loss: 0.013888 | Validation Loss: 0.022345\n","Epoch 570 | Train Loss: 0.013758 | Validation Loss: 0.022673\n","Epoch 571 | Train Loss: 0.012897 | Validation Loss: 0.022206\n","Epoch 572 | Train Loss: 0.013077 | Validation Loss: 0.022826\n","Epoch 573 | Train Loss: 0.013172 | Validation Loss: 0.022383\n","Epoch 574 | Train Loss: 0.013966 | Validation Loss: 0.022442\n","Epoch 575 | Train Loss: 0.013725 | Validation Loss: 0.022956\n","Epoch 576 | Train Loss: 0.013443 | Validation Loss: 0.022535\n","Epoch 577 | Train Loss: 0.013547 | Validation Loss: 0.022448\n","Epoch 578 | Train Loss: 0.013381 | Validation Loss: 0.022557\n","Epoch 579 | Train Loss: 0.013164 | Validation Loss: 0.022559\n","Epoch 580 | Train Loss: 0.013460 | Validation Loss: 0.022285\n","Epoch 581 | Train Loss: 0.013112 | Validation Loss: 0.023213\n","Epoch 582 | Train Loss: 0.013385 | Validation Loss: 0.022257\n","Epoch 583 | Train Loss: 0.012989 | Validation Loss: 0.022489\n","Epoch 584 | Train Loss: 0.013724 | Validation Loss: 0.022757\n","Epoch 585 | Train Loss: 0.013294 | Validation Loss: 0.022563\n","Epoch 586 | Train Loss: 0.013608 | Validation Loss: 0.022427\n","Epoch 587 | Train Loss: 0.013326 | Validation Loss: 0.022526\n","Epoch 588 | Train Loss: 0.013536 | Validation Loss: 0.022401\n","Epoch 589 | Train Loss: 0.013460 | Validation Loss: 0.022557\n","Epoch 590 | Train Loss: 0.013381 | Validation Loss: 0.022721\n","Epoch 591 | Train Loss: 0.014594 | Validation Loss: 0.022436\n","Epoch 592 | Train Loss: 0.013466 | Validation Loss: 0.022708\n","Epoch 593 | Train Loss: 0.013427 | Validation Loss: 0.022556\n","Epoch 594 | Train Loss: 0.013772 | Validation Loss: 0.022674\n","Epoch 595 | Train Loss: 0.013187 | Validation Loss: 0.023143\n","Epoch 596 | Train Loss: 0.014263 | Validation Loss: 0.023010\n","Epoch 597 | Train Loss: 0.013480 | Validation Loss: 0.022713\n","Epoch 598 | Train Loss: 0.013852 | Validation Loss: 0.022617\n","Epoch 599 | Train Loss: 0.013253 | Validation Loss: 0.022503\n","Epoch 600 | Train Loss: 0.013370 | Validation Loss: 0.022603\n","Epoch 601 | Train Loss: 0.013431 | Validation Loss: 0.022356\n","Epoch 602 | Train Loss: 0.012913 | Validation Loss: 0.022343\n","saved!\n","Epoch 603 | Train Loss: 0.012562 | Validation Loss: 0.022069\n","Epoch 604 | Train Loss: 0.012656 | Validation Loss: 0.022320\n","Epoch 605 | Train Loss: 0.013051 | Validation Loss: 0.022145\n","Epoch 606 | Train Loss: 0.012767 | Validation Loss: 0.022278\n","Epoch 607 | Train Loss: 0.013052 | Validation Loss: 0.022314\n","Epoch 608 | Train Loss: 0.012757 | Validation Loss: 0.022672\n","Epoch 609 | Train Loss: 0.012916 | Validation Loss: 0.022449\n","Epoch 610 | Train Loss: 0.012667 | Validation Loss: 0.022531\n","Epoch 611 | Train Loss: 0.012786 | Validation Loss: 0.022740\n","Epoch 612 | Train Loss: 0.012898 | Validation Loss: 0.022369\n","Epoch 613 | Train Loss: 0.013276 | Validation Loss: 0.022626\n","Epoch 614 | Train Loss: 0.012683 | Validation Loss: 0.022301\n","Epoch 615 | Train Loss: 0.012581 | Validation Loss: 0.022646\n","Epoch 616 | Train Loss: 0.012582 | Validation Loss: 0.022554\n","Epoch 617 | Train Loss: 0.012290 | Validation Loss: 0.022792\n","Epoch 618 | Train Loss: 0.012764 | Validation Loss: 0.022569\n","Epoch 619 | Train Loss: 0.013021 | Validation Loss: 0.022589\n","Epoch 620 | Train Loss: 0.013110 | Validation Loss: 0.022618\n","Epoch 621 | Train Loss: 0.012617 | Validation Loss: 0.022515\n","Epoch 622 | Train Loss: 0.012691 | Validation Loss: 0.022420\n","Epoch 623 | Train Loss: 0.012371 | Validation Loss: 0.022239\n","Epoch 624 | Train Loss: 0.012459 | Validation Loss: 0.022854\n","Epoch 625 | Train Loss: 0.012180 | Validation Loss: 0.022401\n","Epoch 626 | Train Loss: 0.012254 | Validation Loss: 0.022456\n","Epoch 627 | Train Loss: 0.012650 | Validation Loss: 0.022549\n","Epoch 628 | Train Loss: 0.012382 | Validation Loss: 0.022776\n","Epoch 629 | Train Loss: 0.012942 | Validation Loss: 0.022553\n","Epoch 630 | Train Loss: 0.013282 | Validation Loss: 0.022874\n","Epoch 631 | Train Loss: 0.013727 | Validation Loss: 0.022620\n","Epoch 632 | Train Loss: 0.012448 | Validation Loss: 0.022692\n","Epoch 633 | Train Loss: 0.012718 | Validation Loss: 0.022554\n","Epoch 634 | Train Loss: 0.012910 | Validation Loss: 0.022765\n","Epoch 635 | Train Loss: 0.012949 | Validation Loss: 0.022577\n","Epoch 636 | Train Loss: 0.012596 | Validation Loss: 0.023063\n","Epoch 637 | Train Loss: 0.013035 | Validation Loss: 0.022507\n","Epoch 638 | Train Loss: 0.012657 | Validation Loss: 0.022757\n","Epoch 639 | Train Loss: 0.013330 | Validation Loss: 0.022967\n","Epoch 640 | Train Loss: 0.012339 | Validation Loss: 0.022916\n","Epoch 641 | Train Loss: 0.013008 | Validation Loss: 0.022466\n","Epoch 642 | Train Loss: 0.012754 | Validation Loss: 0.022879\n","Epoch 643 | Train Loss: 0.013334 | Validation Loss: 0.022674\n","Epoch 644 | Train Loss: 0.012861 | Validation Loss: 0.022651\n","Epoch 645 | Train Loss: 0.012278 | Validation Loss: 0.022137\n","Epoch 646 | Train Loss: 0.013084 | Validation Loss: 0.022339\n","Epoch 647 | Train Loss: 0.012146 | Validation Loss: 0.022321\n","Epoch 648 | Train Loss: 0.012444 | Validation Loss: 0.022356\n","Epoch 649 | Train Loss: 0.012354 | Validation Loss: 0.022325\n","Epoch 650 | Train Loss: 0.012391 | Validation Loss: 0.022629\n","Epoch 651 | Train Loss: 0.012671 | Validation Loss: 0.022417\n","Epoch 652 | Train Loss: 0.012656 | Validation Loss: 0.022444\n","Epoch 653 | Train Loss: 0.012436 | Validation Loss: 0.022611\n","Epoch 654 | Train Loss: 0.012513 | Validation Loss: 0.022280\n","Epoch 655 | Train Loss: 0.012621 | Validation Loss: 0.022400\n","Epoch 656 | Train Loss: 0.012347 | Validation Loss: 0.022579\n","Epoch 657 | Train Loss: 0.013141 | Validation Loss: 0.022747\n","Epoch 658 | Train Loss: 0.012704 | Validation Loss: 0.022287\n","Epoch 659 | Train Loss: 0.012611 | Validation Loss: 0.022761\n","Epoch 660 | Train Loss: 0.012147 | Validation Loss: 0.022739\n","Epoch 661 | Train Loss: 0.012560 | Validation Loss: 0.022734\n","Epoch 662 | Train Loss: 0.012486 | Validation Loss: 0.022478\n","Epoch 663 | Train Loss: 0.011978 | Validation Loss: 0.022809\n","Epoch 664 | Train Loss: 0.012905 | Validation Loss: 0.022483\n","Epoch 665 | Train Loss: 0.012308 | Validation Loss: 0.022563\n","Epoch 666 | Train Loss: 0.012875 | Validation Loss: 0.022767\n","Epoch 667 | Train Loss: 0.012833 | Validation Loss: 0.022479\n","Epoch 668 | Train Loss: 0.013054 | Validation Loss: 0.022907\n","Epoch 669 | Train Loss: 0.012755 | Validation Loss: 0.022299\n","Epoch 670 | Train Loss: 0.012837 | Validation Loss: 0.022249\n","Epoch 671 | Train Loss: 0.012651 | Validation Loss: 0.022555\n","Epoch 672 | Train Loss: 0.012425 | Validation Loss: 0.022165\n","Epoch 673 | Train Loss: 0.012452 | Validation Loss: 0.022194\n","Epoch 674 | Train Loss: 0.012429 | Validation Loss: 0.022198\n","Epoch 675 | Train Loss: 0.012496 | Validation Loss: 0.022638\n","Epoch 676 | Train Loss: 0.012622 | Validation Loss: 0.022473\n","Epoch 677 | Train Loss: 0.012775 | Validation Loss: 0.022577\n","Epoch 678 | Train Loss: 0.012838 | Validation Loss: 0.022672\n","Epoch 679 | Train Loss: 0.012663 | Validation Loss: 0.022373\n","Epoch 680 | Train Loss: 0.012784 | Validation Loss: 0.022769\n","Epoch 681 | Train Loss: 0.012874 | Validation Loss: 0.022472\n","Epoch 682 | Train Loss: 0.012318 | Validation Loss: 0.022483\n","Epoch 683 | Train Loss: 0.012514 | Validation Loss: 0.022539\n","Epoch 684 | Train Loss: 0.012503 | Validation Loss: 0.022480\n","Epoch 685 | Train Loss: 0.012062 | Validation Loss: 0.022236\n","Epoch 686 | Train Loss: 0.012822 | Validation Loss: 0.022560\n","Epoch 687 | Train Loss: 0.012747 | Validation Loss: 0.022865\n","Epoch 688 | Train Loss: 0.011906 | Validation Loss: 0.022493\n","Epoch 689 | Train Loss: 0.012200 | Validation Loss: 0.022812\n","Epoch 690 | Train Loss: 0.012042 | Validation Loss: 0.022403\n","Epoch 691 | Train Loss: 0.012104 | Validation Loss: 0.022965\n","saved!\n","Epoch 692 | Train Loss: 0.012270 | Validation Loss: 0.022044\n","Epoch 693 | Train Loss: 0.012149 | Validation Loss: 0.022600\n","Epoch 694 | Train Loss: 0.012836 | Validation Loss: 0.022655\n","Epoch 695 | Train Loss: 0.012380 | Validation Loss: 0.022637\n","Epoch 696 | Train Loss: 0.012246 | Validation Loss: 0.022818\n","Epoch 697 | Train Loss: 0.011804 | Validation Loss: 0.022655\n","Epoch 698 | Train Loss: 0.011632 | Validation Loss: 0.022959\n","Epoch 699 | Train Loss: 0.011654 | Validation Loss: 0.022307\n","Epoch 700 | Train Loss: 0.012208 | Validation Loss: 0.022703\n","Epoch 701 | Train Loss: 0.012527 | Validation Loss: 0.022562\n","Epoch 702 | Train Loss: 0.012635 | Validation Loss: 0.022570\n","Epoch 703 | Train Loss: 0.012137 | Validation Loss: 0.022783\n","Epoch 704 | Train Loss: 0.012565 | Validation Loss: 0.022568\n","Epoch 705 | Train Loss: 0.012697 | Validation Loss: 0.023011\n","Epoch 706 | Train Loss: 0.012688 | Validation Loss: 0.022536\n","Epoch 707 | Train Loss: 0.012962 | Validation Loss: 0.022592\n","Epoch 708 | Train Loss: 0.012587 | Validation Loss: 0.022859\n","Epoch 709 | Train Loss: 0.012385 | Validation Loss: 0.022714\n","Epoch 710 | Train Loss: 0.011916 | Validation Loss: 0.022458\n","Epoch 711 | Train Loss: 0.011980 | Validation Loss: 0.022661\n","Epoch 712 | Train Loss: 0.012066 | Validation Loss: 0.022482\n","Epoch 713 | Train Loss: 0.012317 | Validation Loss: 0.022611\n","Epoch 714 | Train Loss: 0.012118 | Validation Loss: 0.022318\n","Epoch 715 | Train Loss: 0.012019 | Validation Loss: 0.022388\n","Epoch 716 | Train Loss: 0.012632 | Validation Loss: 0.022217\n","Epoch 717 | Train Loss: 0.011965 | Validation Loss: 0.022837\n","Epoch 718 | Train Loss: 0.011649 | Validation Loss: 0.022230\n","saved!\n","Epoch 719 | Train Loss: 0.011635 | Validation Loss: 0.021964\n","Epoch 720 | Train Loss: 0.011517 | Validation Loss: 0.022508\n","Epoch 721 | Train Loss: 0.012258 | Validation Loss: 0.022429\n","Epoch 722 | Train Loss: 0.013049 | Validation Loss: 0.022567\n","Epoch 723 | Train Loss: 0.013011 | Validation Loss: 0.022710\n","Epoch 724 | Train Loss: 0.012512 | Validation Loss: 0.022419\n","Epoch 725 | Train Loss: 0.011660 | Validation Loss: 0.022288\n","Epoch 726 | Train Loss: 0.011963 | Validation Loss: 0.022261\n","Epoch 727 | Train Loss: 0.012044 | Validation Loss: 0.022535\n","Epoch 728 | Train Loss: 0.011699 | Validation Loss: 0.022408\n","Epoch 729 | Train Loss: 0.011519 | Validation Loss: 0.022560\n","Epoch 730 | Train Loss: 0.011668 | Validation Loss: 0.022721\n","Epoch 731 | Train Loss: 0.011713 | Validation Loss: 0.022675\n","Epoch 732 | Train Loss: 0.012231 | Validation Loss: 0.022574\n","Epoch 733 | Train Loss: 0.012424 | Validation Loss: 0.022754\n","Epoch 734 | Train Loss: 0.012170 | Validation Loss: 0.022620\n","Epoch 735 | Train Loss: 0.012137 | Validation Loss: 0.022713\n","Epoch 736 | Train Loss: 0.011855 | Validation Loss: 0.022487\n","Epoch 737 | Train Loss: 0.011574 | Validation Loss: 0.022592\n","Epoch 738 | Train Loss: 0.011867 | Validation Loss: 0.022524\n","Epoch 739 | Train Loss: 0.011636 | Validation Loss: 0.022369\n","Epoch 740 | Train Loss: 0.011538 | Validation Loss: 0.022288\n","Epoch 741 | Train Loss: 0.012023 | Validation Loss: 0.022315\n","Epoch 742 | Train Loss: 0.011366 | Validation Loss: 0.022629\n","Epoch 743 | Train Loss: 0.011841 | Validation Loss: 0.022266\n","Epoch 744 | Train Loss: 0.011846 | Validation Loss: 0.022480\n","Epoch 745 | Train Loss: 0.011650 | Validation Loss: 0.022616\n","Epoch 746 | Train Loss: 0.011424 | Validation Loss: 0.022445\n","Epoch 747 | Train Loss: 0.011477 | Validation Loss: 0.022413\n","Epoch 748 | Train Loss: 0.011475 | Validation Loss: 0.022656\n","Epoch 749 | Train Loss: 0.011834 | Validation Loss: 0.022877\n","Epoch 750 | Train Loss: 0.011858 | Validation Loss: 0.022739\n","Epoch 751 | Train Loss: 0.011418 | Validation Loss: 0.022814\n","Epoch 752 | Train Loss: 0.011796 | Validation Loss: 0.022644\n","Epoch 753 | Train Loss: 0.011417 | Validation Loss: 0.022807\n","Epoch 754 | Train Loss: 0.012112 | Validation Loss: 0.022494\n","Epoch 755 | Train Loss: 0.011435 | Validation Loss: 0.022857\n","Epoch 756 | Train Loss: 0.011276 | Validation Loss: 0.022442\n","Epoch 757 | Train Loss: 0.011827 | Validation Loss: 0.022629\n","Epoch 758 | Train Loss: 0.011843 | Validation Loss: 0.022880\n","Epoch 759 | Train Loss: 0.012068 | Validation Loss: 0.022662\n","Epoch 760 | Train Loss: 0.011610 | Validation Loss: 0.022843\n","Epoch 761 | Train Loss: 0.011813 | Validation Loss: 0.023101\n","Epoch 762 | Train Loss: 0.011750 | Validation Loss: 0.022795\n","Epoch 763 | Train Loss: 0.011460 | Validation Loss: 0.022566\n","Epoch 764 | Train Loss: 0.011238 | Validation Loss: 0.022857\n","Epoch 765 | Train Loss: 0.011358 | Validation Loss: 0.022426\n","Epoch 766 | Train Loss: 0.011094 | Validation Loss: 0.022799\n","Epoch 767 | Train Loss: 0.011576 | Validation Loss: 0.022563\n","Epoch 768 | Train Loss: 0.011708 | Validation Loss: 0.022768\n","Epoch 769 | Train Loss: 0.011147 | Validation Loss: 0.022535\n","Epoch 770 | Train Loss: 0.011748 | Validation Loss: 0.022695\n","Epoch 771 | Train Loss: 0.012549 | Validation Loss: 0.022333\n","Epoch 772 | Train Loss: 0.011582 | Validation Loss: 0.022774\n","Epoch 773 | Train Loss: 0.011498 | Validation Loss: 0.022555\n","Epoch 774 | Train Loss: 0.011704 | Validation Loss: 0.022471\n","Epoch 775 | Train Loss: 0.012135 | Validation Loss: 0.022338\n","Epoch 776 | Train Loss: 0.011194 | Validation Loss: 0.022673\n","Epoch 777 | Train Loss: 0.011011 | Validation Loss: 0.022632\n","Epoch 778 | Train Loss: 0.011522 | Validation Loss: 0.022451\n","Epoch 779 | Train Loss: 0.011008 | Validation Loss: 0.022919\n","Epoch 780 | Train Loss: 0.011199 | Validation Loss: 0.022434\n","Epoch 781 | Train Loss: 0.010946 | Validation Loss: 0.022695\n","Epoch 782 | Train Loss: 0.011546 | Validation Loss: 0.022721\n","Epoch 783 | Train Loss: 0.011417 | Validation Loss: 0.022538\n","Epoch 784 | Train Loss: 0.011392 | Validation Loss: 0.022530\n","Epoch 785 | Train Loss: 0.011428 | Validation Loss: 0.022775\n","Epoch 786 | Train Loss: 0.011341 | Validation Loss: 0.022526\n","Epoch 787 | Train Loss: 0.011700 | Validation Loss: 0.022488\n","Epoch 788 | Train Loss: 0.011363 | Validation Loss: 0.022679\n","Epoch 789 | Train Loss: 0.011466 | Validation Loss: 0.022806\n","Epoch 790 | Train Loss: 0.011273 | Validation Loss: 0.022240\n","Epoch 791 | Train Loss: 0.011456 | Validation Loss: 0.023007\n","Epoch 792 | Train Loss: 0.011315 | Validation Loss: 0.022613\n","Epoch 793 | Train Loss: 0.011929 | Validation Loss: 0.022615\n","Epoch 794 | Train Loss: 0.011785 | Validation Loss: 0.022393\n","Epoch 795 | Train Loss: 0.011365 | Validation Loss: 0.022296\n","Epoch 796 | Train Loss: 0.011818 | Validation Loss: 0.022716\n","Epoch 797 | Train Loss: 0.011108 | Validation Loss: 0.022562\n","Epoch 798 | Train Loss: 0.011365 | Validation Loss: 0.022732\n","Epoch 799 | Train Loss: 0.010846 | Validation Loss: 0.022764\n","Epoch 800 | Train Loss: 0.011134 | Validation Loss: 0.022632\n","Epoch 801 | Train Loss: 0.011304 | Validation Loss: 0.022829\n","Epoch 802 | Train Loss: 0.011324 | Validation Loss: 0.022709\n","Epoch 803 | Train Loss: 0.011589 | Validation Loss: 0.022812\n","Epoch 804 | Train Loss: 0.011547 | Validation Loss: 0.022709\n","Epoch 805 | Train Loss: 0.011228 | Validation Loss: 0.022651\n","Epoch 806 | Train Loss: 0.011904 | Validation Loss: 0.022807\n","Epoch 807 | Train Loss: 0.011494 | Validation Loss: 0.023123\n","Epoch 808 | Train Loss: 0.011718 | Validation Loss: 0.022727\n","Epoch 809 | Train Loss: 0.012345 | Validation Loss: 0.022982\n","Epoch 810 | Train Loss: 0.011428 | Validation Loss: 0.022967\n","Epoch 811 | Train Loss: 0.011917 | Validation Loss: 0.022751\n","Epoch 812 | Train Loss: 0.012023 | Validation Loss: 0.022985\n","Epoch 813 | Train Loss: 0.011670 | Validation Loss: 0.022946\n","Epoch 814 | Train Loss: 0.011962 | Validation Loss: 0.023635\n","Epoch 815 | Train Loss: 0.012408 | Validation Loss: 0.023084\n","Epoch 816 | Train Loss: 0.011875 | Validation Loss: 0.023193\n","Epoch 817 | Train Loss: 0.012816 | Validation Loss: 0.023069\n","Epoch 818 | Train Loss: 0.011495 | Validation Loss: 0.022668\n","Epoch 819 | Train Loss: 0.011082 | Validation Loss: 0.022957\n","Epoch 820 | Train Loss: 0.011376 | Validation Loss: 0.022488\n","Epoch 821 | Train Loss: 0.010913 | Validation Loss: 0.022653\n","Epoch 822 | Train Loss: 0.011821 | Validation Loss: 0.022725\n","Epoch 823 | Train Loss: 0.011584 | Validation Loss: 0.022615\n","Epoch 824 | Train Loss: 0.011638 | Validation Loss: 0.022749\n","Epoch 825 | Train Loss: 0.011929 | Validation Loss: 0.022903\n","Epoch 826 | Train Loss: 0.011871 | Validation Loss: 0.022682\n","Epoch 827 | Train Loss: 0.011291 | Validation Loss: 0.022700\n","Epoch 828 | Train Loss: 0.011725 | Validation Loss: 0.022498\n","Epoch 829 | Train Loss: 0.011101 | Validation Loss: 0.022959\n","Epoch 830 | Train Loss: 0.011166 | Validation Loss: 0.022543\n","Epoch 831 | Train Loss: 0.010673 | Validation Loss: 0.022686\n","Epoch 832 | Train Loss: 0.010868 | Validation Loss: 0.022457\n","Epoch 833 | Train Loss: 0.011000 | Validation Loss: 0.022366\n","Epoch 834 | Train Loss: 0.010795 | Validation Loss: 0.022648\n","Epoch 835 | Train Loss: 0.010644 | Validation Loss: 0.022948\n","Epoch 836 | Train Loss: 0.011697 | Validation Loss: 0.022692\n","Epoch 837 | Train Loss: 0.011597 | Validation Loss: 0.022605\n","Epoch 838 | Train Loss: 0.011258 | Validation Loss: 0.022788\n","Epoch 839 | Train Loss: 0.011468 | Validation Loss: 0.022198\n","Epoch 840 | Train Loss: 0.011488 | Validation Loss: 0.022753\n","Epoch 841 | Train Loss: 0.010838 | Validation Loss: 0.022539\n","Epoch 842 | Train Loss: 0.011107 | Validation Loss: 0.022634\n","Epoch 843 | Train Loss: 0.011219 | Validation Loss: 0.022245\n","Epoch 844 | Train Loss: 0.010638 | Validation Loss: 0.022686\n","Epoch 845 | Train Loss: 0.010896 | Validation Loss: 0.022524\n","Epoch 846 | Train Loss: 0.011119 | Validation Loss: 0.022618\n","Epoch 847 | Train Loss: 0.011017 | Validation Loss: 0.022940\n","Epoch 848 | Train Loss: 0.011605 | Validation Loss: 0.022379\n","Epoch 849 | Train Loss: 0.011815 | Validation Loss: 0.022853\n","Epoch 850 | Train Loss: 0.011737 | Validation Loss: 0.022818\n","Epoch 851 | Train Loss: 0.011696 | Validation Loss: 0.023198\n","Epoch 852 | Train Loss: 0.012041 | Validation Loss: 0.022855\n","Epoch 853 | Train Loss: 0.011444 | Validation Loss: 0.022831\n","Epoch 854 | Train Loss: 0.011211 | Validation Loss: 0.022582\n","Epoch 855 | Train Loss: 0.011192 | Validation Loss: 0.022644\n","Epoch 856 | Train Loss: 0.011206 | Validation Loss: 0.022601\n","Epoch 857 | Train Loss: 0.011299 | Validation Loss: 0.022721\n","Epoch 858 | Train Loss: 0.011159 | Validation Loss: 0.022942\n","Epoch 859 | Train Loss: 0.011678 | Validation Loss: 0.022747\n","Epoch 860 | Train Loss: 0.011105 | Validation Loss: 0.022785\n","Epoch 861 | Train Loss: 0.010847 | Validation Loss: 0.022852\n","Epoch 862 | Train Loss: 0.011031 | Validation Loss: 0.023183\n","Epoch 863 | Train Loss: 0.011447 | Validation Loss: 0.022950\n","Epoch 864 | Train Loss: 0.011468 | Validation Loss: 0.022859\n","Epoch 865 | Train Loss: 0.011027 | Validation Loss: 0.022704\n","Epoch 866 | Train Loss: 0.011305 | Validation Loss: 0.022546\n","Epoch 867 | Train Loss: 0.010560 | Validation Loss: 0.022675\n","Epoch 868 | Train Loss: 0.011080 | Validation Loss: 0.022330\n","Epoch 869 | Train Loss: 0.011044 | Validation Loss: 0.022767\n","Epoch 870 | Train Loss: 0.011144 | Validation Loss: 0.022783\n","Epoch 871 | Train Loss: 0.011173 | Validation Loss: 0.023127\n","Epoch 872 | Train Loss: 0.010836 | Validation Loss: 0.022753\n","Epoch 873 | Train Loss: 0.011034 | Validation Loss: 0.022540\n","Epoch 874 | Train Loss: 0.010611 | Validation Loss: 0.023162\n","Epoch 875 | Train Loss: 0.010730 | Validation Loss: 0.022648\n","Epoch 876 | Train Loss: 0.010590 | Validation Loss: 0.022923\n","Epoch 877 | Train Loss: 0.010955 | Validation Loss: 0.022808\n","Epoch 878 | Train Loss: 0.010569 | Validation Loss: 0.023078\n","Epoch 879 | Train Loss: 0.011144 | Validation Loss: 0.022928\n","Epoch 880 | Train Loss: 0.011393 | Validation Loss: 0.022908\n","Epoch 881 | Train Loss: 0.011695 | Validation Loss: 0.023029\n","Epoch 882 | Train Loss: 0.011279 | Validation Loss: 0.023085\n","Epoch 883 | Train Loss: 0.010991 | Validation Loss: 0.023162\n","Epoch 884 | Train Loss: 0.010579 | Validation Loss: 0.022723\n","Epoch 885 | Train Loss: 0.010988 | Validation Loss: 0.022943\n","Epoch 886 | Train Loss: 0.011081 | Validation Loss: 0.023060\n","Epoch 887 | Train Loss: 0.011762 | Validation Loss: 0.022925\n","Epoch 888 | Train Loss: 0.012172 | Validation Loss: 0.022807\n","Epoch 889 | Train Loss: 0.011118 | Validation Loss: 0.022927\n","Epoch 890 | Train Loss: 0.010700 | Validation Loss: 0.023021\n","Epoch 891 | Train Loss: 0.010452 | Validation Loss: 0.022775\n","Epoch 892 | Train Loss: 0.010413 | Validation Loss: 0.022855\n","Epoch 893 | Train Loss: 0.010488 | Validation Loss: 0.022567\n","Epoch 894 | Train Loss: 0.010458 | Validation Loss: 0.022996\n","Epoch 895 | Train Loss: 0.010995 | Validation Loss: 0.022511\n","Epoch 896 | Train Loss: 0.010626 | Validation Loss: 0.023124\n","Epoch 897 | Train Loss: 0.010599 | Validation Loss: 0.022411\n","Epoch 898 | Train Loss: 0.010361 | Validation Loss: 0.022867\n","Epoch 899 | Train Loss: 0.010552 | Validation Loss: 0.022564\n","Epoch 900 | Train Loss: 0.010041 | Validation Loss: 0.022598\n","Epoch 901 | Train Loss: 0.010227 | Validation Loss: 0.022961\n","Epoch 902 | Train Loss: 0.010306 | Validation Loss: 0.022968\n","Epoch 903 | Train Loss: 0.010428 | Validation Loss: 0.023098\n","Epoch 904 | Train Loss: 0.010398 | Validation Loss: 0.023001\n","Epoch 905 | Train Loss: 0.010298 | Validation Loss: 0.022883\n","Epoch 906 | Train Loss: 0.010554 | Validation Loss: 0.023004\n","Epoch 907 | Train Loss: 0.011443 | Validation Loss: 0.022986\n","Epoch 908 | Train Loss: 0.010996 | Validation Loss: 0.023194\n","Epoch 909 | Train Loss: 0.010846 | Validation Loss: 0.022902\n","Epoch 910 | Train Loss: 0.011041 | Validation Loss: 0.023077\n","Epoch 911 | Train Loss: 0.011249 | Validation Loss: 0.023066\n","Epoch 912 | Train Loss: 0.010813 | Validation Loss: 0.022925\n","Epoch 913 | Train Loss: 0.010751 | Validation Loss: 0.023016\n","Epoch 914 | Train Loss: 0.011022 | Validation Loss: 0.022919\n","Epoch 915 | Train Loss: 0.010640 | Validation Loss: 0.023064\n","Epoch 916 | Train Loss: 0.010790 | Validation Loss: 0.022806\n","Epoch 917 | Train Loss: 0.011010 | Validation Loss: 0.022815\n","Epoch 918 | Train Loss: 0.010715 | Validation Loss: 0.022940\n","Epoch 919 | Train Loss: 0.010783 | Validation Loss: 0.022721\n","Epoch 920 | Train Loss: 0.010234 | Validation Loss: 0.022778\n","Epoch 921 | Train Loss: 0.010560 | Validation Loss: 0.022929\n","Epoch 922 | Train Loss: 0.010477 | Validation Loss: 0.022463\n","Epoch 923 | Train Loss: 0.010896 | Validation Loss: 0.023317\n","Epoch 924 | Train Loss: 0.010979 | Validation Loss: 0.022702\n","Epoch 925 | Train Loss: 0.010628 | Validation Loss: 0.023228\n","Epoch 926 | Train Loss: 0.011259 | Validation Loss: 0.022845\n","Epoch 927 | Train Loss: 0.010469 | Validation Loss: 0.022896\n","Epoch 928 | Train Loss: 0.011070 | Validation Loss: 0.022717\n","Epoch 929 | Train Loss: 0.011237 | Validation Loss: 0.023210\n","Epoch 930 | Train Loss: 0.010984 | Validation Loss: 0.022657\n","Epoch 931 | Train Loss: 0.011116 | Validation Loss: 0.023350\n","Epoch 932 | Train Loss: 0.010846 | Validation Loss: 0.022917\n","Epoch 933 | Train Loss: 0.011003 | Validation Loss: 0.023055\n","Epoch 934 | Train Loss: 0.010611 | Validation Loss: 0.022814\n","Epoch 935 | Train Loss: 0.010319 | Validation Loss: 0.022776\n","Epoch 936 | Train Loss: 0.010814 | Validation Loss: 0.022934\n","Epoch 937 | Train Loss: 0.011174 | Validation Loss: 0.022923\n","Epoch 938 | Train Loss: 0.010430 | Validation Loss: 0.022865\n","Epoch 939 | Train Loss: 0.010414 | Validation Loss: 0.022755\n","Epoch 940 | Train Loss: 0.010490 | Validation Loss: 0.022628\n","Epoch 941 | Train Loss: 0.010457 | Validation Loss: 0.022802\n","Epoch 942 | Train Loss: 0.010691 | Validation Loss: 0.022601\n","Epoch 943 | Train Loss: 0.010923 | Validation Loss: 0.022589\n","Epoch 944 | Train Loss: 0.011380 | Validation Loss: 0.022643\n","Epoch 945 | Train Loss: 0.010545 | Validation Loss: 0.022748\n","Epoch 946 | Train Loss: 0.010659 | Validation Loss: 0.022884\n","Epoch 947 | Train Loss: 0.010475 | Validation Loss: 0.022687\n","Epoch 948 | Train Loss: 0.010254 | Validation Loss: 0.022339\n","Epoch 949 | Train Loss: 0.010438 | Validation Loss: 0.022912\n","Epoch 950 | Train Loss: 0.010397 | Validation Loss: 0.022643\n","Epoch 951 | Train Loss: 0.010591 | Validation Loss: 0.022871\n","Epoch 952 | Train Loss: 0.010273 | Validation Loss: 0.022770\n","Epoch 953 | Train Loss: 0.010689 | Validation Loss: 0.022599\n","Epoch 954 | Train Loss: 0.010064 | Validation Loss: 0.022860\n","Epoch 955 | Train Loss: 0.010522 | Validation Loss: 0.022592\n","Epoch 956 | Train Loss: 0.010992 | Validation Loss: 0.022942\n","Epoch 957 | Train Loss: 0.011288 | Validation Loss: 0.022789\n","Epoch 958 | Train Loss: 0.010695 | Validation Loss: 0.022734\n","Epoch 959 | Train Loss: 0.010544 | Validation Loss: 0.023037\n","Epoch 960 | Train Loss: 0.010327 | Validation Loss: 0.022908\n","Epoch 961 | Train Loss: 0.010481 | Validation Loss: 0.023173\n","Epoch 962 | Train Loss: 0.010418 | Validation Loss: 0.022797\n","Epoch 963 | Train Loss: 0.009799 | Validation Loss: 0.022906\n","Epoch 964 | Train Loss: 0.010125 | Validation Loss: 0.022716\n","Epoch 965 | Train Loss: 0.010262 | Validation Loss: 0.022832\n","Epoch 966 | Train Loss: 0.010256 | Validation Loss: 0.022876\n","Epoch 967 | Train Loss: 0.009958 | Validation Loss: 0.022898\n","Epoch 968 | Train Loss: 0.009948 | Validation Loss: 0.022896\n","Epoch 969 | Train Loss: 0.010162 | Validation Loss: 0.022901\n","Epoch 970 | Train Loss: 0.010656 | Validation Loss: 0.022907\n","Epoch 971 | Train Loss: 0.010645 | Validation Loss: 0.023179\n","Epoch 972 | Train Loss: 0.010265 | Validation Loss: 0.022850\n","Epoch 973 | Train Loss: 0.010142 | Validation Loss: 0.022821\n","Epoch 974 | Train Loss: 0.010186 | Validation Loss: 0.022897\n","Epoch 975 | Train Loss: 0.009969 | Validation Loss: 0.023064\n","Epoch 976 | Train Loss: 0.009951 | Validation Loss: 0.023061\n","Epoch 977 | Train Loss: 0.010161 | Validation Loss: 0.022796\n","Epoch 978 | Train Loss: 0.010491 | Validation Loss: 0.022416\n","Epoch 979 | Train Loss: 0.010493 | Validation Loss: 0.023340\n","Epoch 980 | Train Loss: 0.010365 | Validation Loss: 0.022706\n","Epoch 981 | Train Loss: 0.010387 | Validation Loss: 0.023211\n","Epoch 982 | Train Loss: 0.010232 | Validation Loss: 0.022865\n","Epoch 983 | Train Loss: 0.010351 | Validation Loss: 0.023057\n","Epoch 984 | Train Loss: 0.010123 | Validation Loss: 0.022832\n","Epoch 985 | Train Loss: 0.009523 | Validation Loss: 0.022750\n","Epoch 986 | Train Loss: 0.009816 | Validation Loss: 0.022962\n","Epoch 987 | Train Loss: 0.010230 | Validation Loss: 0.022769\n","Epoch 988 | Train Loss: 0.010282 | Validation Loss: 0.022772\n","Epoch 989 | Train Loss: 0.010086 | Validation Loss: 0.023039\n","Epoch 990 | Train Loss: 0.009681 | Validation Loss: 0.022784\n","Epoch 991 | Train Loss: 0.009699 | Validation Loss: 0.022900\n","Epoch 992 | Train Loss: 0.010033 | Validation Loss: 0.022863\n","Epoch 993 | Train Loss: 0.010177 | Validation Loss: 0.023148\n","Epoch 994 | Train Loss: 0.010207 | Validation Loss: 0.022976\n","Epoch 995 | Train Loss: 0.010032 | Validation Loss: 0.022696\n","Epoch 996 | Train Loss: 0.009603 | Validation Loss: 0.022822\n","Epoch 997 | Train Loss: 0.009919 | Validation Loss: 0.022954\n","Epoch 998 | Train Loss: 0.009946 | Validation Loss: 0.023121\n","Epoch 999 | Train Loss: 0.010378 | Validation Loss: 0.023036\n","Epoch 1000 | Train Loss: 0.010403 | Validation Loss: 0.023018\n"]}],"source":["# Training\n","trainer = Trainer(model, optimizer, device='cuda' if torch.cuda.is_available() else 'cpu', max_patience=max_patience)\n","trainer.train(train_loader, val_loader, epochs=epochs, name=name)"]},{"cell_type":"code","execution_count":159,"id":"2cbfbe84-b0ea-4aac-b097-1df904622beb","metadata":{"id":"2cbfbe84-b0ea-4aac-b097-1df904622beb","executionInfo":{"status":"ok","timestamp":1769374763006,"user_tz":-180,"elapsed":15,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[],"source":["# CSV dosyasına kaydet\n","train_losses = trainer.train_cost  # liste veya numpy array\n","val_losses = trainer.val_cost\n","\n","np.savetxt(\"/content/drive/MyDrive/UAE/results/losses.csv\",\n","           np.column_stack((train_losses, val_losses)),\n","           delimiter=\",\",\n","           header=\"train_loss,val_loss\",\n","           comments=\"\")"]},{"cell_type":"code","execution_count":159,"id":"ce602141-ac5d-4830-8e0e-08c3444f0e44","metadata":{"id":"ce602141-ac5d-4830-8e0e-08c3444f0e44","executionInfo":{"status":"ok","timestamp":1769374763011,"user_tz":-180,"elapsed":3,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2f9c585d-79fc-48ef-9bff-c81d30f338e9","metadata":{"id":"2f9c585d-79fc-48ef-9bff-c81d30f338e9"},"source":["# Gift için"]},{"cell_type":"code","execution_count":160,"id":"32ecc4e9-72ac-4dab-a9df-8e87ae1ce900","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"32ecc4e9-72ac-4dab-a9df-8e87ae1ce900","outputId":"1112d93b-c743-4c89-a2dd-4fe334146c3b","executionInfo":{"status":"ok","timestamp":1769374763020,"user_tz":-180,"elapsed":7,"user":{"displayName":"Tayfun","userId":"12622046788964367496"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'# Latent uzayı gift olarak kaydet\\n\\n!pip install kaleido imageio\\n\\nimport os\\nimport imageio.v2 as imageio\\nimport plotly.express as px\\n\\n# Klasör oluştur\\nos.makedirs(\"results/frames\", exist_ok=True)\\n\\n# 3D scatter plot (senin figürün)\\nfig = px.scatter_3d(\\n    df, x=\\'x1\\', y=\\'x2\\', z=\\'x3\\',\\n    color=df[\\'label\\'].astype(str),\\n    labels={\\'x1\\': \\'x₁\\', \\'x2\\': \\'x₂\\', \\'x3\\': \\'x₃\\', \\'color\\': \\'Digit\\'},\\n    opacity=1.\\n)\\nfig.update_layout(title=\\'Digits Dataset – 3D Latent Space\\')\\n\\n# Kamera açılarını döndürerek frame\\'ler oluştur\\nangles = range(0, 360, 5)\\nimages = []\\n\\nfor angle in angles:\\n    fig.update_layout(\\n        scene_camera=dict(\\n            eye=dict(x=2*np.sin(np.radians(angle)), y=2*np.cos(np.radians(angle)), z=0.5)\\n        )\\n    )\\n    filepath = f\"results/frames/frame_{angle:03d}.png\"\\n    fig.write_image(filepath, width=800, height=600)\\n    images.append(imageio.imread(filepath))\\n\\n# GIF olarak kaydet\\nimageio.mimsave(\"results/digits_3d_latent_space.gif\", images, fps=10)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":160}],"source":["\"\"\"# Latent uzayı gift olarak kaydet\n","\n","!pip install kaleido imageio\n","\n","import os\n","import imageio.v2 as imageio\n","import plotly.express as px\n","\n","# Klasör oluştur\n","os.makedirs(\"results/frames\", exist_ok=True)\n","\n","# 3D scatter plot (senin figürün)\n","fig = px.scatter_3d(\n","    df, x='x1', y='x2', z='x3',\n","    color=df['label'].astype(str),\n","    labels={'x1': 'x₁', 'x2': 'x₂', 'x3': 'x₃', 'color': 'Digit'},\n","    opacity=1.\n",")\n","fig.update_layout(title='Digits Dataset – 3D Latent Space')\n","\n","# Kamera açılarını döndürerek frame'ler oluştur\n","angles = range(0, 360, 5)\n","images = []\n","\n","for angle in angles:\n","    fig.update_layout(\n","        scene_camera=dict(\n","            eye=dict(x=2*np.sin(np.radians(angle)), y=2*np.cos(np.radians(angle)), z=0.5)\n","        )\n","    )\n","    filepath = f\"results/frames/frame_{angle:03d}.png\"\n","    fig.write_image(filepath, width=800, height=600)\n","    images.append(imageio.imread(filepath))\n","\n","# GIF olarak kaydet\n","imageio.mimsave(\"results/digits_3d_latent_space.gif\", images, fps=10)\"\"\""]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.17"},"colab":{"provenance":[],"gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}
