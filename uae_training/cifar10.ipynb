{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9773b26-6bef-4aed-b8f7-14ba52992e30",
        "outputId": "16fd7707-ec1f-41fb-a063-57469be73c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "print(os.getcwd()) # dosya yolunu ver\n",
        "%run ./Model.ipynb\n",
        "%run ./Dataset.ipynb\n",
        "\n",
        "plt.rcParams['font.size'] = 14\n",
        "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "SSLuwPCsrLx9",
      "metadata": {
        "id": "SSLuwPCsrLx9"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f7601ea5-682f-44a9-9b06-0f0efd697f46",
      "metadata": {
        "id": "f7601ea5-682f-44a9-9b06-0f0efd697f46"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, device='cpu', max_patience=20):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "\n",
        "        self.max_patience = max_patience\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience = 0\n",
        "\n",
        "        self.val_cost = []\n",
        "        self.train_cost = []\n",
        "\n",
        "    def train(self, train_loader, val_loader=None, epochs=10, print_every=1, name=None):\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            total_loss = 0.0\n",
        "            for x, x_, y in train_loader:  # label kullanılmıyor\n",
        "                x = x.to(self.device)\n",
        "                x_ = x_.to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    z_hat_, x_hat_ = self.model(x_)\n",
        "                z_hat, x_hat = self.model(x)\n",
        "\n",
        "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            if val_loader:\n",
        "                val_loss = self.validate(val_loader)\n",
        "\n",
        "                # Early stopping kontrolü\n",
        "                if val_loss < self.best_val_loss:\n",
        "                    print('saved!')\n",
        "                    torch.save(self.model, name + '.model')\n",
        "                    self.best_val_loss = val_loss\n",
        "                    self.patience = 0\n",
        "\n",
        "                else:\n",
        "                    self.patience = self.patience + 1\n",
        "\n",
        "                if self.patience > self.max_patience:\n",
        "                    break\n",
        "\n",
        "            if epoch % print_every == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Train Loss: {total_loss / len(train_loader):.6f} | Validation Loss: {val_loss:.6f}\")\n",
        "\n",
        "            self.val_cost.append(val_loss)\n",
        "            self.train_cost.append(total_loss / len(train_loader))\n",
        "\n",
        "            #torch.cuda.empty_cache()\n",
        "            #torch.cuda.ipc_collect()\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, x_, _ in val_loader:\n",
        "                x = x.to(self.device)\n",
        "                x_ = x_.to(self.device)\n",
        "\n",
        "                z_hat_, x_hat_ = self.model(x_)\n",
        "                z_hat, x_hat = self.model(x)\n",
        "\n",
        "                loss = self.model.criterion(z_pred=z_hat_, z_true=z_hat, x_pred=x_hat, x_true=x)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        #print(f\"→ Validation Loss: {avg_loss:.6f}\")\n",
        "        self.model.train()\n",
        "        return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6982ede7-a39e-433c-90a9-8850b1f73c85",
      "metadata": {
        "id": "6982ede7-a39e-433c-90a9-8850b1f73c85"
      },
      "outputs": [],
      "source": [
        "# Custom Transform\n",
        "class NoiseTransform:\n",
        "    \"\"\"Add some noise.\"\"\"\n",
        "\n",
        "    def __init__(self, split_ratio=0.001, dim=3072):\n",
        "        self.split_ratio = split_ratio\n",
        "    def __call__(self, x):\n",
        "      return x + split_ratio * torch.randn_like(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f",
      "metadata": {
        "id": "54c32132-3b3a-40ef-8c48-eba7f2aae31f"
      },
      "outputs": [],
      "source": [
        "# Hyper-Parameters & Settings\n",
        "\n",
        "batch_size = 250\n",
        "lr = 0.001\n",
        "\n",
        "epochs = 300\n",
        "max_patience = 300\n",
        "\n",
        "split_ratio = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "07121ef5-db8d-4bd5-a327-a8646f696809",
      "metadata": {
        "id": "07121ef5-db8d-4bd5-a327-a8646f696809"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "train_dataset = CIFAR10Dataset(mode='train', transform=NoiseTransform(split_ratio))\n",
        "val_dataset = CIFAR10Dataset(mode='val', transform=NoiseTransform(split_ratio))\n",
        "test_dataset = CIFAR10Dataset(mode='test')\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3e227713-ae19-4545-9fe6-6edbe081f977",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "3e227713-ae19-4545-9fe6-6edbe081f977",
        "outputId": "d0fba9c8-58d0-4992-a5f3-8eae0d723970"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nX_list = []\\ny_list = []\\n\\nfor x, _, y in train_dataset:\\n    X_list.append(x)\\n    y_list.append(y)\\n\\nX = torch.stack(X_list)\\ny = torch.tensor(y_list)\\n\\nprint('9 dan',X[y==9].shape[0], 'sayıda var')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\"\"\"\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "for x, _, y in train_dataset:\n",
        "    X_list.append(x)\n",
        "    y_list.append(y)\n",
        "\n",
        "X = torch.stack(X_list)\n",
        "y = torch.tensor(y_list)\n",
        "\n",
        "print('9 dan',X[y==9].shape[0], 'sayıda var')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c9abf79c-a874-418b-b46f-aaa2fe649d6e",
      "metadata": {
        "id": "c9abf79c-a874-418b-b46f-aaa2fe649d6e"
      },
      "outputs": [],
      "source": [
        "# \"results\" klasörünü oluştur (zaten varsa hata vermez)\n",
        "os.makedirs(\"./results\", exist_ok=True)\n",
        "\n",
        "name = 'results/UAE_Cifar_10'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model oluştur\n",
        "model = To_Uniform(input_channels=3, latent_dim=256, output_channels=3).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d4bfd07-bccb-4b11-8d2d-3569e8fc0d48",
        "outputId": "baded78a-a12a-484a-a20e-f7bcacccffc8",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved!\n",
            "Epoch   1 | Train Loss: 0.040023 | Validation Loss: 0.025531\n",
            "saved!\n",
            "Epoch   2 | Train Loss: 0.023233 | Validation Loss: 0.021703\n",
            "saved!\n",
            "Epoch   3 | Train Loss: 0.019144 | Validation Loss: 0.017068\n",
            "saved!\n",
            "Epoch   4 | Train Loss: 0.016338 | Validation Loss: 0.015372\n",
            "saved!\n",
            "Epoch   5 | Train Loss: 0.014872 | Validation Loss: 0.014227\n",
            "saved!\n",
            "Epoch   6 | Train Loss: 0.013582 | Validation Loss: 0.012939\n",
            "saved!\n",
            "Epoch   7 | Train Loss: 0.012910 | Validation Loss: 0.012411\n",
            "saved!\n",
            "Epoch   8 | Train Loss: 0.011840 | Validation Loss: 0.011110\n",
            "saved!\n",
            "Epoch   9 | Train Loss: 0.011114 | Validation Loss: 0.010703\n",
            "saved!\n",
            "Epoch  10 | Train Loss: 0.010514 | Validation Loss: 0.010366\n",
            "Epoch  11 | Train Loss: 0.010114 | Validation Loss: 0.010768\n",
            "saved!\n",
            "Epoch  12 | Train Loss: 0.009694 | Validation Loss: 0.009144\n",
            "Epoch  13 | Train Loss: 0.009347 | Validation Loss: 0.009220\n",
            "saved!\n",
            "Epoch  14 | Train Loss: 0.008950 | Validation Loss: 0.008564\n",
            "saved!\n",
            "Epoch  15 | Train Loss: 0.008707 | Validation Loss: 0.008281\n",
            "Epoch  16 | Train Loss: 0.008460 | Validation Loss: 0.008339\n",
            "saved!\n",
            "Epoch  17 | Train Loss: 0.008252 | Validation Loss: 0.008038\n",
            "saved!\n",
            "Epoch  18 | Train Loss: 0.008084 | Validation Loss: 0.007916\n",
            "saved!\n",
            "Epoch  19 | Train Loss: 0.007745 | Validation Loss: 0.007870\n",
            "saved!\n",
            "Epoch  20 | Train Loss: 0.007626 | Validation Loss: 0.007403\n",
            "Epoch  21 | Train Loss: 0.007524 | Validation Loss: 0.007547\n",
            "saved!\n",
            "Epoch  22 | Train Loss: 0.007349 | Validation Loss: 0.007244\n",
            "saved!\n",
            "Epoch  23 | Train Loss: 0.007183 | Validation Loss: 0.006934\n",
            "saved!\n",
            "Epoch  24 | Train Loss: 0.007017 | Validation Loss: 0.006734\n",
            "saved!\n",
            "Epoch  25 | Train Loss: 0.006930 | Validation Loss: 0.006566\n",
            "Epoch  26 | Train Loss: 0.006806 | Validation Loss: 0.006969\n",
            "saved!\n",
            "Epoch  27 | Train Loss: 0.006647 | Validation Loss: 0.006500\n",
            "saved!\n",
            "Epoch  28 | Train Loss: 0.006679 | Validation Loss: 0.006410\n",
            "Epoch  29 | Train Loss: 0.006447 | Validation Loss: 0.006980\n",
            "saved!\n",
            "Epoch  30 | Train Loss: 0.006418 | Validation Loss: 0.006225\n",
            "saved!\n",
            "Epoch  31 | Train Loss: 0.006258 | Validation Loss: 0.006103\n",
            "Epoch  32 | Train Loss: 0.006245 | Validation Loss: 0.006294\n",
            "Epoch  33 | Train Loss: 0.006134 | Validation Loss: 0.006145\n",
            "Epoch  34 | Train Loss: 0.006115 | Validation Loss: 0.006113\n",
            "saved!\n",
            "Epoch  35 | Train Loss: 0.005999 | Validation Loss: 0.006033\n",
            "saved!\n",
            "Epoch  36 | Train Loss: 0.005969 | Validation Loss: 0.005956\n",
            "Epoch  37 | Train Loss: 0.005969 | Validation Loss: 0.006159\n",
            "saved!\n",
            "Epoch  38 | Train Loss: 0.005857 | Validation Loss: 0.005702\n",
            "Epoch  39 | Train Loss: 0.005820 | Validation Loss: 0.005894\n",
            "Epoch  40 | Train Loss: 0.005675 | Validation Loss: 0.005965\n",
            "saved!\n",
            "Epoch  41 | Train Loss: 0.005674 | Validation Loss: 0.005581\n",
            "Epoch  42 | Train Loss: 0.005567 | Validation Loss: 0.005591\n",
            "saved!\n",
            "Epoch  43 | Train Loss: 0.005576 | Validation Loss: 0.005526\n",
            "saved!\n",
            "Epoch  44 | Train Loss: 0.005456 | Validation Loss: 0.005379\n",
            "Epoch  45 | Train Loss: 0.005445 | Validation Loss: 0.005656\n",
            "saved!\n",
            "Epoch  46 | Train Loss: 0.005397 | Validation Loss: 0.005320\n",
            "Epoch  47 | Train Loss: 0.005366 | Validation Loss: 0.005867\n",
            "Epoch  48 | Train Loss: 0.005286 | Validation Loss: 0.005758\n",
            "Epoch  49 | Train Loss: 0.005298 | Validation Loss: 0.005408\n",
            "Epoch  50 | Train Loss: 0.005404 | Validation Loss: 0.005393\n",
            "Epoch  51 | Train Loss: 0.005270 | Validation Loss: 0.006119\n",
            "saved!\n",
            "Epoch  52 | Train Loss: 0.005198 | Validation Loss: 0.005308\n",
            "saved!\n",
            "Epoch  53 | Train Loss: 0.005126 | Validation Loss: 0.005177\n",
            "Epoch  54 | Train Loss: 0.005117 | Validation Loss: 0.005198\n",
            "Epoch  55 | Train Loss: 0.005075 | Validation Loss: 0.005521\n",
            "Epoch  56 | Train Loss: 0.005082 | Validation Loss: 0.005259\n",
            "saved!\n",
            "Epoch  57 | Train Loss: 0.005097 | Validation Loss: 0.005043\n",
            "saved!\n",
            "Epoch  58 | Train Loss: 0.005002 | Validation Loss: 0.005005\n",
            "Epoch  59 | Train Loss: 0.004956 | Validation Loss: 0.005024\n",
            "Epoch  60 | Train Loss: 0.004901 | Validation Loss: 0.005080\n",
            "Epoch  61 | Train Loss: 0.004896 | Validation Loss: 0.005082\n",
            "Epoch  62 | Train Loss: 0.004860 | Validation Loss: 0.005120\n",
            "saved!\n",
            "Epoch  63 | Train Loss: 0.004836 | Validation Loss: 0.005001\n",
            "Epoch  64 | Train Loss: 0.004804 | Validation Loss: 0.005053\n",
            "saved!\n",
            "Epoch  65 | Train Loss: 0.004796 | Validation Loss: 0.004783\n",
            "Epoch  66 | Train Loss: 0.004727 | Validation Loss: 0.005070\n",
            "Epoch  67 | Train Loss: 0.004807 | Validation Loss: 0.004941\n",
            "Epoch  68 | Train Loss: 0.004674 | Validation Loss: 0.005068\n",
            "Epoch  69 | Train Loss: 0.004694 | Validation Loss: 0.005215\n",
            "Epoch  70 | Train Loss: 0.004690 | Validation Loss: 0.004959\n",
            "Epoch  71 | Train Loss: 0.004624 | Validation Loss: 0.005095\n",
            "Epoch  72 | Train Loss: 0.004590 | Validation Loss: 0.004885\n",
            "Epoch  73 | Train Loss: 0.004582 | Validation Loss: 0.005381\n",
            "Epoch  74 | Train Loss: 0.004596 | Validation Loss: 0.004894\n",
            "Epoch  75 | Train Loss: 0.004534 | Validation Loss: 0.004874\n",
            "Epoch  76 | Train Loss: 0.004532 | Validation Loss: 0.004845\n",
            "Epoch  77 | Train Loss: 0.004448 | Validation Loss: 0.004842\n",
            "Epoch  78 | Train Loss: 0.004464 | Validation Loss: 0.004965\n",
            "Epoch  79 | Train Loss: 0.004460 | Validation Loss: 0.004824\n",
            "Epoch  80 | Train Loss: 0.004449 | Validation Loss: 0.004920\n",
            "Epoch  81 | Train Loss: 0.004439 | Validation Loss: 0.004977\n",
            "Epoch  82 | Train Loss: 0.004403 | Validation Loss: 0.004790\n",
            "Epoch  83 | Train Loss: 0.004376 | Validation Loss: 0.004969\n",
            "Epoch  84 | Train Loss: 0.004367 | Validation Loss: 0.005114\n",
            "Epoch  85 | Train Loss: 0.004338 | Validation Loss: 0.004825\n",
            "Epoch  86 | Train Loss: 0.004347 | Validation Loss: 0.004997\n",
            "Epoch  87 | Train Loss: 0.004332 | Validation Loss: 0.004973\n",
            "Epoch  88 | Train Loss: 0.004303 | Validation Loss: 0.004869\n",
            "saved!\n",
            "Epoch  89 | Train Loss: 0.004268 | Validation Loss: 0.004779\n",
            "Epoch  90 | Train Loss: 0.004195 | Validation Loss: 0.004930\n",
            "Epoch  91 | Train Loss: 0.004248 | Validation Loss: 0.004984\n",
            "saved!\n",
            "Epoch  92 | Train Loss: 0.004234 | Validation Loss: 0.004754\n",
            "Epoch  93 | Train Loss: 0.004197 | Validation Loss: 0.004822\n",
            "saved!\n",
            "Epoch  94 | Train Loss: 0.004172 | Validation Loss: 0.004709\n",
            "Epoch  95 | Train Loss: 0.004162 | Validation Loss: 0.004807\n",
            "Epoch  96 | Train Loss: 0.004130 | Validation Loss: 0.004713\n",
            "Epoch  97 | Train Loss: 0.004120 | Validation Loss: 0.004724\n",
            "Epoch  98 | Train Loss: 0.004122 | Validation Loss: 0.004755\n",
            "Epoch  99 | Train Loss: 0.004125 | Validation Loss: 0.004900\n",
            "Epoch 100 | Train Loss: 0.004100 | Validation Loss: 0.004787\n",
            "Epoch 101 | Train Loss: 0.004129 | Validation Loss: 0.004803\n",
            "saved!\n",
            "Epoch 102 | Train Loss: 0.004073 | Validation Loss: 0.004675\n",
            "Epoch 103 | Train Loss: 0.004028 | Validation Loss: 0.004871\n",
            "Epoch 104 | Train Loss: 0.004032 | Validation Loss: 0.004927\n",
            "Epoch 105 | Train Loss: 0.004072 | Validation Loss: 0.004757\n",
            "Epoch 106 | Train Loss: 0.003999 | Validation Loss: 0.005118\n",
            "Epoch 107 | Train Loss: 0.003966 | Validation Loss: 0.004764\n",
            "Epoch 108 | Train Loss: 0.003954 | Validation Loss: 0.004846\n",
            "Epoch 109 | Train Loss: 0.003963 | Validation Loss: 0.004792\n",
            "Epoch 110 | Train Loss: 0.003955 | Validation Loss: 0.004843\n",
            "Epoch 111 | Train Loss: 0.003952 | Validation Loss: 0.004829\n",
            "Epoch 112 | Train Loss: 0.003940 | Validation Loss: 0.004841\n",
            "Epoch 113 | Train Loss: 0.003918 | Validation Loss: 0.004845\n",
            "Epoch 114 | Train Loss: 0.003878 | Validation Loss: 0.004721\n",
            "Epoch 115 | Train Loss: 0.003885 | Validation Loss: 0.005030\n",
            "Epoch 116 | Train Loss: 0.003905 | Validation Loss: 0.004787\n",
            "Epoch 117 | Train Loss: 0.003840 | Validation Loss: 0.004754\n",
            "saved!\n",
            "Epoch 118 | Train Loss: 0.003836 | Validation Loss: 0.004674\n",
            "Epoch 119 | Train Loss: 0.003847 | Validation Loss: 0.004896\n",
            "Epoch 120 | Train Loss: 0.003798 | Validation Loss: 0.004868\n",
            "Epoch 121 | Train Loss: 0.003826 | Validation Loss: 0.004698\n",
            "saved!\n",
            "Epoch 122 | Train Loss: 0.003831 | Validation Loss: 0.004635\n",
            "Epoch 123 | Train Loss: 0.003773 | Validation Loss: 0.004809\n",
            "Epoch 124 | Train Loss: 0.003780 | Validation Loss: 0.004875\n",
            "Epoch 125 | Train Loss: 0.003774 | Validation Loss: 0.004782\n",
            "Epoch 126 | Train Loss: 0.003772 | Validation Loss: 0.004986\n",
            "Epoch 127 | Train Loss: 0.003750 | Validation Loss: 0.004888\n",
            "Epoch 128 | Train Loss: 0.003752 | Validation Loss: 0.004817\n",
            "Epoch 129 | Train Loss: 0.003737 | Validation Loss: 0.004945\n",
            "Epoch 130 | Train Loss: 0.003741 | Validation Loss: 0.004826\n",
            "Epoch 131 | Train Loss: 0.003709 | Validation Loss: 0.005103\n",
            "Epoch 132 | Train Loss: 0.003683 | Validation Loss: 0.004775\n",
            "Epoch 133 | Train Loss: 0.003710 | Validation Loss: 0.004757\n",
            "Epoch 134 | Train Loss: 0.003697 | Validation Loss: 0.004780\n",
            "Epoch 135 | Train Loss: 0.003669 | Validation Loss: 0.004772\n",
            "Epoch 136 | Train Loss: 0.003647 | Validation Loss: 0.004781\n",
            "Epoch 137 | Train Loss: 0.003653 | Validation Loss: 0.005190\n",
            "Epoch 138 | Train Loss: 0.003683 | Validation Loss: 0.004698\n",
            "Epoch 139 | Train Loss: 0.003668 | Validation Loss: 0.004830\n",
            "Epoch 140 | Train Loss: 0.003660 | Validation Loss: 0.004914\n",
            "Epoch 141 | Train Loss: 0.003629 | Validation Loss: 0.004810\n",
            "Epoch 142 | Train Loss: 0.003599 | Validation Loss: 0.004839\n",
            "Epoch 143 | Train Loss: 0.003586 | Validation Loss: 0.004860\n",
            "Epoch 144 | Train Loss: 0.003611 | Validation Loss: 0.004884\n",
            "Epoch 145 | Train Loss: 0.003582 | Validation Loss: 0.004757\n",
            "Epoch 146 | Train Loss: 0.003589 | Validation Loss: 0.004821\n",
            "Epoch 147 | Train Loss: 0.003590 | Validation Loss: 0.004867\n",
            "Epoch 148 | Train Loss: 0.003571 | Validation Loss: 0.004728\n",
            "Epoch 149 | Train Loss: 0.003563 | Validation Loss: 0.004913\n",
            "Epoch 150 | Train Loss: 0.003546 | Validation Loss: 0.004900\n",
            "Epoch 151 | Train Loss: 0.003537 | Validation Loss: 0.004821\n",
            "Epoch 152 | Train Loss: 0.003572 | Validation Loss: 0.004803\n",
            "Epoch 153 | Train Loss: 0.003526 | Validation Loss: 0.004922\n",
            "Epoch 154 | Train Loss: 0.003526 | Validation Loss: 0.004750\n",
            "Epoch 155 | Train Loss: 0.003495 | Validation Loss: 0.004773\n",
            "Epoch 156 | Train Loss: 0.003515 | Validation Loss: 0.004843\n",
            "Epoch 157 | Train Loss: 0.003549 | Validation Loss: 0.005008\n",
            "Epoch 158 | Train Loss: 0.003488 | Validation Loss: 0.004752\n",
            "Epoch 159 | Train Loss: 0.003497 | Validation Loss: 0.004843\n",
            "Epoch 160 | Train Loss: 0.003478 | Validation Loss: 0.004754\n",
            "Epoch 161 | Train Loss: 0.003495 | Validation Loss: 0.005042\n",
            "Epoch 162 | Train Loss: 0.003460 | Validation Loss: 0.004880\n",
            "Epoch 163 | Train Loss: 0.003465 | Validation Loss: 0.004757\n",
            "Epoch 164 | Train Loss: 0.003479 | Validation Loss: 0.004825\n",
            "Epoch 165 | Train Loss: 0.003479 | Validation Loss: 0.004746\n",
            "Epoch 166 | Train Loss: 0.003453 | Validation Loss: 0.005153\n",
            "Epoch 167 | Train Loss: 0.003446 | Validation Loss: 0.004953\n",
            "Epoch 168 | Train Loss: 0.003411 | Validation Loss: 0.004751\n",
            "Epoch 169 | Train Loss: 0.003397 | Validation Loss: 0.004816\n",
            "Epoch 170 | Train Loss: 0.003443 | Validation Loss: 0.005042\n",
            "Epoch 171 | Train Loss: 0.003416 | Validation Loss: 0.004886\n",
            "Epoch 172 | Train Loss: 0.003392 | Validation Loss: 0.004857\n",
            "Epoch 173 | Train Loss: 0.003416 | Validation Loss: 0.004865\n",
            "Epoch 174 | Train Loss: 0.003391 | Validation Loss: 0.004969\n",
            "Epoch 175 | Train Loss: 0.003392 | Validation Loss: 0.004860\n",
            "Epoch 176 | Train Loss: 0.003385 | Validation Loss: 0.005009\n",
            "Epoch 177 | Train Loss: 0.003381 | Validation Loss: 0.004853\n",
            "Epoch 178 | Train Loss: 0.003362 | Validation Loss: 0.004938\n",
            "Epoch 179 | Train Loss: 0.003376 | Validation Loss: 0.004925\n",
            "Epoch 180 | Train Loss: 0.003386 | Validation Loss: 0.004814\n",
            "Epoch 181 | Train Loss: 0.003382 | Validation Loss: 0.004840\n",
            "Epoch 182 | Train Loss: 0.003347 | Validation Loss: 0.004889\n",
            "Epoch 183 | Train Loss: 0.003339 | Validation Loss: 0.004798\n",
            "Epoch 184 | Train Loss: 0.003318 | Validation Loss: 0.004971\n",
            "Epoch 185 | Train Loss: 0.003325 | Validation Loss: 0.004891\n",
            "Epoch 186 | Train Loss: 0.003326 | Validation Loss: 0.004885\n",
            "Epoch 187 | Train Loss: 0.003327 | Validation Loss: 0.004794\n",
            "Epoch 188 | Train Loss: 0.003312 | Validation Loss: 0.004964\n",
            "Epoch 189 | Train Loss: 0.003339 | Validation Loss: 0.004792\n",
            "Epoch 190 | Train Loss: 0.003308 | Validation Loss: 0.004926\n",
            "Epoch 191 | Train Loss: 0.003310 | Validation Loss: 0.004893\n",
            "Epoch 192 | Train Loss: 0.003279 | Validation Loss: 0.004932\n",
            "Epoch 193 | Train Loss: 0.003310 | Validation Loss: 0.004858\n",
            "Epoch 194 | Train Loss: 0.003297 | Validation Loss: 0.004862\n",
            "Epoch 195 | Train Loss: 0.003278 | Validation Loss: 0.004952\n",
            "Epoch 196 | Train Loss: 0.003293 | Validation Loss: 0.004776\n",
            "Epoch 197 | Train Loss: 0.003308 | Validation Loss: 0.005039\n",
            "Epoch 198 | Train Loss: 0.003273 | Validation Loss: 0.005037\n",
            "Epoch 199 | Train Loss: 0.003273 | Validation Loss: 0.004830\n",
            "Epoch 200 | Train Loss: 0.003252 | Validation Loss: 0.004842\n",
            "Epoch 201 | Train Loss: 0.003229 | Validation Loss: 0.004881\n",
            "Epoch 202 | Train Loss: 0.003246 | Validation Loss: 0.004960\n",
            "Epoch 203 | Train Loss: 0.003260 | Validation Loss: 0.004937\n",
            "Epoch 204 | Train Loss: 0.003249 | Validation Loss: 0.005014\n",
            "Epoch 205 | Train Loss: 0.003194 | Validation Loss: 0.004943\n",
            "Epoch 206 | Train Loss: 0.003245 | Validation Loss: 0.004998\n",
            "Epoch 207 | Train Loss: 0.003232 | Validation Loss: 0.004920\n",
            "Epoch 208 | Train Loss: 0.003225 | Validation Loss: 0.005057\n",
            "Epoch 209 | Train Loss: 0.003226 | Validation Loss: 0.005070\n",
            "Epoch 210 | Train Loss: 0.003225 | Validation Loss: 0.005137\n",
            "Epoch 211 | Train Loss: 0.003221 | Validation Loss: 0.004989\n",
            "Epoch 212 | Train Loss: 0.003214 | Validation Loss: 0.004924\n",
            "Epoch 213 | Train Loss: 0.003188 | Validation Loss: 0.004880\n",
            "Epoch 214 | Train Loss: 0.003196 | Validation Loss: 0.004870\n",
            "Epoch 215 | Train Loss: 0.003197 | Validation Loss: 0.004874\n",
            "Epoch 216 | Train Loss: 0.003203 | Validation Loss: 0.004862\n",
            "Epoch 217 | Train Loss: 0.003187 | Validation Loss: 0.004936\n",
            "Epoch 218 | Train Loss: 0.003173 | Validation Loss: 0.004955\n",
            "Epoch 219 | Train Loss: 0.003187 | Validation Loss: 0.004932\n",
            "Epoch 220 | Train Loss: 0.003182 | Validation Loss: 0.004950\n",
            "Epoch 221 | Train Loss: 0.003167 | Validation Loss: 0.004867\n",
            "Epoch 222 | Train Loss: 0.003183 | Validation Loss: 0.004809\n",
            "Epoch 223 | Train Loss: 0.003162 | Validation Loss: 0.004911\n",
            "Epoch 224 | Train Loss: 0.003172 | Validation Loss: 0.004912\n",
            "Epoch 225 | Train Loss: 0.003163 | Validation Loss: 0.004841\n",
            "Epoch 226 | Train Loss: 0.003159 | Validation Loss: 0.004914\n",
            "Epoch 227 | Train Loss: 0.003141 | Validation Loss: 0.004915\n",
            "Epoch 228 | Train Loss: 0.003160 | Validation Loss: 0.004988\n",
            "Epoch 229 | Train Loss: 0.003111 | Validation Loss: 0.004874\n",
            "Epoch 230 | Train Loss: 0.003146 | Validation Loss: 0.004985\n",
            "Epoch 231 | Train Loss: 0.003142 | Validation Loss: 0.004837\n",
            "Epoch 232 | Train Loss: 0.003129 | Validation Loss: 0.005020\n",
            "Epoch 233 | Train Loss: 0.003138 | Validation Loss: 0.004918\n",
            "Epoch 234 | Train Loss: 0.003144 | Validation Loss: 0.004906\n",
            "Epoch 235 | Train Loss: 0.003122 | Validation Loss: 0.004884\n",
            "Epoch 236 | Train Loss: 0.003113 | Validation Loss: 0.004879\n",
            "Epoch 237 | Train Loss: 0.003117 | Validation Loss: 0.004854\n",
            "Epoch 238 | Train Loss: 0.003112 | Validation Loss: 0.004922\n",
            "Epoch 239 | Train Loss: 0.003099 | Validation Loss: 0.004976\n",
            "Epoch 240 | Train Loss: 0.003102 | Validation Loss: 0.004997\n",
            "Epoch 241 | Train Loss: 0.003080 | Validation Loss: 0.004924\n",
            "Epoch 242 | Train Loss: 0.003105 | Validation Loss: 0.004885\n",
            "Epoch 243 | Train Loss: 0.003104 | Validation Loss: 0.004899\n",
            "Epoch 244 | Train Loss: 0.003089 | Validation Loss: 0.004886\n",
            "Epoch 245 | Train Loss: 0.003101 | Validation Loss: 0.004910\n",
            "Epoch 246 | Train Loss: 0.003089 | Validation Loss: 0.004888\n",
            "Epoch 247 | Train Loss: 0.003097 | Validation Loss: 0.004823\n",
            "Epoch 248 | Train Loss: 0.003084 | Validation Loss: 0.004941\n",
            "Epoch 249 | Train Loss: 0.003102 | Validation Loss: 0.004964\n",
            "Epoch 250 | Train Loss: 0.003087 | Validation Loss: 0.004962\n",
            "Epoch 251 | Train Loss: 0.003075 | Validation Loss: 0.004914\n",
            "Epoch 252 | Train Loss: 0.003074 | Validation Loss: 0.004949\n",
            "Epoch 253 | Train Loss: 0.003069 | Validation Loss: 0.004850\n",
            "Epoch 254 | Train Loss: 0.003083 | Validation Loss: 0.004849\n",
            "Epoch 255 | Train Loss: 0.003061 | Validation Loss: 0.004837\n",
            "Epoch 256 | Train Loss: 0.003052 | Validation Loss: 0.005088\n",
            "Epoch 257 | Train Loss: 0.003048 | Validation Loss: 0.004897\n",
            "Epoch 258 | Train Loss: 0.003058 | Validation Loss: 0.004986\n",
            "Epoch 259 | Train Loss: 0.003056 | Validation Loss: 0.004850\n",
            "Epoch 260 | Train Loss: 0.003035 | Validation Loss: 0.004878\n",
            "Epoch 261 | Train Loss: 0.003050 | Validation Loss: 0.004828\n",
            "Epoch 262 | Train Loss: 0.003051 | Validation Loss: 0.004844\n",
            "Epoch 263 | Train Loss: 0.003036 | Validation Loss: 0.004881\n",
            "Epoch 264 | Train Loss: 0.003032 | Validation Loss: 0.004953\n",
            "Epoch 265 | Train Loss: 0.003030 | Validation Loss: 0.004997\n",
            "Epoch 266 | Train Loss: 0.003021 | Validation Loss: 0.004987\n",
            "Epoch 267 | Train Loss: 0.003024 | Validation Loss: 0.004928\n",
            "Epoch 268 | Train Loss: 0.003037 | Validation Loss: 0.004860\n",
            "Epoch 269 | Train Loss: 0.003009 | Validation Loss: 0.004934\n",
            "Epoch 270 | Train Loss: 0.003025 | Validation Loss: 0.004861\n",
            "Epoch 271 | Train Loss: 0.003021 | Validation Loss: 0.004931\n",
            "Epoch 272 | Train Loss: 0.003006 | Validation Loss: 0.004942\n",
            "Epoch 273 | Train Loss: 0.003042 | Validation Loss: 0.004983\n",
            "Epoch 274 | Train Loss: 0.003025 | Validation Loss: 0.004993\n",
            "Epoch 275 | Train Loss: 0.003005 | Validation Loss: 0.004857\n",
            "Epoch 276 | Train Loss: 0.002996 | Validation Loss: 0.004990\n",
            "Epoch 277 | Train Loss: 0.003044 | Validation Loss: 0.004957\n",
            "Epoch 278 | Train Loss: 0.002992 | Validation Loss: 0.004905\n",
            "Epoch 279 | Train Loss: 0.003025 | Validation Loss: 0.004967\n",
            "Epoch 280 | Train Loss: 0.002998 | Validation Loss: 0.004981\n",
            "Epoch 281 | Train Loss: 0.003001 | Validation Loss: 0.004896\n",
            "Epoch 282 | Train Loss: 0.003000 | Validation Loss: 0.005051\n",
            "Epoch 283 | Train Loss: 0.002970 | Validation Loss: 0.005048\n",
            "Epoch 284 | Train Loss: 0.002997 | Validation Loss: 0.004874\n",
            "Epoch 285 | Train Loss: 0.002981 | Validation Loss: 0.004987\n",
            "Epoch 286 | Train Loss: 0.002998 | Validation Loss: 0.004853\n",
            "Epoch 287 | Train Loss: 0.002976 | Validation Loss: 0.004968\n",
            "Epoch 288 | Train Loss: 0.002976 | Validation Loss: 0.004952\n",
            "Epoch 289 | Train Loss: 0.002962 | Validation Loss: 0.004914\n",
            "Epoch 290 | Train Loss: 0.002956 | Validation Loss: 0.004901\n",
            "Epoch 291 | Train Loss: 0.002962 | Validation Loss: 0.005081\n",
            "Epoch 292 | Train Loss: 0.002963 | Validation Loss: 0.004950\n",
            "Epoch 293 | Train Loss: 0.002992 | Validation Loss: 0.004967\n",
            "Epoch 294 | Train Loss: 0.002981 | Validation Loss: 0.004951\n",
            "Epoch 295 | Train Loss: 0.002951 | Validation Loss: 0.004927\n",
            "Epoch 296 | Train Loss: 0.002971 | Validation Loss: 0.004919\n",
            "Epoch 297 | Train Loss: 0.002978 | Validation Loss: 0.004965\n",
            "Epoch 298 | Train Loss: 0.002954 | Validation Loss: 0.004938\n",
            "Epoch 299 | Train Loss: 0.002942 | Validation Loss: 0.004919\n",
            "Epoch 300 | Train Loss: 0.002945 | Validation Loss: 0.004974\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "trainer = Trainer(model, optimizer, device=device, max_patience=max_patience)\n",
        "trainer.train(train_loader, val_loader, epochs=epochs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a226fa68-825d-400d-ab2f-5985c3ffe4d3",
      "metadata": {
        "id": "a226fa68-825d-400d-ab2f-5985c3ffe4d3"
      },
      "outputs": [],
      "source": [
        "# CSV dosyasına kaydet\n",
        "train_losses = trainer.train_cost  # liste veya numpy array\n",
        "val_losses = trainer.val_cost\n",
        "\n",
        "np.savetxt(\"results/losses.csv\",\n",
        "           np.column_stack((train_losses, val_losses)),\n",
        "           delimiter=\",\",\n",
        "           header=\"train_loss,val_loss\",\n",
        "           comments=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7341e459-e765-447e-a51f-f7157fe8f477",
      "metadata": {
        "id": "7341e459-e765-447e-a51f-f7157fe8f477"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}